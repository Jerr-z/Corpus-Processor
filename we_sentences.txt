In this work, we propose AspEntQuaNet, one of the ﬁrst methods for aspect-based sentiment quantiﬁcation.
Using the modiﬁed version of the SemEval 2016 dataset for aspect-based sentiment quantiﬁcation, we show that AspEntQuaNet is superior to all other considered existing methods based on obtained results for various aspect categories.
Using the methods of ordinary SA, we would run into the issue that the review conveys both negative and positive sentiment.
However, by performing the SA task on an aspect level, we would obtain separate sentiment scores for each of the involved aspects.
Therefore, with the methods of ABSA, we would be able to obtain more in-depth insights.
Additionally, for the purposes of this paper, we introduce the term aspect-based sentiment quantiﬁcation (ABSQ), a ﬁeld at the intersection of SQ and ABSA./C15The authors are with Econometric Institute, Erasmus University Rotter- dam, 3062 Rotterdam, PA, Netherlands.
In this paper, we aim to carry out ABSA on the sentence level.
To illustrate this, we look at the F1score, considered to be a common measure for classiﬁcation algorithms F1¼2/C1TP 2/C1TPþFPþFN; (1) where TPis the number of true positive predictions, FPrep- resents the number of false positives, FNis the number of false negatives, and TNequals the number of true negatives.
However, since the ﬁrst two meth- ods have been much more widely used in academic litera- ture, we will consider “ Classify and Count ” and its adjusted variant as baselines in this paper.
In this paper, we have opted for ter- nary sentiment quantiﬁcation, with sentiment classes posi- tive, neutral, and negative.
In particular, we use task 5 of SemEval 2016 [36].
Most importantly, we delete those observations where the target is implicit (equal to NULL).NULL values represent less than 10% of all observations and therefore represent a compara- tively rare category.
With data from each of these catego- ries, we will also train an aspect-speciﬁc QuaNet model, called AspEntQuaNet.
For the four categories under consideration, we also pro- vide the distribution of review sentences among sentiment classes (positive, neutral, or negative).
In Table 3, we see the distribution of sentiment classes for the observations in the training and test data.
For SERVICE #GENERAL, we ﬁnd a total absolute difference (TAD, the sum of absolute differences in the true prevalen- ces of each sentiment class between training and test data) equal to 4.54%; for RESTAURANT #GENERAL, we ﬁnd a TAD of 5.48%.
Namely, for FOOD #QUALITY, we obtain a TAD of 24.46%, and for AMBIENCE #GENERAL, there we ﬁnd a TAD equal to 26.92%.
4.1.1 LCR-Rot-Hop++ Neural Network To utilize any of the quantiﬁcation methods that are investi- gated in this paper, we ﬁrst need to incorporate a classiﬁer.
For this purpose, we will utilize the deep learning compo- nent of a state-of-the-art Hybrid Approach for Aspect-BasedSentiment Analysis (HAABSA) which was ﬁrst developed by [40] and further improved by [18] (under the name of HAABSA++).
It has to be noted that considering the fact that for Qua- Net we will need to obtain document embeddings and sort them in the order of increasing probabilities (as will be dis- cussed in detail in Section 4.2.6), we can only use the second part of HAABSA++ for this method (LCR-Rot-hop++).
Therefore, to ensure a fair comparison with the other meth- ods, we will omit the ontology part completely.
Additionally, we have to notice that in the research work of [18], there are four ways of implementing hierarchical attention.
In the end, we end up with 1x2400 vectors which are to be used as input for quantiﬁcation methods.
Since we seek to identify the possible best model and build upon this in terms of estimat- ing prevalences, only the BERT word embedding will be used for the purposes of this paper.
Although we have used three sentiment classes in this paper instead of two, this does not form a problem for CC.
Furthermore, in (7) we denote the sentiment clas- ses positive, neutral, and negative by 1, 2, and 3, respec- tively.
In fact, provided the “ true positive rate ” and “ false positive rate ” are known, we could adjust the share to obtain the optimal result.
The idea can be seen in the following equation ^pACC cðDÞ¼^pCC cðDÞ/C0cfprh ctprh/C0cfprh; (8) wherectprh¼TPh TPhþFNhandcfprh¼FPh FPhþTNh, and we have omitted superscript cdue to verbosity.
Essentially, it shows that in the case of known true positive and false positive rates (which in reality are estimated from the training data), we can obtain the true prevalence of a certain class.
Considering that the true rates ctprhandcfprhare unknown in a real-life scenario, we need to estimate them from our data.
The starting point is the following two equations Pð^1Þ¼Pð^1j1Þ/C1Pð1ÞþPð^1j2Þ/C1Pð2ÞþPð^1j3Þ/C1Pð3Þ; (9) Pð^2Þ¼Pð^2j1Þ/C1Pð1ÞþPð^2j2Þ/C1Pð2ÞþPð^2j3Þ/C1Pð3Þ: (10) After rewriting these equations we obtain the following three formulas that represent the adjusted prevalence esti- mations Pð1Þ¼Pð^1Þ/C0ðdf1=2r/C0df1=3rÞ/C1ðPð^2Þ/C0df2=3rÞ bt2r/C0df2=3r/C0df1=3r ct1r/C0ðdf1=2r/C0df1=3rÞ/C1ðdf2=1r/C0df2=3rÞ bt2r/C0df2=3r/C0df1=3r; (11) Pð2Þ¼Pð^2Þ/C0ðdf2=1r/C0df2=3rÞ/C1ðPð^1Þ/C0df1=3rÞ bt1r/C0df1=3r/C0df2=3r ct2r/C0ðdf2=1r/C0df2=3rÞ/C1ðdf1=2r/C0df1=3rÞ bt1r/C0df1=3r/C0df2=3r; (12) Pð3Þ¼1/C0Pð1Þ/C0Pð2Þ; (13) wherect1r¼T1h T1hþF2=1hþF3=1h, df2=1r¼F2=1h T1hþF2=1hþF3=1h, df3=1r¼ F3=1h T1hþF2=1hþF3=1h, ct2r¼T2h F1=2hþT2hþF3=2h, df1=2r¼F1=2h F1=2hþT2hþF3=2h, df3=2r¼F3=2h F1=2hþT2hþF3=2h, ct3r¼T3h F1=3hþF2=3hþT3h, df1=3r¼ F1=3h F1=3hþF2=3hþT3h, df2=3r¼F2=3h F1=3hþF2=3hþT3h.T h el e t t e r rin these formulas stands for ‘ rate ’.
For this reason, we propose a two-step clipping algorithm.
As a result, we obtain the probabilities of each aspect opinion to belong to a certain sentiment class, along with their embeddings (concatena- tions of rl, rtl, rtr, rr).
Concurrently, using the classiﬁed data from the initial step, we calculate prevalences ^pCC cðDÞ, ^pACC cðDÞ, ^pPCC cðDÞ, ^pPACC c ðDÞ, as well as rates ctprs, cfprs, ctnrs, dfnr s. These eight values are then combined in a single vector before being concatenated to the hidden state of the bi-LSTM.
This two-step approach entails that we will ﬁrst train the LCR-Rot-hop++ classiﬁcation model.
First, we concatenate to the document embeddings all three probabilities (each corresponding to a certain senti- ment class) instead of only one.
Next to that, we concatenate the value of entropy calculated from the three given proba- bilities.
In general, we argue that in our case, entropy-based sort- ing is better at treating minority data categories because it takes into account all probabilities at once.
As such, we consider entropy-based sorting to be a ﬁtting option for the sentiment quantiﬁcation task under consideration.
Considering the advancements with the proposed entropy-based sorting and the extension of three sentiment categories, we call this new model EntQuaNet.
Additionally, to make sure EntQuaNet indeed improves upon QuaNet for the ABSQ task in this paper, we train the ordinary QuaNet on the whole training set without includ- ing the value of entropy as a separate variable.
Because we consider three probabilities instead of two, it is unclear which probability should be chosen for sorting.
Then, in case the probabilities of at least two opinions belonging to the positive class were equal, we employed a similar sorting based on the probability of opin- ions belonging to the negative class (second-most common).
To account for the ABSQ task and to see that the aspect restriction of the training sample improves the performance of the quantiﬁers, we select the four most prevalent aspect categories (FOOD #QUALITY, SERVICE #GENERAL, AMBI- ENCE #GENERAL, RESTAURANT #GENERAL), and train separate EntQuaNets for each of them.
In our case, for each of the aspect categories as well as the full dataset, we create sub- datasets by sampling without replacement the opinions from the total or category datasets.
In this paper, we chose this size as approximately 40% of the test dataset size.
For eachof these subdatasets, we calculate the corresponding quantiﬁ- cations of size 1/C23.
For testing, we have opted to determine the performance over a constant set of 100 randomly generated subdatasets.
Additionally, taking into account that the RAE and KLD may become numerically unstable in case the true preva- lence of one of the categories is equal to zero, we make sure there is at least one opinion from every sentiment class in each of the subdatasets.
5R ESULTS In this section, we discuss and compare results obtained from all previously mentioned ABSQ models and methods.
Hence, to opti- mize performance, we employ an RGS to ﬁnd the optimal con- ﬁguration of hyperparameters.
For a fair comparison of results, we compared the models from RGS on a validation dataset comprising 20% of the train- ing dataset from each of the aspect categories.
Considering the characteristics for each of the m odels, displayed in Table 4, we have identiﬁed the generally be st-performing optimizer for this problem to be Adam and the best-performing loss func- tion to be Mean Absolute Error (MAE), calculated over the constant number of test subdatas ets.
Second, as discussed previously, ACC and PACC c a ni nt h e o r yr e t u r np r o b a b i l i t y values that are lower than zero or higher than one (for this reason, [23] proposed a clip- ping procedure for binary data, which we extended to be usable for ternary data).
First, we have run CC on it and obtained the following predic- tions for, respectively, positive, negative, and neutral classes: [0.893333, 0.106667, 0].
For ACC, we obtain for this subdataset: [0.978551, 0.11480645, -0.09335745].
In this case, we observe that PACC again underper- forms on all three metrics.
In this case, we found ordinary CC to perform best according to RAE; for the AE and KLD performance measures, AspEntQuaNet proved most effective once more.
6C ONCLUSION In this article, we proposed EntQuaNet, an aspect-based sentiment quantiﬁcation (ABSQ) method built upon LCR- Rot-hop++, a state-of-the-art classiﬁcation architecture for the task of aspect-based sentiment analysis.
To answer this question, we carry out an empirical study on four representative CER models by a context-replacement methodology.
To tackle the problem, we propose a semantic-guided contrastive context-aware CER method, namely C3ER, to augment/regularize a backbone CER model, which can be any neural CER framework.
In this paper, we divide them into three types: (1) the emotion labels of context utterances, (2) the semantics car- ried by the actual content of utterances (e.g., topic or dialogue intent), and (3) the relationship between speakers, i.e., intra/ inter-speaker inﬂuence.
For the convenience of presentation, we refer the ﬁrst two collectively to as semantic context, and the third as structure context.
In order to ﬁll this gap, we conduct an empirical study on four representative CER methods to further explore what the models actually learn from the conversational context, which are reported in more detail in Sections 4 and 6.6.
Speciﬁcally, for each target utterance, we replace its context utterances with different content that yet carry the same emotions as the original ones.
To alleviate the problems mentioned above, we further propose a semantic-guided context-enhanced mechanism to regularize a CER model and facilitate a more effective understanding of conversational context.
In this sense, we call u4as ” context-relevant ”.
In this paper, we incorporate the above ideas into a con- trastive learning scheme, and propose a contrastive context- aware CER method, namely C3ER, to augment a CER framework.
Then we take the target utterance itself and its subsequent ones within a certain proximity as the context-relevant (positive) samples, and randomly sample context-irrelevant utteran- ces from other conversations in the dataset (termed as “ cross-dialogues ”) as the negative samples, to construct contrast pairs.
According to its historical utterances u1tou3, we can infer that the conversation is talking about “ name ”, instead of “ camera ”, “ coffee ” o r “ TV ”.
And then, we choose four repre- sentative CER methods as baselines to conduct a empirical study in Section 4, and describe the details of the proposed method in Section 5.
Finally, we con- clude the paper in Section 7 and discuss the promising directions for future research in Section 8.
To the best of our knowledge, we are the ﬁrst to use con- trastive learning in CER.
Different from the existing CER models which mainly focus on developing new neural net- work structures for performance improvement, we create a semantic context modelling method using contrastive learn- ing to enhance a CER model, which can be incorporated into and jointly learnt with any existing CER framework to improve the classiﬁcation accuracy and robustness.
In order to better illustrate our method, we ﬁrst introduce the general structure of typi- cal Neural CER Frameworks.
Instead, we propose a method based on context- replacement to indirectly infer what a model has learned from the conversational context by changing the input of the model.
Concretely, we aim to observe the classiﬁcation performance of the trained CER models under the setting of deliberately modiﬁed conversational context of each target utterance.
To do so, we have designed three different con- text replacement methods.
(2) Random Modiﬁcation (RM): Sim- ilarly, we need to replace the conversational context for each target utterance.
(3) Altering Modiﬁcation (AM): As a more extreme case of RM, we replace conversa- tional context for each target utterance with the ones which have exact different emotion labels and conversational context.Fig.
Then, we test the performance of these models, under the different context replacement modes, on the widely used IEMOCAP dataset [20] which contains dyadic conversations among 10 speakers and is proved to sensitive to conversa- tional context [23].
In the next sections, we will address this problem by pro- posing a semantic-enhanced regularization method to aug- ment the CER models via contrastive learning and joint training.
Based on the assumption that consistency of semantic context tends to be maintained in the conversational ﬂow, we construct positive (relevant) and negative (irrelevant) contrast samples from both forward and reverse directions.
In this part, we design a contrastive loss to push a target utterance ’ s context semantically closer to its relevant utterances but away from irrelevant ones.
For a full utilization of the backbone CER mod- el ’ s ability and ensuring the adaptability of the C3ER method, we directly take the utterance-level hidden sequence from the CER model, as input to C3ER.
In other words, we injecting the C3ER module onto the classiﬁcation head of the CER model.
The basic idea is that, for the conversational context of a tar- get utterance, we can identify its relevant utterances as posi- tive examples and its irrelevant utterances as negative example.
For a target utterance ’ s hidden state hk, we con- struct contrast pairs from two directions.
For the forward direction, we use a forward GRU/C131/C131/C131! model to encode the historical utterances of hkas the context representation c!
Therefore we construct the context and its relevant utterances as positive pairs, formalized as P!
Similarly to the above procedure, we use a backward GRU /C131/C131/C131 to encode the future context of the target utterance hk, a n d obtain a backward direction context representation c k.T h e n, the corresponding backward positive pairs set P kis con- structed.
Now, we get all positive pairs for the target utterance Pk¼fP!
kand c kdescribed earlier, we can obtain irrelevant utterances randomly sampled from the cross-dialogues to construct negative pairs, formalized as N!
For each utter- ance ’ s hidden emotional representation hk, its contrast pairs can be represented as Dk¼fNk; Pkg: (10) In this section, given the hidden emotional sequences extracted from the backbone CER model, we construct a series of context-irrelevant/relevant contrast pairs for each utterance.
In the next section, we will use them within the contrastive learning setting to regularize the CER model and force the model to learn the characteristi cs of these semantic patterns.
5, given a contrast pair sample, we ﬁrst design a matching network to calculate two semantic matching scores, one for the positive pair (i.e., between the context representation and the relevant utterance) and the other for the negative pair (i.e., between the context repre- sentation and the irrelevant utterance), and then build a contrastive loss based on the scores.
Given a pair dj¼ð~cj; ~ujÞselected from Dk, we ﬁrst concatenate the vector representations of a contrast sample, and then a MLP (Multi-layered Perceptron) network is used to compute the matching scores for the pair oj¼mlpð½~cj/C14~uj/C138Þ: (11) In our method, we use a 2-layer MLP, which inputs the concatenated vector and output a real value as the matching score.
As for AGMHN [9] and its variants, we also fol- low the data pre-processing method provided in the original source code of the model2.
In the same way, we use the pre- trained features provided by the open source3for DAG [15].
With respect to the modeling direction (i.e., forward or backward) of the contrast pair construction process, we con- sider two variations for C3ER: namely Uni-C3ER and Bi- C3ER.
In addition, we conduct the sig- niﬁcant test for each CER model with/without the Bi-C3ER module on IEMOCAP and MELD, the statistical signiﬁcance result is reported by permutation test with p <0:1.
During our experiments, we only choose 8 and 16 as batch size.
(2) Given the improvement over DAG, we have achieved a new SOTA performance on both IEMOCAP and MELD.
4.We also implemented the DialogueGCN [13], but we ﬁnd it easy to collapse during the model training phrase, so we do not include it as our baselines.1886 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL.
we explore how the regulariza- tion coefﬁcient /C21affects the emotion classiﬁcation perfor- mance.
With the increase of /C21, F1-TABLE 3 Performances on IEMOCAP Dataset Model Happy Sad Neural Angry Excitd Frustrated Avg P R F1 P R F1 P R F1 P R F1 P R F1 P R F1 Acc F1 bc-lstm 41.0 34.7 37.6 69.4 66.5 67.9 51.6 53.9 52.7 60.0 67.1 63.3 66.4 56.2 60.9 56.2 62.2 59.0 57.8 57.9 +Uni-C3ER 37.3 26.4 30.9 67.3 69.8 68.5 53.8 53.1 53.5 60.1 64.7 62.3 63.4 67.2 65.3 57.2 58.3 57.7 58.2 57.8 +Bi-C3ER 37.7 41.7 39.6 68.8 71.0 69.9 53.5 53.6 53.6 66.9 59.4 62.9 66.2 52.5 58.6 58.2 67.0 62.3 58.7 {58.7 {1.6%" 1.4%" DialogueRnn 52.4 30.6 38.6 86.9 64.9 74.3 54.9 57.8 56.3 69.0 64.1 66.5 69.6 74.2 71.8 56.4 70.3 62.6 63.1 62.9 +Uni-C3ER 53.6 25.7 34.7 53.6 25.7 83.7 59.1 60.9 60.0 68.2 60.6 64.2 64.9 89.6 64.9 61.8 53.5 57.4 65.2 64.0 +Bi-C3ER 55.9 26.4 35.8 89.7 78.4 83.7 63.7 58.1 60.8 64.1 68.2 66.1 65.1 90.3 75.6 59.2 61.4 60.3 66.1 {65.2 {4.8%" 3.7%" AGMHN 50.3 53.9 52.0 81.6 66.9 73.5 49.6 58.6 53.7 69.5 61.8 65.4 75.6 55.9 64.2 56.1 65.1 60.1 60.8 61.3 +Uni-C3ER 50.9 38.46 43.8 81.5 64.9 72.3 50.1 58.1 53.8 82.1 56.5 66.9 76.2 62.5 68.6 54.8 73.5 62.8 61.9 61.7 +Bi-C3ER 55.0 38.5 45.3 80 63.7 70.9 54.0 64.1 58.6 74.6 60.6 66.9 72.8 69.9 71.3 58.1 68.0 62.6 63.4 {63.4 {4.3%" 3.4%" DAG 43.0 36.4 39.4 83.7 77.6 50.5 63.6 76.6 69.5 71.7 64.1 67.7 70.8 64.9 67.7 68.9 69.8 69.4 68.1 67.9 +Uni-C3ER 48.8 42.0 45.1 79.1 80.4 79.8 65.7 74.2 69.7 69.6 64.7 67.1 73.1 68.2 70.6 69.1 68.8 68.9 68.9 68.8 +Bi-C3ER 50.0 41.3 45.2 78.0 81.2 79.6 66.4 74.5 70.2 68.2 68.2 68.2 70.9 69.2 70.1 71.6 67.0 69.2 69.2 {68.9 {1.6%" 1.5%" It shows the overall performance of the four baselines and two variants Uni-C3ER and Bi-C3ER, which we added on the basis.
In order to better con- trol the variables, we select the nearest 3 utterances adjacent to the target as the positive samples, and set the batch size values for all CER methods to 8.
A similar trend also holds for the MELD dataset.6.6 Perturbation Test In order to ﬁgure out where the performance gain of the C3ER comes from and quantify the inﬂuence of C3ER on the backbone CER framework ’ s performance, we carry out a series of perturbation tests on IEMOCAP dataset, which is known more sensitive to contextual information.
6.6.1 Test Setting Since it is hard to quantify the context semantic consistency between target utterance and its conversational context, we follow the context-replacement approach used in our empir- ical study Section 4, to indirectly validate the performance of C3ER through perturbation tests by replacing the conver- sational context of the target utterance under different con- text-replacement modes.
Furthermore, we chose 5 different ran- dom seeds to test results and report the average F1-score.
According to the observations we have made in the empirical analysis of the baseline models (see Section 4), we divide them into two types, called Label Copying Approaches (i.e., BC-LSTM, DialogueRnn and AGHMN) and Non-label Copying Approaches (i.e., DAG) respectively.
For the baseline DAG [15], we observe an interesting phenomenon that the perturbation no matter under EM, RM, AM operation, has less signiﬁcant inﬂuence on its emotion classiﬁcation performance com- pared to other three baselines.
According to the principles and framework structure of DAG, we know that DAG mainly focuses on the construction of structural information of a conversation, that is, the relationship between speakers established by a directed acyclic graph.
Therefore, in order to ﬁgure out where the gains come from, we do a further ablation experi- ment and design 3 different modes to break the conversa- tional context for DAG.
For the “ structure ” mode, we take randomly replace the edges constructed by the DAG and speaker information as the destruction of the structural con- text of the dialogue.
In a summary, we have conducted a quantitative analysis of how different CER models use the conversational contex- tual information.
6.7 Case Study In order for a more intuitive understanding of the effect of our method, we select an example conversation from IEMO- CAP as a case, to show how C3ER inﬂuences its attention in comparison with DialogueRnn, which correspond to a large performance improvement.
Speciﬁcally, we analyze the attention distribution of the two models (i.e., DialogueRnn with and without C3ER) in recognizing the last utterance of the conversation.
7C ONCLUSION In this paper, we ﬁsrt conduct an extensive empirical inves- tigation of the effect of conversational context on the perfor- mance of conversational emotion recognition models.
The results reveal that the representative CER models we have studied tend to overﬁt certain individual aspects of context, e.g., the emotion labels and intra/inter-speaker structures, but lack a holistic understanding of the conversational con- text, specially semantic context.
To solve this problem, we proposed a semantic-guided regularization method, namely C3ER, which can be inte- grated into a baseline CER model via contrastive learning and joint training.
In the future, we will es tablish more quantitative analysis mechanisms for each inﬂuencing factor, rather than just heuristic exploration.
Based on the ﬁndings, we can construct an holistic emotion decision-making frame- work that considers a range of factors to improve the per- formance of CER.
Using the inconsistency as an indicator of sample informativeness, we further propose an inconsistency-based multi-task cooperative learning framework that integrates multi-task active learning and self-training semi-supervised learning.
In MDEE, we consider emotion estimation in the 3DValance-Arousal-Dominance space.
Given an emotional sample that is predicted to have low valence, high arousal, and low domi- nance, e.g., a sample with fearemotion, we can calculate its label inconsistency in the Dominance dimension using the labeled dataset, based on its estimated labels in the other two dimensions.
First, we identify the labeled samples that have similar Valence and Arousal values with this sample and check their Dominance labels.
Similarly, we can obtain its label inconsistency with the labeled data- set in Valence and Arousal.
Aggregating the label inconsis- tency in all three dimensions, we obtain the sample ’ s total inconsistency with the labeled dataset.
For SECE, we measure the label inconsistency differently, since additional categorical labels are available.
Then, the given sample is inconsis- tent with the category-conditional label distribution in Arousal but consistent in Valence and Dominance.Based on this informativeness measure, we further pro- pose an inconsistency-based multi-task cooperative learning (IMCL) framework that integrates AL and SSL.
2) Based on the inconsistency measure, we further pro- pose IMCL, a multi-task cooperative learning frame- work that integrates AL and SSL.
In MDEE, T¼f Valence estimation, Arousal estimation, Dominance estimation g, and in SECE, T¼f Emo- tion classiﬁcation, Valence estimation, Arousal estimation, Dominance estimation g. The data pool consists of a small number of labeled samples XL¼fðxxxxxxxL i; yyyyyyyL i; TÞgNL i¼1and a large number of unlabeled samples XU¼fxxxxxxxU jgNU j¼1, w h e r e xxxxxxxi2 Rd/C21is a d-dimensional feature vector, and yyyyyyyi2RjTj/C2 1its jTj-dimensional label vector corresponding to the tasks in T. 2.2 Emotion Recognition Model ft For each task t2T, we train a ridge regression (RR) model for emotion regression, or a logistic regression (LR) model for emotion classiﬁcation, ftðxxxxxxxÞ, o nfðxxxxxxxL i; yL i; tÞgNL i¼1, u s i n gt h e original features as inputs to estimate the labels of the unlabeled samples.2018 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL.
For an unlabeled sample xxxxxxxU j, its estimated label vector is ^yyyyyyyU j; T¼^yU j; 1; ...; ^yU j; jTjhi ¼f1ðxxxxxxxU jÞ; ...; fjTjðxxxxxxxU jÞhi: (1) 2.3 Conditional Label Distribution Model gt To represent the label distribution of the labeled dataset in TasksTdis, we also construct a set of models fgtgt2Tdis.F o ra Task tinTdis, gttakes the labels of the related tasks Trel tas inputs and outputs the conditional label distribution in Task t. In MDEE, we estimate the conditional label distribution of each task from the remaining ones, i.e., Tdis¼T¼f Valence estimation, Arousal estimation, Dominance estimation gand Trel t¼fTn tg.gtadopts k-nearest neighbors (kNN) regressor with k¼5, which takes the estimated labels of an unlabeled sample xxxxxxxU jinTrel t, i.e., ^yyyyyyyU j; Trel t, as inputs, ﬁnds its nearest labeled neighbors in the label spaces of Trel t, and averages their labels in Task tas the conditional task label ~yU j; t, i.e., ~yU j; t¼gt^yyyyyyyU j; Trel t/C18/C19 ¼kNN ^yyyyyyyU j; Trel t/C18/C19; t2Tdis: (2) In SECE, the conditional distributions of the dimensional emotions are computed from the estimated categorical emo- tion probabilities ^yyyyyyyC, i.e., Tdis¼fValence estimation, Arousal estimation, Dominance estimation g, andTrel t¼f Emotion classiﬁcation g. We obtain the conditional task labels of the dimensional emotions from the estimated cate- gorical emotion probabilities ^yyyyyyyCthrough gð^yyyyyyyCÞ.
It includes 1,182 images, but we removed four duplicate ones.
3.3 Implementation Details In MDEE, we used Ridge Regression (RR) as the base model in each dimensional emotion estimation task.
In SECE, we used a Logistic Regression base model for emotion classiﬁcation, in addition to the three RR base mod- els for Valence-Arousal-Dominance estimation.
Here we set both pin SECE and ttin MDEE empirically to a median value.
3.4 Performance Evaluation For the two speech datasets, we consider the cross-speaker scenario, i.e., the speakers in the training set do not overlap at all with those in the test set.
Finally, we used these 30% /C1947 samples as the training set and the remaining 70% /C1947 samples for test to validate the performances of dif- ferent algorithms.
Each time we used two sessions as the training set and the remaining three as the test set, and then switched the training and test set, resulting in 10 different training-test partitions in total.
For IAPS, the image dataset, we randomly selected 30% of the samples as the training set, and the remaining 70% as the test set, and repeated this process 100 times.
Then, we iteratively selected samples from the unlabeled data pool by different algo- rithms to annotate, and updated the emotion recognition models accordingly.
As the dimensional emotions are more ﬁne-grained and contain richer information, we mainly focus on the perform- ances of the dimensional regression tasks.
Experiments in MDEE were carried out to analyze their inﬂuence.4.3.1 Effect of Weight a To analyze the sensitivity of IMCL to the weight ain the calcu- lation of the pseudo-labels, we ﬁxed each tttoet=2and con- ducted experiments for a¼f0:1; 0:5; 0:9g.
3) In our experiments, we ﬁxed the hyper-parameters a and set the sample selection rules in SSL heuristi- cally.
In this paper, we create a multimodal decision system for three level mania classiﬁcation based on recordings of the patients in acoustic, linguistic, and visual modalities.
The system is evaluated on the Turkish Bipolar Disorder corpus we have recently introduced to the scientiﬁc community.
Using acoustic, linguistic, and visual features in a multimodal fusion system, we achieved a 64.8% unweighted average recall score, which advances the state-of-the-art performance on this dataset.
In this paper, we work with data collected from such patients encompassing manic, hypomanic, and remission stages.
In this work, we propose a multimodal machine learning system that uses information from acoustic, linguistic, and visual modalities to classify the bipolar patients into remis- sion, mania and hypomania classes.
We evaluate our proposed multi- modal approach using the Turkish Audio-Visual Bipolar Disorder corpus that we have recently collected and made available to the research community [3], [14], and push the state-of-the-art performance achieved on the corpus so far.
2R ELATED WORK In this section, we ﬁrst brieﬂy summarize the main ﬁndings in the related area of multimodal depression analysis.
Then we describe our dataset, before moving to a more technical exposition of speciﬁc works on BD estimation.
2.2 The Turkish Audio-Visual Bipolar Disorder (BD) Corpus In this paper, we use the Turkish Audio-Visual Bipolar Dis- order Corpus [14] 1to report experimental results.
Before discussing the related work performed on this corpus, we provide some details about the data.
In our experiments, we have adhered to the 2018 AVEC Bipolar Disorder and Cross-cultural Affect Recognition Competition [3] protocol to ensure comparability of results with the literature.
In this section, we summarize the feature extraction and machine learning approaches that were used for the mania level estimation problem.
In the next section, we present a tri-modal system that advances the state of the art in this problem.
Thus, we conducted comparative unimodal experiments to choose the optimal sets of features in each modality.
Subsequently, we perform acoustic analysis of the patient interviews.
For acoustic feature extraction, we use the openSMILE fea- ture extraction toolkit [32], which provides many built-in con- ﬁguration ﬁles that extract the baseline audio features from INTERSPEECH and AVEC challenges, and some parameter sets proposed for voice research and affective computing studies on audio.
In our experiments, we use eGeMAPS [33] (Extended Geneva Minimalistic Acoustic Parameters Set), which is a parsimonious set of audio features, chosen for their ability to represent affective physiological changes in voice production.
23 eGeMAPS low level descriptors (LLD) are summarized using the functionals from the original eGe- MAPS conﬁguration [33], and this set is enriched by 10 func- tionals we have added [14] (see Table 2 for the entire set).
Consequently, we use three alternative fea- ture sets for the linguistic experiments, which are linguistic inquiry and word count (LIWC) [44], term frequency-inverse document frequency (TF-IDF), and polarity features.
For the visual experiments, we use facial action units (FAUs), as well as geometric features extracted from each face, provided as baseline features in the AVEC chal- lenge [3].
The set of 23 geometric features we use are based on our early work for video-based emotion recognition in uncon- trolled conditions.
The features we used for the classiﬁcation of the clips are represented as two-dimensional matrices, where columns are the functionals of the low level descriptors and each row contains the feature vector of a clip.
For the row-level, we apply L2normalization, which effectively transforms a linear ker- nel into a cosine similarity kernel.
Considering the sample size of the BD dataset, we consid- ered reducing the feature dimensionality with feature selec- tion.
In our experiments, we employ a Kernel Extreme Learn- ing Machine (Kernel ELM) classifer [56].
In weighted ELM [34], which is a variant that is used with class imbalanced problems, we deﬁne a N/C2Ndiago- nal weight matrix W. Each diagonal element stores the mul- tiplicative inverse of the number of training samples Niof the corresponding class i.
To reach the best performance, we investigate a weighted decision level fusion approach: Pfusion ¼aPunweighted þð1/C0aÞPweighted; (3) where Pis an N/C2tmatrix that contains the class probabili- ties of each sample.
For the multimodal systems, we use the best performing unimodal models and features.
First, we consider two late fusion methods, namely, majority voting and weighted sum.
For the fusion of three modalities, we apply a variant of Equation (4).
While selecting the fusion models to test, we considered the Multimodal 1 (MM1) metric [59], which measures the improvement in the ﬁnal fusion model: MM 1¼UAR fusion /C0maxðUAR 1; UAR 2; UAR 3Þ maxðUAR 1; UAR 2; UAR 3Þ; (7) where UAR fusion is the UAR score of the fusion model, UAR i are the UAR scores of the unimodal models.
While calculat- ing the MM1 score, we use 4-fold cross-validation scores, since it gives more robust results.
After getting the test set results for the selected fusion models, we calculate MM1 scores using test set UARs.
3.6 Shapley Additive Explanations To further investigate the contribution of features for classi- ﬁcation, we use the SHAP method [60], which aims to pro- vide an explanation to a particular prediction by means of2124 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL.
In all tables, we denote tree-based fea- ture selection with a star.
4.1 Unimodal Experiments 4.1.1 Audio Classiﬁcation For the clip level audio classiﬁcation, we used eGeMAPS and IS10 feature sets, which are extracted using the open- SMILE feature extraction toolkit [32].
Since better fea- ture extraction tools are available for the English language compared Turkish [50], we use Google Translation engine to process the text in English.
4.2 Fusion of Modalities After investigating unimod al performances, we perform multimodal fusion experi ments using weighted sum, majority voting, and feature fusion methods as explained in Section 3.5.
Mainly, we select the feature sets that per- formed well in the unimodal experiments and use them in multimodal fusion.
For the acoustic modality, we select eGeMAPS10 and eGeMAPS feature sets, since the eGeMAPS feature set is created speciﬁcally for the affective paralinguistic tasks, and may provide better intelligibility compared to the IS10 feature set.
With this setup, we achieve 64.8% UAR score, which is 5.5% (absolute) higher than the best result published on this challenging corpus (see Table 6).
For this, we randomly generated predictions for each modality and combined them with the true predictions of from the other modalities via majority voting.
In System 2, the linguistic modality has the highest contribution.4.3 Choice of Classiﬁer In earlier sections, we have compared the unimodal per- formances of the acoustic, linguistic and visual features used in the proposed pipeline.
In order to motivate the choice of Kernel ELM in the proposed system, we compare it with Ordinal Multi-Class SVM (OMSVM) [64] classiﬁer with the same set of features and optimization procedure as in the Kernel ELM.
4), we observe that the multimodal system improves the recall performance of the hypomania class on both 4F-CV setting and the test set.
4.4 Interpretability Analysis In order to provide insights into the decision making pro- cess of the multimodal system, we carried out two sets of experiments.
4.4.1 SHAP Based Interpretability Analysis Since the top multimodal systems are based on decision fusion rather than feature fusion, we analyse each modality- speciﬁc model separately using Kernel SHAP [60], a model- agnostic explanation method.
To provide insight into our trained models, we aver- aged the absolute SHAP feature attributions corresponding to the predicted class over the test set.
Analyzing the plots for eGEMAPS related acoustic fea- tures, we observe a large difference in the top features, due to the additional functionals used in the proposed eGE- MAPS10 set over the set of LLDs (see Table 2).
However, in both plots we observe the dominance of formant related supra-segmental features among the most inﬂuential fea- tures.
In both plots, we observe Harmonics-to-Noise Ratio (HNR), although sum- marized with different functionals.
Jitter and shimmer, two other voice quality features, are also commonly associated with mood disorders, mainly with depression [4], and we observe them among the inﬂuential features.
When we look at the FAU features in Fig.
5, we see that AU12 (lip corner puller) is the most prominent facial fea- ture, which is an important feature for capturing mirth, as it is activated in smiling and laughter.
For the LIWC features, we have an interesting pattern, with the “ Religion ” category showing a large feature impor- tance.
When we investigate the transcripts that show high value for this category, we notice several manic patients producing an incoherent discourse intermingled with heavy religious terminology.
While some of these items are can be predicted from acoustic, linguistic and visual features/ correlates, some involve evaluation of the clothing choice (YMRS10-Appearance) or medical expert ’ s insight (YMRS11-Insight), which we cannot model directly using the modalities and features in question.
Therefore, we set up the following experiment in order to have an insight on the extent a modality/feature set used in the proposed pipeline can predict activity in these items.
For the sake of uniformity of treatment and ease of interpretation, we binarized each YMRS item score yiat a threshold of 0 to indicate the existence of activity aiin the respective item: ai¼Iðyi> 0Þ, w h e r e IðÞis the indicator function.
Looking at the 4F-CV UAR performances, on the overall we observe the dominance of acoustic models ’ recognition of item activity (seven out of 11 items).
When we use the ground truth binarized YMRS item activity labels to predict mania levels, we obtain a 4F-CV UAR performance of 83.5% and a corresponding test set UAR performance of 72.2%.
However, we are cautious not to over- interpret the results, due to the low number of samples, which limits the successful application of regression on original YMRS item scores.
5D ISCUSSION AND CONCLUSIONS In this paper, we investigated mania-level classiﬁcation (mania, hypomania, remission) of bipolar disorder (BD) patients using the Turkish Audio-Visual BD dataset, and proposed a trimodal architecture.
The accuracy results we have obtained are not high enough to use the proposed system in a real-world clinical application as a decision support system for the clinician.
For the clinician, this may inform the diagnosis, but for an automatic system, it is difﬁ- cult to take such features into account, and for the standard assessment methodology we use in this paper to ensure comparability, these cases cause issues.
First, we use automatic transcription, which is prone to errors, as Turkish is not a well-studied language for automated speech recognition.
For a fair and direct comparison with the works presented in the challenge, we have strictly adhered to the challenge protocol in this work, and did not use manual translation.
In this study, we opted for a compact set of interpretable features in each modality and we analyzed the models to gain insights into the inﬂuential features in the decision-making process.
To provide further insights into the capability of the used feature sets in symptomatic mania clas- siﬁcation, we conducted item-wise YMRS activity modeling.
It is crucial to note that AI systems similar to the one we have proposed in this paper use a very limited set of sources in their assessment compared to the clinician, and are pri- marily statistical (as opposed to causal) in their nature.
The Turkish Audio-Visual BD dataset we have opened to the research community is the ﬁrst dataset including audio, visual, and text modalities in this area, and we hope it will foster the development of richer analysis tools for helping clinicians.BAKI ET AL.
The Arousal Video Game AnnotatIoN (AGAIN) Dataset David Melhart, Antonios Liapis, and Georgios N. Yannakakis, Senior Member, IEEE Abstract— How can we model affect in a general fashion, across dissimilar tasks, and to which degree are such general representations of affect even possible?
Motivated by the lack of corpora for the study of general properties of affect across tasks and participants, in this paper we introduce The Arousal video Game AnnotatIoN (AGAIN) dataset, which contains data from over 120 partic- ipants who played and annotated over 1,000 gameplay ses- sions.
While the Circumplex model and the PAD model repre- sent affect across two and three dimensions, respectively, in the AGAIN dataset we focus currently on soliciting annota- tions based on the dimension of arousal.
In this section we review representative affective corpora that rely on audiovi- sual elicitors and discuss the contribution of AGAIN to the current list of datasets that are enriched with affect labels.
The affective datasets we survey appear to be rather split in terms of annotation type used.
By focusing on a core affect dimension (i.e., arousal) instead of task-related complex emotional outcomes, we aim to make the dataset more rele- vant to traditional AC research.
Finally, we choose to record continuous unbounded traces of arousal using RankTrace [8] via the PAGAN online annotation framework [7].
5.2 Participants Through the procedure presented in Section 5.1, we col- lected data from 124 participants4which include 1,116 Fig.
Inspired by advances in machine learning with privileged informa- tion [56], [57] we view telemetry data as privileged informa- tion and we include such ad-hoc features in the dataset.
Table 3 shows the number of features we have extracted per game with the zero-variance features removed.
To preserve this subjectivity encoded in the annotation, we apply data transformation (i.e., normalisation and pairwise transformation) based on individual sessions.
5.6 Data Cleaning To ease any subsequent analysis and future studies based on the dataset, in this section we propose a preprocessing pipeline which removes 10.8% of the dataset as outliers.
Since PAGAN only records annotations when there is a change in the signal and the Unity engine loop is affected by hardware performance, as a ﬁrst step we resample the whole dataset at 4Hz to get a consistent signal.
[55], we apply Dynamic Time Warping (DTW) to clean the dataset of irregular anno- tations.
We apply the cumulative DTW distance as a similarity measure between arousal traces, in order to remove irregular annota- tion patterns; we do not transform any of the signals.
As a ﬁrst step in the cleanup process, we calculate the cumulative DTW distance to an artiﬁcial ﬂat baseline (arousal annotations at 0 in all time windows).
Since games in the dataset are quite short and players encounter similar situations, we assume that their experience would be somewhat similar.
Therefore, we remove sessions where the annotation traces are too far from other traces in the dataset.
To this end, we apply the cumulative DTW distance metric between each datapoint and sum up the resulting distances.
This last step removes annotations which are too dissimilar from the general trends of partic- ipants ’ annotations; we presume that either the annotation was improper or that this session ’ s elicitor was somehow not in line with how other players played the same game.
6.2 Preliminary Arousal Models In this section we provide an initial modelling approach for the AGAIN dataset, serving as a baseline study for future research with this dataset.
As a preliminary step, we process the clean AGAIN dataset to predict arousal.
To this end, we split the annotation traces into 3-second time windows— computing the mean of the window—and introduce a 1-sec- ond lag to the annotation trace.
Here we chose a 1-second lag to con- form to the aforementioned body of research.
While in the published dataset both clean and raw data is available for the application of different machine learning techniques, we treat arousal modelling in AGAIN as a pref- erence learning task [9], [10], [75] and focus on predicting arousal change from a 3-second time window to the next.
During this transformation we observe consecutive datapoints within sessions in pairs and create a new repre- sentation of the dataset.
For every ðxi; xjÞ2Xpair of game data we observe the relation- ship of their affect output ðyi; yjÞ2Y.I fyiis preferred to yj, we can label the distance between the corresponding data points (xi/C0xj) and 1.
Conversely, we can label the reverse of this observation (xj/C0xi) as 0.
While either one of these observations is sufﬁcient to describe the relationship between xiand xj, by keeping both observations (/C21xi/C0xj¼1 and /C21xj/C0xi¼0), we can maintain a 50% baseline accuracy in the post-transformation dataset independently of the trends in the dataset before the transformation.
To reduce experimen- tal noise from trivial changes within the arousal trace, we omit all consecutive time windows between which the arousal change is less than 10% of the total amplitude of the session ’ s arousal value.
In this paper we are using the RF implementation in the Scikit-learn Python library [78].
For controlling overﬁtting we set the number of estimators in the RF to 100 and the maximum depth of each tree to 10.
This experimental setup is meant to provide a simple baseline prediction performance for the dataset, and thus we are not tuning the hyperparameters of the algorithm.
Because RFs are stochastic algorithms, we run each experiment 5 times and we report the 10-fold cross validation accuracy.
Looking at individual games across different feature sets, we observe that the general features manage to perform comparably to the speciﬁc features independently of the game tested.
In this section, we discuss some of the limitations and propose avenues for future work, before concluding the paper.
In this work, we train an agent with batch reinforcement learning to generate nods and smiles as backchannels in order to increase the naturalness of the interaction and to engage humans.
In this work, we aim to train an agent for non-ver- bal behaviors (smiles and nods) as backchannels which maximizes user engagement.
3M ETHODOLOGY We formulate the problem of backchannel generation in HRI as a Markov decision process (MDP), which we solve with ofﬂine batch-RL using a human-human interaction dataset as the batch of samples.
In this section we describe our MDP formulation and the Sequential Random Deep Q- Network (SRDQN) algorithm proposed as a batch-RL method to address this problem.
In this work, we follow the method proposed by Rich et al.
Similarly, we extract the connection events from our dyadic human-human interaction dataset.
To describe back- channel events, we use smiles, laughs, nods and head shakes.
Since in our dataset, we do not have objects of interest at which both parties look at, we exclude directed gaze event from our deﬁnition.
Keeping in mind the numeri- cal limitations, we calculate the step-wise WIS estimates over trajectory lengths of 250 samples.
4.2.4 Similarity to Human Behavior The ﬁnal metric we use is the similarity of backchannel behaviour to humans in terms of duration of the back- channel.
The participants we re blind to the backchannel policies implemen ted in the robot.
For the object-oriented analysis, we determine the engage- ment values of each participant using the annotated laughs, smiles, nods, and the online logged values of adjacency pairTABLE 4 Sample Dialog Between Furhat and the Players Furhat [Looks at Player1] Sailor, should we explore around the beach or should we rather go and discover the forest.
Now we are in the forest.
Shall we go towards the river or hunt for animals?
Lastly, in order to highlight the difference between the two types of interaction, we noted the total number of smiles and nods performed by the participants during the interactions.
Since each player experienced both games, and a signiﬁcant difference was observed in the engagement values for the two types of games, we can conclude that the game with the RL policy was perceived as more engaging.
To avoid this problem, we propose SRDQN with constraint as one possible solution for learning a policy that remains close to the dataset policy.
In this work, we propose to address the semantic richness issue in the FER problem, with an emphasis on the granularity of the emotion concepts.
Particularly, we take inspiration from former psycho-linguistic research, which conducted a prototypicality rating study and chose 135 emotion names from hundreds of English emotion terms.
Based on the 135 emotion categories, we investigate the corresponding facial expressions by collecting a large-scale 135-class FER image dataset and propose a consequent facial emotion recognition framework.
To demonstrate the accessibility of prompting FER research to a ﬁne- grained level, we conduct extensive evaluations on the dataset credibility and the accompanying baseline classiﬁcation model.
In this paper, we aim at studying the FER problem on a semantic-rich level.
Different from the previous methods that simply blend or add more emotion classes to enhance the FER quality, we thoroughly exploit the linguistic space and leverage a reasonable lexicon to describe the emotion/C15Keyu Chen, Changjie Fan, Wei Zhang, and Yu Ding are with Netease Fuxi AI Lab, Beijing 100084, China.
Inspired by previous psychological research [7], we extend the recognizable emotions to an exhaustive set, covering 135 English words which can semantically describe most of all distinctive emotional feelings or inner statements of humankind.
Accordingly, we argue that the 135-class emotion model is desirable for semantic-rich FER research.
Based on the 135-class emotion model, we construct a large-scale FER dataset in a labor-free manner.
First, we use the 135 emotion terms as class labels, collect more than one million web images from the internet.
Then, we design an automatic data cleaning process by efﬁciently evaluating the expression consistency of the collected images.
To evalu- ate the label credibility of our categorical dataset, we set up a manual veriﬁcation test in which multiple participants are required to give their judgments on given images and dif- ferent emotion labels.
In this way, we successfully build up theEmo135 dataset, which contains 135 emotion categories and 728,946 facial images in total.
Next, we propose a baseline method to validate the feasi- bility of conducting FER on the semantic-rich representa- tion.
Finally, we make the weight matrices as prior knowledge and inject them into the recognition network training softly.
3M ETHODOLOGY In this section, we ﬁrst review the background of our lever- aged emotion model, which contains 135 lexicon terms rep- resenting the semantic atlas of the emotion domain.
Then we introduce the data acquisition and processing details that help us construct a large-scale facial image dataset.
Finally, we propose a baseline approach for ﬁne-grained facial emotion recognition by considering the cross-label relationships among the multiple emotions.
Therefore, in this paper, we refer to the 135 emotion words/categories as the semantic-rich emotion representation.
3.2Emo135 Dataset With the semantic-rich emotion representation, we establish a facial image dataset Emo135 according to the 135 emotion words.
While downloading the valid images from the internet, we also apply face detection by dlib library2and crop the face area from the entire image into 224/C2224size.
In this way, we collect 135 image categories with more than one million facial expression images.
In order to eliminate the noisy samples of each emotion category (which could be titled or indexed with wrong words), we design a data post-processing strat- egy to clean the collected image dataset introduced above.
Speciﬁcally, we adopt an advanced facial expression embedding model [33] for expression similarity evaluation.
Within that latent space, we gather the expression embedding codes of all Fig.
Based on the k-nearest neighbors algo- rithm, we can efﬁciently detect the embedding outliers by ﬁltering the mean distance between each sample and its K nearest neighbors with a predeﬁned threshold.
While we employing the knn-based data cleaning method, we ﬁnd it is quite sensitive to the speciﬁed hyper- parameters, i.e., the neighborhood size Kand the threshold distance s. Generally, larger neighborhood size and smaller threshold distance will encourage more strict elimination policy and thus reduce the dataset size (eliminating even matched images), and vice versa.
To preserve the label qual- ity as well as the image quantities, we empirically choose K¼10and the threshold distance s¼0:2.
3, we visualize the expression embedding distri- butions of the single class hope before/after the data clean process.
Besides, we also visualize the multi-class expression embedding distribu- tions before/after the automatic data cleaning procedure in Fig.
In Table 1, we compare our proposed Emo135 dataset with some other existed FER datasets.
Besides, we also illustrate the image quantity distribution per each emotion category in Fig.
3.3 Modeling Correlation Matrix for 135 Emotions After obtaining the Emo135 dataset, we propose to analyze the cross-emotion similarities for the 135 emotions.
This phe- nomenon suggests we have to carefully consider the corre- lations of different classes.
Thus we can make them facilitate the emotion label prediction process.To begin with, let us denote the 135 emotion categories as C1; C2;:::; C135.
Because of some inevitable synonyms like anger and fury existed in the 135 emotion classes, and even words in hierarchical rela- tionship like astonishment and amazement which are both subject to surprise, we are motivated to quantitatively calcu- late the distances between different emotions and moreover apply this knowledge to help our network training.
Speciﬁcally, we ﬁrst send every image In kinto the pre-trained facial expression embedding model and generate the correspond- ing expression embedding vector Vn k2R16.
With the expression similarity structure of the embed- ding space, we are now able to evaluate the cross-emotion relationships and model the distances among 135 classes.
Next, we pack the calculated results into a facial expres- sion similarity matrix Fexp2R135/C2135, in which each Fig.
element Fexp ijis given as: Fexp ij¼1 ½dHðCi; CjÞ/C1382ði6¼jÞ: (2) Particularly, the diagonal element Fexp iiis computed by adding the rest entries within the ith row like: Fexp ii¼X j6¼iFexp ij; i2f1; 2;:::; 135g: (3) Finally, we normalize the correlation matrix along each row into sum 1.0.
In this way, we can use Fexpto efﬁciently guide the recognition process to be aware of the cross-emotion relationships.
The other metric we adopted for evaluating cross-emotion similarities is the semantic word embedding.
First, we adopt a Word2Vec model [54], [55] which is pre-trained on large-scale dataset including English blogs, texts, and comments.
For every pair of emotion categories ðCi; CjÞ, the word embedding distance is calculated as follows and then normalized in rows as well: Fword ij¼Word 2VecðCi; CjÞði6¼jÞ: (4) Fword ii¼X j6¼iFword ij; i2f1; 2;:::; 135g: (5) In practice, we deem the word embedding distances as label correlations.
Given a training image I2R224/C2224/C23and its one-hot ground-truth vector Y2R135/C21, we ﬁrst input the image into the expression embedding model for feature extraction.
In particular, we implement the three correlation layers by initializing them with Fexp.
Before comparing Pwith the one-hot ground-truth label Y, we take the word embedding correlation matrix Fwordinto consideration by applying the transformed cross entropy (TCE) loss: LTCE¼ /C0½ðFword/C1YÞlogPþð1/C0Fword/C1YÞlogð1/C0PÞÞ/C138: (6) It is worth noting that despite the formulation of Eq.
4E XPERIMENT In this section, we ﬁrst give some implementation details about our experiments.
Then we report the subjective sur- vey results on evaluating the Emo135 dataset.
Finally, we compare our proposed baseline framework with other feasi- ble approaches and prove the efﬁciency of our method.
Therefore, we make a subjective survey by recruiting 62 participants to validate the semantic corre- spondence of our collected facial images and their emotion labels.
Speciﬁcally, we offer the participants three rating choices including “ I agree the given word faithfully conveys the facial emotions ”, “ I prefer another similar word to describe the facial emotion ”, and “ I prefer another dissimilar word to describe the facial emotion ”, respectively standing for different accu- racy levels of the emotion terms.
Considering the large size of our constructed dataset (roughly 700k images), we choose to carry out the aforemen- tioned manual evaluation by sampling the whole Emo135 dataset.
For instance, we extract 198 images from the largest category (containing 12,794 images) and 16 images from the smallest category (containing 994 images).
In return, we receive 33,651 ratings on 11,217 images, in which each image and its label are evaluated by three differ- ent raters.
Furthermore, to demonstrate the test validity of the rat- ing experiment, we also evaluate the inter-rater reliability byKrippendorff ’ s alpha test [57].
Following the open-source code4forKrippendorff ’ s alpha calculation, we compute the a efﬁcient of our collected results, with a score of 0.776 (a¼1:0indicates perfect reliability, a¼0:0indicates the absence of reliability, and a <0means systematically dis- agreement).
4.3 Model Evaluation To evaluate our proposed model on recognizing facial emo- tional expressions, we conduct both performance compari- son and ablative study.
Therefore, we choose two kinds of competitive models and train them on theEmo135 dataset.
First, we adopt two commonly used pre-trained backbones, ResNet-50 and VGGFace2, assem- bled with Multilayer Perceptron (MLP) classiﬁers.
Second, we adopt two latest FER models, ARM [58] and DACL [59], which have achieved state-of-the-art performance on basic emotion recognition tasks, and modify their output layers for 135-class recognition.
Therefore we integrate the most recent released ResNet-50 model trained on ImageNet-1k [60] dataset as backbone and a 5-layer MLP to regress the image features to 135 dimensional logits.
To fairly compare our model with ARM [58], we mod- ify its regression module (ﬁnal layer dimension) to make it compatible with 135-class FER.
Similar as ARM [58], we adopt the main structure of DACL [59] including the attention net and the sparse center loss calculation module but change the target output expression dimension to 135.
Besides, we also conduct ablative studies to evaluate some key component including the expression embedding model, correlation layer, and label transforma- tion loss of our framework.
For those components, we pro- vide vanilla alternatives to evaluate the effectiveness of ourdesign.
In Table 2, we compare the prediction results from each method on the test set, including F1 score and accuracy for the top 1, 5, 10 classes.
4.4 Semantic Evaluation To evaluate the semantic relationships between the emotion labels and the predicted results, we design several experi- ments in this section.
First, we compare different word embedding models in terms of their inﬂuences on the semantic similarity distances.
By our problem setting, we choose three popular pre-trained word embedding model, including Word2Vec [54], GloVe [55], and BERT [62], to study the corresponding correlations between emotion word semantics.
Then we show some example testing results in Fig.
Furthermore, we employ the chosen word embed- ding model [54], [55] to calculate the semantic distances between the ground-truth label and our predictions/the rest of 135 emotion terms (See Table 4).
5L IMITATION AND DISCUSSION Despite the fact that we have evaluated the emotion labeling results of Emo135 dataset by conducting the subjective sur- vey in the experiment, there still remains a lot of potential improvements in the future.
Besides, we would like to point out that the 135-cate- gorical emotion representa tion, which stands for the semantic richness in this paper, is not a ﬁxed standard.
6C ONCLUSION In this work, we address the semantic-rich facial emotional expression recognition problem.
Unlike the existing FER researches that only focus on a few basic emotion catego- ries, we aim at the granularity of emotion concepts and the entire emotion space.
To this end, we construct a novelFER dataset by leveraging a 135-class categorical model which can exhaustively represent the semantic atlas for t h ee m o t i o nd o m a i n .W ef u r t h e rp r o p o s eab a s e l i n e approach for the emotion recognition task on our built dataset.
In the future, we believe it would be meaningful to propose more dedicated methods and large-scale datasets to promote the understanding and analysis of ﬁne-grained facial emotions.
To this end, in this paper, we propose a novel Causal Narrative Comprehension Model (CNCM) for emotion cause extraction, which learns and leverages causal narrative information smartly to address the above problem.
Speciﬁcally, we develop a Narrative-aware Causal Association (NCA) unit, which mines the narrative cue about emotional results and uses the semantic correlation between causes and results to model causal narratives of documents.
Besides, we design a Result-aware Emotion Attention (REA) unit to make full use of the known result of causal narrative for multiple understanding about emotional causal associations.
Through the ingenious combination and collaborative utilization of these two units, we could better identify the emotion cause in the text with causal narrative comprehension.
Thus, we could preliminarily locate the possible area of emotion causes, namely two alternative cause regions (i.e., clause 1-4 and clause 4-6).
Further, with the causal narrative, we could efﬁ- ciently mine emotional causal correlations between the result clause and other clauses for the ECE task.
Consequently, we in this paper focus on causal narrative comprehension and exploring the emotional semantics cor- relations within causal narratives for better emotion cause extraction.
For one thing, we leverage the causal structure of causal narrative to perceive the possi- ble scope of emotion cause clauses.
For another, based on the guidance of causal structure, we focus on the clauses that have strong causal correlations with the known emotion result clause in a causal narrative to predict emotion cause clauses.
To achieve the above solutions, we must consider the following challenges: 1) How to properly represent the tex- tual causal structure via the causal narrative understandingof a document; 2) Under the guidance of causal narrative, how to explore and understand the causal association between cause clauses and result clauses within the docu- ment for emotion cause extraction.
To address the above challenges, in this paper, we pro- pose a Causal Narrative Comprehension Model (CNCM) for emotion cause extraction.
For the ﬁrst challenge, we design a Narrative-aware Causal Association (NCA) unit, which uses the narrative cue about the known emotion result to learn the semantic correlation between causes and results for causal narrative representation.
For the second challenge, we develop a Result-aware Emotional Attention (REA) unit to acquire the cognition of emotional causal cor- relation through the attention mechanism between the known result clause and other clauses within the causal nar- rative.
With this preliminary cognition, we utilize the NCA unit for the representation of causal narrative struc- ture for good comprehension of causality and perception about the possible scope of cause clauses.
Then we utilize REA to help understand the emotional causal correlations guided by the causal narrative informa- tion.
In this way, we can grasp causality and identify the emotion causes of documents accurately.
Then, in Section 4, we dem- onstrate the model details and training techniques of the developed CNCM.
Finally, we conclude this paper in Section 6.
2R ELATED WORK In this section, we will review the related works from three aspects: Emotion Analysis, Emotion Cause Extraction and Nar- rative Understanding, which are closely related to our work in this paper.
Thus, we regard clause 4 as result clause and aim to ﬁnd out the cause clause (i.e., clause 3) among all the clauses in this text.1744 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL.
Unlike the above studies, in this paper, we deal with this task as an issue of causal narrative compre- hension for documents.
Particularly, we dig deeply into documents information to subtly model causal narratives of documents.
In this way, we can efﬁciently localize emotion 1.https: //weibo.com/CAO ET AL.
Inspired by these studies, we introduce the idea of narra- tive understanding into the ECE task.
In this paper, we regard the clause inDwhose emotion is consistent with that of epas the emo- tion result clause.
Based on the above, this article aims to reveal the clauses that trigger the emotion “ grateful ” of the document: if a clause triggers the overall emotion of its document, we will mark it as the emotion cause clause of this document; otherwise, we will mark it as a non-cause clause.
4M ETHOD In this section, we will introduce model details as well as model training of our proposed Causal Narrative Compre- hension Model (CNCM).
First, we mark the emotion result clause according to the cue of the given emo- tion phrase and realize the embedded representation of each clause for the document.
Second, we adopt the REATABLE 1 Mathematical Notations Symbol Description D the original document with several clauses ep the emotion phrase of D ce k thekthclause of Dthat contains emotion phrase ep, namely the emotion result clause of D y the cause label of the clauses in D a the emotional attention vector in REA r the causal narrative association vector of D Eb the feature representation of Dwith BERT Eh the hidden state by processing EEEEEEEbwith LSTM in REA Eu the representation of Dthat contains the preliminary Emotion Causality Understanding by REA Ec the representation of Dthat fuses the representation EEEEEEEuand the causal narrative information rrrrrrr Eru the ﬁnal representation of Dwith Emotion Causality Re-understanding1746 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL.
Next, we perform the NCA unit, which utilizes the semantic coherence of causal narrative to learn the document ’ s possible causal narrative for realizing the causal narrative representation of the docu- ment.
Finally, imitating the human habit of repeated com- prehension while reading long text, we execute the REA unit again to re-understand the emotional causality of the document.
Based on the ﬁnal effective text representations, we predict the emotion cause clause to realize this task.
4.2 Input Embedding As stated in the problem deﬁnition above, to cognize the causal narrative of document Dfor the subsequent narrative comprehension, we need to mark the result clause at the ﬁrst step.
we adopt BERT-wwm to accom- plish the vectorization of input texts: EEEEEEEb¼BERT ðDÞ¼f xxxxxxx1; xxxxxxx2;:::; xxxxxxxk/C01; xxxxxxxe k; xxxxxxxkþ1;:::; xxxxxxxng; (4) where EEEEEEEb2Rn/C2dwrepresents the documental semantic embeddings at sentence level.
Specially, xe irefers to the embedding of the result clause ce kwhen i¼k.4.3 Emotion Causality Understanding Inspired by the human habit of ﬁrst preferring an initial grasp when reading a complex text [79], at the beginning of CNCM, we also try to make a preliminary understanding of the text, especially its causal narrative.
3, we focus on emotional causal correlations and utilize the known result clause ce kto design a Result-aware Emotional Attention (REA) unit to explore this emotional causal association for emotion causality understanding.
Considering the foundational role of clause representational quality in causal association modeling, we ﬁrst focus on representing the clauses accurately before achieving the emotional causal association among these clauses.
Since BiLSTM is good at modeling long texts and capturing context information, we employ it to cope with document D, ensuring that each clause ’ semantics would not deviate from the context information of D: EEEEEEEh¼BiLSTM E EEEEEEbðÞ; (5) where EEEEEEEh¼fhhhhhhh1;:::; hhhhhhhe k;:::; hhhhhhhng2Rn/C2dhis the hidden state of each clause in document Dprocessed by BiLSTM.
After acquiring the good representation by integrating document context, we construct the emotional causal corre- lation based on the known result clause as follows.
Considering that attention mechanism can simulate humans ’ visual attention and highlight critical information by concentrating on differences of its input, we adopt an attention mechanism to capture different causal associations between the result clause and each clause: MMMMMMM¼tanh WWWWWWW1EEEEEEEhþWWWWWWW2hhhhhhhe k/C0/C1; aaaaaaa¼softmax W WWWWWW3MðÞ; (6) where aaaaaaa2Rnis the attention weight scores, standing for each clause ’ s emotional causal association to the current representation hhhhhhhe kof the result clause.
Special to document D, we leverage the known result clause ssssssse kto speculate the alternative cause regions EEEEEEEu1and EEEEEEEu2of document D, as shown in Fig.
Thus, we ﬁrstly devise a narrative-aware causal association (NCA) unit to model the two possible causal narratives in regions EEEEEEEu1andEEEEEEEu2respectively.
Subse- quently, the causal narrative information of these two regions is integrated together for the causal narrative association vec- torrof its whole document D. To pursue effective text repre- sentations, we integrate the causal narrative information rrrrrrr into the document feature EEEEEEEufrom the previous layer: EEEEEEEc¼rrrrrrr/C1WWWWWWW4EEEEEEEu; (9) where EEEEEEEc2Rn/C2dmrefers the causal narrative representation of document D.dmis the dimension of the hidden layer in CNCM.
Here, we take the alternative cause region EEEEEEEu1: fsssssss1; sssssss2;:::; ssssssse kgas an example to illustrate the imple- ment process of CNA, as shown in Fig.
Hence, we utilize the semantic correlation between the sequence ½sssssss1; sssssss2;:::; sssssssk/C01/C138and result clause ssssssse kto measure the possible causal association between the two for the causal narrative modeling of the region EEEEEEEu1.
Spe- ciﬁcally, considering that LSTM [16] is good at processing and understanding the semantics of sequence data, we uti- lize LSTM to handle the sequential clauses of this sequence by time step for the representation of this sequence: fffffffr1¼LSTM ½sssssss1; sssssss2;:::; sssssssk/C01/C138 ðÞ; (10) Fig.
Here, we take the region EEEEEEEu1as an example to illustrate its process.1748 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL.
Sec- ond, we refer to the narrative modeling in the study about story completion [73] and adopt cosine similarity to ﬁgure out the semantic correlation between this sequence and the result clause ssssssse k: b1¼Similarity f ffffffr1; ssssssse k/C0/C1 ¼fr1/C2ssssssse k/C0/C1 ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ fffffffr1ðÞ2q /C2ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ ssssssse k/C0/C12q; (11) where b12Ris used to measure the degree of likelihood of causality in the alternative cause region EEEEEEEu1.
Similarly, we utilize the CNA unit to handle the other alternative cause region EEEEEEEu2to acquire the value b22R, which is used to measure the possibility of causal narrative inEEEEEEEu2.
And for the sake of calculation, we convert b1andb2 to the corresponding vector form by copying and padding operations, respectively: rrrrrrrui¼ðb1; b1;:::; b1|ﬄﬄﬄﬄﬄﬄﬄﬄ {zﬄﬄﬄﬄﬄﬄﬄﬄ}; 0; 0;:::; 0|ﬄﬄﬄﬄﬄ {zﬄﬄﬄﬄﬄ} Þ; i ¼1; kn /C0k ð0; 0;:::; 0|ﬄﬄﬄﬄﬄ {zﬄﬄﬄﬄﬄ}; b2; b2;:::; b2|ﬄﬄﬄﬄﬄﬄﬄﬄ {zﬄﬄﬄﬄﬄﬄﬄﬄ} Þ; i ¼2; k/C01n/C0kþ18>>>>>>> <>>>>>>>: (12) where rrrrrrrui2Rnstands for the causal narrative association vector of the region EEEEEEEui.kis the number of clauses in EEEEEEEu1, while n/C0kþ1is the number of clauses in EEEEEEEu2.
Since rrrrrrru1 andrrrrrrru2contain the desired information at the position of clause ssssssse k, we realize this integration by the mean operation as below: rrrrrrr¼ðrrrrrrru1þrrrrrrru2Þ=2; (13) where rrrrrrr2Rnrefers to the causal narrative association vec- tor of document D. As stated before, rrrrrrris integrated into the document representation Euto achieve the causal narrative modeling of this document and obtain the effective docu- ment representation Ec.
4.5 Emotion Causality Re-Understanding Inspired by humans ’ multiple understanding of long texts [79], we in this section employ the REA unit again to re- understand the causality of document D. Speciﬁcally, the above text representation EEEEEEEcis fed into REA as below: EEEEEEEru¼REA ðEEEEEEEcÞ; (14) where EEEEEEEru2Rn/C2dhstands for the ﬁnal text representation after this re-cognition of emotional causality of document D. It could facilitate CNCM to comprehend the emotional causality of this document very well.
The relevant details are similar to those of the preliminary understanding of emotional causal correlation for document D.4.6 Cause Prediction In this layer, we leverage the ﬁnal representation EEEEEEEruof doc- ument Dto predict emotion cause clauses.
Speciﬁcally, we fed each clause embedding of EEEEEEErusuccessively into a single- layer fully connected (FC) network and a sigmoid function for cause clauses prediction: ppppppp¼Sigmoid FC E EEEEEEruðÞðÞ; (15) where ppppppp2Rnindicates the probability vector of emotion cause labels for the clauses in document D. 4.7 Model Learning Since the ECE task is a classiﬁcation problem, we employ thecross /C0entropy function as our loss function.
To minimize the loss, we use the Adam optimizer to update the parameters of each layer.
Additionally, we conduct the dropout operation and K-fold (k=10) cross validation trick to prevent our model training from overﬁtting.
5E XPERIMENTS STUDY In this section, we ﬁrst introduce the experiment prepara- tion, involving the dataset details, evaluative criteria, and experimental settings.
Then, we list some baseline methods, analyze the experimental results in detail, and conduct some ablation studies.
Subsequently, we make some detailed analysis of some meaningful issues for our pro- posed model.
Finally, we present several visualization cases to illustrate the workﬂow of CNCM.
5.1 Dataset Description Following the general practice in many previous studies [4], [14], we conduct experiments on two publicly ECE cor- puses: a Chinese benchmark dataset [48] based on Sina City News2and an English benchmark dataset [3], [19], [83] based on an English novel.
To provide an intuitive sense of the datasets, we also present a document example from the Chinese dataset as shown in Fig.
Additionally, we pres- ent some key information about these two datasets in Table 2.
Noticeably, according to the num- bers of the cause clause and general clause (the non-cause clause) shown in Table 2, we can conclude that the size of each dataset is not large enough and the number of cause clauses in documents is extremely uneven.
Given that document size is closely related to document narrative complexity, we perform statistical analysis on the 2.https: //city.sina.com.cn/CAO ET AL.
5.2 Evaluative Criteria Following [14], [19], [52], we also adopt Precision, Recall, and F1 to evaluate the performance of models in this paper.
5.3 Experimental Settings For better training, we initialize the related parameters of our model by the following settings.
First, we split thedataset into training (80%) and test (20%) sets.
In terms of data vectorization, consistent with [18], we adopt the advanced pre-trained language model BERT [78] to encode each input clause into an embedding vector.
By investiga- tion of BERT-wwm used in other works, we ﬁne-tune the pre-trained model on the training set and acquire the text vectors with the dimension dw= 1,024.
During intermediate steps, we set dh= 512 to be the dimensions of hidden state in the LSTM and BiLSTM of CNCM.
Besides, we use the Adam optimizer with the learning rate of 0.005 to train the networks on an NVIDIA Tesla K80 GPU with the batch size of 128.
Considering that the data- sets are not very large and the number of cause clauses in documents is extremely uneven, we adopt 10-folds cross- validation and the dropout rate of 0.5 to mitigate possible overﬁtting.
Here, we analyze these two tables, respectively.
Combining Tables 3 and 4 together, we can ﬁnd that EF-BHA and RHNN are the SOTA benchmark models.
we perform holistic t-tests of the overall performance for CNCM and these two SOTA benchmark models (EF-BHA and RHNN).
Through the t-tests, we can acquire over 95%, and 99% of conﬁdence that CNCM has signiﬁcant improve- ment over EF-BHA and RHNN, respectively, which indi- cates that CNCM has certain superiority.
5.6 Ablation Study To conﬁrm the effectiveness of each unit in CNCM, we con- duct ablation experiments via removing or replacing the fol- lowing three aspects: the causal narrative representations of NCA, the pre-training model for text embeddings, and the context-aware emotion attention of REA.
To examine the effect of this innovation on the ﬁnal experimental performance, we conduct experiments on the ablation model CNCM (w/o NCA), which is derived from CNCM by removing the NCA unit.
5.6.2 Pre-Trained Model for Text Embedding As we know, BERT can output outstanding text representa- tions which have been shown to be very effective in many downstream tasks of natural language processing.
Owing to this reason, we achieve the initial vectorization of texts in CNCM through BERT ’ s evolution model BERT-wwm.
To explore the importance of the pre-trained language model on the ECE task, we replace BERT-wwm with another pop- ular pre-trained language model, word2vec [86] to conduct experiments.
5.7 Detailed Analysis In this subsection, we carry out some supplementary experi- ments to give a detailed analysis of our proposed model from multiple aspects.
Furthermore, we discuss the effects of the balance about cause-result order on the performance of the ECE task.
Thus, we conduct relevant data statistics and experiments to explore the effects of this factor on the performance of CNCM.
Based on this observation, we choose the documents whose emotion cause clauses are no more than 1 or 2 or 3 clauses away from the emotion result clauses to construct three sub-datasets.
5.7.3 Effects of Document Size Considering that document size is one of the important fac- tors in determining the narrative complexity of a document, we also explore the effects of document size on CNCM.
7, we select the document size in four ranges, i.e,3-4, 5, 6, 7-12 to conduct experiments.
If a document contains multiple cause clauses, we select its ﬁrst cause clause for statistics.
5.7.4 Effects of Emotion Category In order to study whether CNCM had a bias for emotion categories when performing the ECE task, we also under- take a statistical analysis of emotion category.
Taking the Chinese dataset as an example, ﬁrst, we use the dictionary- based approach to identify the emotion of the documents in the dataset.
Here, we use the Chinese emotion ontology database [89] to classify the emotions of these documents into seven categories: “ glad ”, “ good ”, “ angry ”, “ sad ”, “ afraid ”, “ bad ” and “ amazed ”.
Second, we conduct statis- tics of emotion category for documents in the Chinese data- set and its test set, respectively.
Finally, we perform the same statistics for the documents whose emotion cause clauses are correctly predicted in the test set.
5.7.5 Effects of Training Dataset Scale To present the performance of CNCM systematically, we compare the results under different scales of the training dataset for our developed model.
Considering that the training dataset of the original experiment accounts for 80% of the total data in the Chinese dataset, we take into account the other three training data settings: 20%, 40%, and 60% of the total data.
To be speciﬁc, we implement experiments under these training data settings and compare the corresponding per- formances to the original experiment.
5.8 Case Studies To provide some intuitive demonstrations of how causal narrative representation and emotional causal association improve the effectiveness of our model, we show some case studies in Fig.
In addition, we also provide two error cases to illustrate the existing problems of CNCM.
As a further exploration, we aim to address this issue in the future.Fig.
6C ONCLUSION AND FUTURE WORK In this work, we focused on emotion cause extraction and argued that causal narrative comprehension is very impor- tant to this issue.
To this end, we proposed a novel Causal Narrative Comprehension Model (CNCM) based on Causal Narrative Comprehension to address this task.
Guided by the representation of causal narrative, we developed a result- aware emotional attention unit to understand the emotional causal association multiple times, so as to realize the task of emotion cause detection.
Extensive experimental results on the benchmark datasets demonstrated the effectiveness of CNCM for the ECE task.In the future, we will strive to develop a multilingual cor- pus of the ECE task to reﬁne our studies.
Based on this, we also hope to conduct further research on much more general narrative material and attempt to make utilization of the narrative information to promote some appropriate tasks about text semantic understanding.
To address these two issues, we propose an aspect-opinion correlation aware and knowledge-expansion few shot cross-domain sentiment classiﬁcation model.
Inspired by the success of applying syntactic infor- mation in the aspect-opinion pairs extraction task [11], [12], we introduce the syntactic knowledge structure to capture the relational features between the aspect and opinion terms for the cross-domain learning, aiming to solve the sentiment transfer error problem.
As we can observe, the terms with similar semantics usually share the relational knowledge structures.
In this paper, we propose an aspect-opinion correlation aware and knowledge-expansion few-shot cross-domain sentiment classiﬁcation model (AKFSM).
For this prob- lem, we propose an aspect-opinion correlation aware and knowledge-expansion few-shot cross-domain sentiment classiﬁcation model.
In our paper, we propose a few-shot learning based cross-domain sentiment classiﬁcation model to effectively address the problem of ignoring domain-speciﬁc features.
3M ODEL To solve the sentiment transfer error problem and the domain-speciﬁc features ignoring problem suffered in exist- ing methods, we propose a knowledge-expansion few-shot learning model for the cross-domain sentiment classiﬁcation task.
Based on this observation, we design an aspect-opinion correlation aware graph learning method to capture the relational features between the aspect and opinion terms, aiming to solve the sentiment transfer error problem.
Speciﬁ- cally, for each unlabeled document, we use the Standard CoreNLP libraries [6] to recognize the part-of-speech of each token and the dependency relations among the tokens.
2020 [9], for each node (i.e., term) viinG, we utilize a two-layer GCN encoder (which are stacked one another) to learn its feature repre- sentation gggggggi, as follows, gggggggiiiiiii¼hhhhhhhð2Þ i¼fðhhhhhhhð1Þ i; 2Þ; hhhhhhhð1Þ i¼fðvvvvvvvi; 1Þ (2) fðxxxxxxxi; lÞ¼sX r2RX j2Nr i1 jNr ijWðlÞ rxxxxxxxjþWðlÞ 0xxxxxxxi0 @ 1 A (3) where Nr idenotes the neighbouring nodes of node viunder the relation r2R; sdenotes the activation function such as ReLU; vvvvvvviis the randomly initialized representation vector; Wð1=2Þ randWð1=2Þ 0denotes the learnable parameters.
To adapt the BERT into the speciﬁc domains (including both source and target domain), we conduct1696 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL.
Therefore, based on the pre- trained BERT encoder1, we can obtain the semantic feature representation xxxxxxxw2Rdwfor the instance x, where dwdenotes the dimension of the vector.
For each sentiment polarity categories ci2Cwith ksupport instances (i.e., 2- way k-shot setting), we can obtain the prototype pppppppiof cate- gory cias follows: pppppppi¼1 kXk j¼1xxxxxxxj i (15) Finally, the probability of query instance qbelonging to sen- timent polarity category ci2Ccan be measured as follows: pfðcijqÞ¼expð/C0dðpppppppi; xxxxxxxqÞÞPC j¼1expð/C0dðpppppppj; xxxxxxxqÞÞ (16) where fdenotes all the trainable parameters in sentence encoder; dð:;: Þis the Euclidean distance function for the two given vectors.
Speciﬁcally, we randomly select Kinstances for each senti- ment category ci2Cto construct the support set S.M e a n - while, the query set Qis constructed by randomly selecting jQj¼5instances respectively from the positive and negative sentiment category, where S\Q¼;.
As shown in Tables 2, 3, and 4, we compare the perfor- mance of our proposed model with that of two categories of related works: 1) the few-shot learning baselines (i.e., GNN [49], MetaNet [50], SNAIL [61], Proto-CNN [16], Proto-CNN with adversarial training (Proto-CNNy) [17], Proto_HATT [51], Proto-BERT [17], Proto-BERT with adversarial training (Proto-BERTy) [17], BERT-PAIR [17], MLADA [54], PtNet [55]) and 2) current cross-domain sentiment classiﬁcation models (i.e., DANN [40], PBLM [35], HATN [34], ACAN [43], IATN [42], HATN-BERT [34], CoCMD [14], KinGDOM [9], BERT-DAAT [8]) and SENTIX [10]).
4.2 Result Analysis In our experiments, we compare the performance of our pro- posed model with 11 few-shot learning based baselines (i.e., GNN, MetaNet, SNAIL, Proto-CNN, Proto_HATT [51], Proto- BERT, Proto-CNN with adversarial training (Proto-CNNy), Proto-BERT with adversarial training (Proto-BERTy), BERT- PAIR [17]), MLADA [54]), PtNet [55] and 10 current cross- domain sentiment classiﬁcation models (i.e., DANN [40], PBLM [35], HATN [34], ACAN [43], IATN [42], HATN-BERT [34], CoCMD [14], KinGDOM [9], BERT-DAAT [8]) and SEN- TIX [10] and 3 supervised learning based baselines (i.e., CNN, LSTM [1] and BERT [62]) which are trained with 1000 target domain labeled data, validated with 200 labeled data and tested with 800 labeled data.
As we can observe, our proposed model achieves higher accuracies with a large margin in all of the cross-domain experimental settings.
To the best of our knowledge, we are the ﬁrst to focus on the cross-domain few-shot sentiment classiﬁcation task.
4.2.2 Comparison With Related Cross-Domain Sentiment Classiﬁcation Models Meanwhile, we compare the performance of our proposed model with current cross-domain sentiment classiﬁcation models.
As we can observe, most of the cross-domain sen- timent classiﬁcation models (e.g., DANN [40], PBLM [35], ACAN [43] IATN [42] and so on) mainly focus on extracting the domain-invariant features by the way of unsupervised learning, but ignoring the domain-speciﬁc features.
As we can observe, our proposed model obtains higher accuracies with a large margin than the model KinG- DOM in all cross-domain tasks, which can prove that our model can effectively capture the domain-speciﬁc features and the aspect-opinion correlation features in the external commonsense knowledge learning.
In addition, to evaluate the effectiveness of the expanded rela- tional knowledge in our proposed model, we conduct the ablation experiment for the module Graph Feature Enocder.
Furthermore, we also conduct the ablation experiments for the representation fusion strategy (i.e., the feature mapping layer with a recon- struction loss £ recon).
Finally, we conduct the ablation experiments for the shared-knowledge aware attention.
As shown in Table 7, we can observe that the model with 2-hop knowl- edge linking strategy outperforms the model with 1-hop linking knowledge strategy by only less than 1%.
Further- more, we also conduct the resource cost comparison, as shown in Table 8.
4.2.5 Viusalization To better understand the effectiveness of our proposed model, we randomly select 100 support instances from posi- tive and negative categories and encode them into the hid- den embeddings in the task of cross-domain (i.e., from Kitchen domain to Electronic domain) sentiment classiﬁca- tion.
Then, we map them into 2Dpoints using Principal Component Analysis (PCA).
Finally, we conduct the visualization analysis for our proposed model with the shared-knowledge aware attent ion.
5c and 5d, we can ﬁnd that the model with a shared-knowledge attention module can better distinguish the positive and nega- tive sentiment polarities in the feature space, which can evalu- ate the effectiveness of the attention strategy in our model.
5C ONCLUSION In this paper, we propose an aspect-opinion correlation aware and knowledge-expansion cross-domain sentiment classiﬁcation model in the few-shot scenario.
Furthermore, to solve the sentiment transfer error problem, we design an aspect-opinion correc- tion aware graph learning module to capture the relational features between the aspect and opinion terms.
To this end, herein we propose a novel spatio-temporal transformer architecture – to the best of our knowledge, the ﬁrst purely transformer based approach (i.e., void of any convolutional network use) for micro-expression recognition.
In order to capture automatically both short- and long- range relations at the same time, we apply Multi-head Self- attention Mechanism (MSM) instead of a Convolutional Kernel as the cornerstone of our deep learning MER architec- ture.
In this work we show how a transformer based deep learning architecture can be applied to MER in a manner which outperforms the current state of the art.
The main contributions of the present work are as follows: 1) We propose a novel spatio-temporal deep learning transformer framework for video based micro- expression recognition, which we name Short and Long range relation based Spatio-Temporal Transformer (SLSTT), the structure whereof is summarized in Fig.
Some go further and employ temporal frame interpola- tion (as indeed we do herein) so as to increase the frame count [6], [9], [10], [12], [39].
However, none of the existing approaches to micro- expression recognition adequately make use of both the spatial and temporal information due to the design difﬁ- culties posed by the challenges we discussed in the pre- vious sections.3P ROPOSED METHOD In the present work we propose a method that takes advan- tage both of the physiological understanding of micro-expres- sions and their characteristics, as well as of the transformer framework.
Therefore, herein we pro- pose to calculate optical ﬂow between each sample frame and the onset frame instead of consecutive frames, see Fig.
To handle 2D images, we represent each image as a sequence of rasterized 2D patches.
Herein we do not use appearance images, that is the original video sequence frames, as input but rather the corresponding optical ﬂow ﬁelds.
[24], we ﬁrst extract local spatial features in patch regions with a patch-wise fully con- nected layer.
5, we extract the short-range spatial features from image Xto feature map X2RH P/C2W P/C2D, ﬂatten and transpose them to ND -dimensional vectors, where N¼HW P2the resulting number of patches in each image.
Our encoder contains LTtransformer layers; herein we use LT¼12, adopting this value from the ViT-Base model of Dosovitskiy et al.
[24] (the pre-trained encoder we use in experiments).
Dmis typi- cally set toD M, so as to keep the number of parameters con- stant with changing M. 3.3 Temporal Aggregation After extracting both local and global spatial features associ- ated with each frame using a transformer encoder, we intro- duce an aggregation block to extract temporal features before performing the ultimate classiﬁcation.
Therefore, we propose an LSTM architecture for temporal aggregation.Long Short-Term Memory (LSTM) [56] is a type of recur- rent neural network with feedback connections, which over- comes two well-known problems associated with RNNs: the vanishing gradient problem, and the sensitivity to the varia- tion of the temporal gap length between salient events in a processed sequence.
Herein we achieve this using cosine annealing [57], i.e., using the the cosine function to modulate the learning rate which initially decreases slowly, and then rather rapidly before sta- bilizing again.
4E XPERIMENTS AND EVALUATION In this section we describe the empirical experiments used to evaluate the proposed method.
4.1 Databases Following the best practices in the ﬁeld, for our evaluation we adopt the use of three large data sets, namely the Spon- taneous Micro-Expression Corpus (SMIC) [34], the Chinese Academy of Sciences Micro-Expression II data set (CASME II) [35], and the Spontaneous Actions and Micro-Movement database (SAMM) [36], thus ensuring sufﬁcient diversity of data, evaluation scale, and ready and fair comparison with other methods in the literature.
To achieve uniformity with the other two corpora, namely CASME-II and SAMM which are described next, which only contain high-speed camera videos, it is the HS subset from SMIC that we make use of herein.
In order to maintain data consistency across different databases, in our experiments we employ a different face extraction approach.
In particular, we utilize the Ensemble of Regression Trees (ERT) [58] algorithm implemented in DLib [59] to localize salient facial loci (68 of them) in a uniform manner regardless of which data set a speciﬁc video sample came from.
However, in this paper we employ an alternative strat- egy.
Therefore, herein we instead simply use a non- reﬂective 2D euclidean transformation, i.e., one comprising only rotation and translation.
By doing so, at the same time we ensure the correct alignment of salient facial points and maintain information containing facial contour variability.
Furthermore, unlike the authors of SMIC-HS and CASME II, we do not perform facial landmark detection in the ﬁrst frame of a micro-expression sample, but rather in the apex, thereby increasing the registration accuracy of the most informative parts of the video.
In an attempt to extract accurate temporal information, we also apply frame interpolation from raw videos, effec- tively synthetically augmenting data.
Herein we propose a novel approach to interpolation so that its result is smoother in terms of optical ﬂow, it being the nexus of our entire MER methodology.
Original RIFE interpolates one frame between two given consecutive frames, so we apply it recursively to interpolate multiple intermediate frames.
Speciﬁcally, given any twoconsecutive input frames I0; I1, we apply RIFE once to get intermediate frame ^I0:5att¼0:5.
We then apply RIFE to interpolate between I0and ^I0:5to get ^I0:25, a n ds oo n .I no u r experiment, we prioritize interpolation in the temporal vicin- ity of the apex frame.
Recall that the apex frames are speci- ﬁed explicitly in CASME II and SAMM, and for SMIC-HS we choose the middle frame of each sample video as the apex.
If the number of interpolation frames is lower than the reference count (the average number of frames in this period across the database), we use the same method on the updated frame sequence iteratively to generate further intermediate frames.
4.3 Experimental Settings 4.3.1 Implementation Details In the spatial feature extraction procedure, we employed base ViT blocks, with 12 Encoder layers, hidden size of 768, MLP size of 3072, and 12 heads.
For initialization, we use the ofﬁcial ViT-B/16 model [24] pre-trained on ImageNet [62].
For temporal aggregation, we select 11 frames (apex, and ﬁve preceding and succeeding it) per sam- ple as inputs for the mean aggregator and LSTM aggregator.
4.3.3 Evaluation Metrics Following previous work and the Micro-Expressions Grand Challenges (MEGCs), we conducted experiments on SMIC- HS, CASME II, and SAMM, evaluating the classiﬁcation Fig.
In this way we best mimic real-world situations and in particular assess the robustness to variability in ethnicity, gender, emotional sensitivity, etc.
The micro-expression databases con- taining multi-modal signals [74], [75], which have begun emerging recently, seem promising in overcoming some of the limitations of the existing corpora, and we intend to make use of them in our future work.
5C ONCLUSION In this paper, we proposed a novel transformer based spa- tio-temporal deep learning framework for micro-expression recognition, which is the ﬁrst deep learning work in the ﬁeld entirely void of convolutional neural network use.
These ﬁndings strongly motivate fur- ther research on the use of transformer based architectures rather than convolutional neural networks in micro-expres- sion analysis, and we hope that our theoretical contributions will help direct such future efforts.
Hence, in this work, we focus on learning optical ﬂow specialized for faces, which we will attempt to constrain the algorithm to learn only lifelike expressions on faces.
In doing so, we explore how well a deep network can perform in this tas k. We demonstrate that the pro- posed architecture will work w ell for faces compared and com- pare it to traditional optical ﬂow algorithms.
Finally, we test the usefulness of our network by using the learned optical ﬂow predictions for micro-expression detection using optical ﬂow and the Shallow Triple Stream Three- dimensional CNN (STSTNet) [8].
And ﬁnally, we present the concluding remarks and recommendations for improvement and future work in Section 6.
2R ELATED WORK First, we discuss works related to optical ﬂow estimation using classical and deep learning techniques, along with some of the common challenges.
For our experiments, we use the FlowNetS architecture adapted from [6] to train on our dataset.
By demonstrating how we can adapt FlowNetS to perform well on datasets consisting of only faces, we can later improve even further by training more advanced architectures on such datasets.
[22] used the pyramid-structure CNN architec- ture PWC-Net for optical ﬂow prediction, which we use in this work to test on the face optical ﬂow dataset as a bench- mark implementation and compare with our performance.
Another optical ﬂow CNN we use for comparison in this work is LiteFlowNet by Hui et al.
Our work can be considered to be a contribution to the study of optical ﬂow ’ s effects on CNNs, with the difference being that we focus on facial datasets instead.
One key difference between our work and theirs is that we incorporate a cyclic loss to test how well the ﬂow ﬁelds reconstruct the second image in the image pairs.
Additionally, the training data we generate is based upon the BP4D-Spontaneous dataset, which is speciﬁcally tuned to exhibit various emotions and thus more specialized for expression recognition tasks.
In addition, we also test our network ’ s performance on micro- expression detection.
We introduce a noisy optical ﬂow dataset, that we generate using the motion of sparse facial landmarks.
In addition, we demonstrate the usefulness of our approach by applying it for micro-expression detection.
They also use apex and onset frames in [8] to com- pute optical ﬂow along with an added feature, the optical strain, as input to STSTNet, which we adapt in this work to test for micro-expression recognition.
After having reviewed several related works, we now describe the dataset preparation in our work.
1 shows the overall pipeline for a pair of frames and how they can be used for dataset generation and CNN training.1 We introduce the notation that we ’ ll use throughout this section to generate the optical ﬂow ground truth from the BP4D-Spontaneous dataset [5].
For a given sequence Sin the dataset, we denote the frames contained in SbyF¼ffkgNf k¼0, where fk2RH/C2W/C23are the ordered frames.
Next, we use Scipy ’ s Delaunay triangulation package to obtain a triangular mesh over PPPPPPP0in the ﬁrst face f0.T h i sm e s h divides the face in f0intoNtdisjoint triangles TTTTTTT0¼ftttttttlgNt l¼0, where each tttttttl¼vvvvvvv0; vvvvvvv1; vvvvvvv2 ðÞT l2R3/C22is the matrix with rows composed of vertices of triangle l. After triangulating f0, we use similar triangulation on the remaining frames in the sequence, yielding the set of triangulations fTTTTTTTkgNf k¼0onS.
Given the triangle tttttttl k/C01, we infer an afﬁne map AAAAAAAl k/C012R3/C23 that sends its vertices to the vertices in tttttttl k.S p e c i f y i n gt h r e e mappings are sufﬁcient to uniquely deﬁne an afﬁne map [45].
Repeating this for all fkin the sequence is overall computationally expensive, so we only do it for triangles in the ﬁrst frame of that video.
After determining the afﬁne maps and mapping the tri- angles and their interior pixels vvvvvvvk/C01tovvvvvvvk, we compute the per-pixel optical ﬂow vector ~uuuuuuuk/C01by ~uuuuuuuk/C01¼vvvvvvvk/C0vvvvvvvk/C01: (3) However, when the domain is not a discrete grid, the opti- cal ﬂow ﬁelds ~uuuuuuukare deﬁned on points that are not neces- sarily pixel coordinates, which affects the frames after f0.
To recover the optical ﬂow ﬁeld uuuuuuuk/C01on a discrete grid in the target image, we use bicubic spline interpola- tion over the irregular grid using ~uuuuuuuk/C01.
We only do this to deﬁne the optical ﬂow ﬁeld at each frame, but continue to learn the afﬁne maps on the irregular grids, since we wish t op r e s e r v et h es a m eb a r y c e n t r i cc o o r d i n a t e so b t a i n e di n f0for all frames.
4B ASELINE NETWORKS In this section, we describe the CNN architecture used to train the optical ﬂow, followed by the training and ablation study details.
Should we discover animprovement, we can expand it in the future to tackle other problems (e.g., robustness to occlusions).
Although the artifacts caused by the warping pro- duced a ﬂawed image in the FlyingChairs dataset, we hypothesize and show that it still helps guide the directions of the predicted ﬂow when training on faces since the unde- sirable effects are considerably less due to the lower amount of new structure.
Note that, in the following, we drop the added subscript and refer to them as Yiand ^Yi.
Since we assume that the background is stationary, much of the ground-truth ﬂow ﬁeld outside of the boundaries deﬁned by the key-points are zero vectors.
To make the train- ing more practical, we zoom on the box with vertices deﬁned by the key-points with maximal and minimal coordinates plus somean offset of 10 pixels each in the xandydirections.
Next, we describe the different experimental setups used to train the networks.4.3.1 Experimental Setup 1: No Cyclic Loss In this experiment, the architecture is used without the addi- tional warping layer.
For preliminary experimentation, we trained the network once on the face data for a 15k and 1k training and valida- tion split.
Also, we used the aforemen- tioned validation sets for FlyingChair and Sintel validation sets as test sets, since we do not have access to their original test sets.
Then we repeated this by training on FlyingChairs and Sintel training datasets individually and testing them on all three test sets.
After the preliminary experiment, we trained the same network again from scratch on faces only for a 228k, 65k, and 32.5k train/val/test split, exactly as in the next two experiments, to make them comparable.
For this experiment, we deﬁne the additional cyclic loss function Li 2ðXiþ1; ^Xiþ1Þfor one out- put pair ias: TABLE 1 The Average EPE for Each Network Described in Section 4.3.1, Trained and Tested on All Three Datasets Tested on Trained on Faces FlyingChairs Sintel Faces 0.4054 5.8495 5.1731 FlyingChairs 1.4040 1.4413 3.0300 Sintel 0.8282 7.7613 6.23580 2.Source code for training and evaluation, as well as the trained models, will be available at https: //github.com/malkaddour/Self- Supervised-Approach-for-facial-movement-based-optical-ﬂow2076 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL.
In this experiment, we train the network on both faces and Flying- Chairs datasets using two different sets of loss weights /C211; /C21 2.
For both cases, we trained the network on faces for 15 epochs and 228160 training pairs.
4.3.3 Experimental Setup 3: With Cyclic Loss, Smoothness Constraint, and Average Angular Error In this experiment, we added an additional loss function Li 3ð^YiÞ.
In the next sections, we will use abbreviations for experi- ment and case numbers in the discussions for brevity.
2II, and no Roman numerals mean we refer to both cases of that particular experiment.
4.4 Micro-Expression Detection In this section, we describe how optical ﬂow features are used for a micro-expression recognition task to demonstrate the efﬁcacy of the optical ﬂow generated using our method.
4.4.1 CNN and Optical Flow Features To train the optical ﬂow features, we use the three-dimen- sional lightweight CNN proposed by Liong et al.
For comparison, we do the same using our optical ﬂow features on the SAMM and SMIC datasets.
For SMIC, since OpenFace failed to detect the keypoints for some images, we instead use the dlib facial landmark detector [55], which is based on an ensemble of regression trees [56], and deﬁne the crop border at 15 pixels away from the maximum and minimum xandyimage coordinates.
4.4.2 Micro-Expression Detection Experimental Setup The authors of STSTNet [8] evaluate their model using leave- one-subject-out cross-validation (LOSOCV), and we do the same to train the micro-expression recognition networks.
For every optical ﬂow network, we train the network three times: once on SAMM, once on SMIC, and once on the combined dataset consisting of both.
To deal with the class imbalance, we use macro-averaged recall, precision, and F1-scores to evaluate the performance of every trained network.
We will combine the metrics to a single scalar, which we will refer to as the aggregated metric.
5R ESULTS AND DISCUSSION To evaluate the ﬂow network, we ﬁrst report preliminary results as discussed in Section 4.3.1.
Next, we evalu- ate a number of other popular optical ﬂow methods on the test set.
To select Gunnar-Farneback ﬂow, we used Allaert et al. ’ s [30] review, who recommend optical ﬂow methods well- suited for facial motion, namely Gunnar-Farneback, Flow- Field, and Normalized Convolutional Upsampling (NCUP).
Based on this, we felt that Farneback would be an appropriate candidate from the non-CNN category.
For this task, we use the same networks as in optical ﬂow prediction with the addition of TV-L1, since the latter was used to compute the optical ﬂow in STSTNet [8], the microexpression classi- ﬁer we used in this work.
From Table 1, we observe that the network trained on our BP4D-derived face dataset performs best when tested on faces.
Due to thelarge ﬂow vectors present in Sintel, we suspect that the net- work trained on Sintel tends to also predict ﬂow ﬁelds with large vector magnitudes when tested on Sintel.
After adding the cyclic loss and training for more data and epochs, we expect to observe a difference in perfor- mance compared with Exp.
Here, we train the setup for Exp 1 again, using the same data split as the other experi- ments, for comparison purposes.
Now we show the results of the networks trained with cyclic loss as described in Sec- tions 4.3.2 and 4.3.3.
Note that in the case of CK+, we trained the networks on BP4D-Spontaneous data, which is a completely different data- set.
What we mean here, is not the magnitude in general, but rather magnitude in regions that do not correspond or assist in microexpression detection.
We hypothesize that our method will overcome the per- formance difference in some of the results if we use a denser keypoint tracker during the optical ﬂow training phase to generate the BP4D ground-truth.
Furthermore, as previously discussed, we have used FlowNetS to train the face data to benchmark its efﬁcacy compared to other networks, and thus using a bet- ter-designed CNN along with the denser keypoint ground- truth will likely further improve the performance.
6C ONCLUSION AND FUTURE WORK In this paper, we explore the possibility of using a facial expression dataset to learn optical ﬂow representations based on a self-supervised technique.
Using evaluation strategies suitable for messy, real-world data, we show that nonverbal vocalizations can be classiﬁed by function (with 4- and 5-way classiﬁcations) with F1 scores above chance for all participants.
Here we focus on a subset of this population, abbreviated as mv *, who have fewer than 20 words or word approximations and limited expressive language through speech and writing.
In our prior work [4], we developed a novel longitudinal data collection process to col- lect real-world audio with in-the-moment labels provided by a close family member or caregiver.
Here we extend our pre- vious work, presenting new analytical approaches applied to a larger number of vocalizations from more individuals.
In this paper, we present the results of the largest real- world nonverbal vocalization classiﬁcation experiments to date with vocalizations by eight mv * communicators.
In this work, we use the term verbal speech to specify speech with typical verbal content, which is different from nonverbal speech which is also richly expressive and com- municative (as from mv * communicators) but may not contain verbal content like words or phrases.
In our prior work, we describe in detail the participatory design process used to design our approach [61], our novel data collection system [4], [62], and provide preliminary classiﬁcation results with three mv * communicators [4].
Here we extend this work to include new analytical approaches for classiﬁcation with eight mv * communicators, and discuss our experimental results in the context of real-world data collection.
In [65] and [61], we provide additional infor- mation on how the labeling system was designed and vali- dated.
5C ONCLUSION In this paper, we presented results from the largest study of nonverbal vocalizations with mv * communicators to date along with modeling approaches appropriate for dealing with real-world, messy data.
We employ the following entropy formula Entropy ¼/C0X c2C^pðcÞlog^pðcÞ: (16) We then sort the document embedding obtained with LCR-Rot-hop++ from lowest to highest entropy.
We have selected 16 training epochs for this model (MAE was the best loss function for the QuaNet-based models).
We have randomly selected a subdataset of the test dataset for the FOOD #QUALITY aspect category.
We conclude that AspEnt- QuaNet can be successfully a pplied to ternary ABSQ, and often outperforms all existing methods by at least a factor of 2.
We would like to investigate other dimensionality reduction techniques such as PCA or the use of autoencoders to represent the three probabilities in one value and sort the document embeddings on that value.TABLE 7 Results of the Quantiﬁcation Method Evaluated on the SERV- ICE #GENERAL Aspect Category (Best Performances in Bold) Model AE RAE KLD CC 0.075444 0.468466 0.267874 ACC 0.065944 1.447930 0.262462 PCC 0.042534 0.254345 0.030829 PACC 0.091526 1.291486 0.094069 QuaNet 0.057297 0.261275 0.031820 EntQuaNet 0.030276 0.187150 0.015957 AspEntQuaNet 0.029821 0.188882 0.013353MATSIIAKO ET AL.
We ﬁnd that in general the current CER frameworks fall short in understanding of the semantics of a conversation.
We found that such replacement can hardly affect the classiﬁcation accuracy of the model.
We then map the historical information for a target utter- ance onto a compact latent semantic space, which can be regarded as a summary of its historical context.
We conduct experiments on two datasets (i.e., IEMO- CAP [20] and MELD [16]), with four representative CER frameworks in two scenarios including real-time and non-real- time.
In a nutshell, our main contributions are summarized as follows: (1) We motivate and explore the problem of semantic understanding the conversational context in CER.
(2) We propose a pluggable approach, namely C3ER, to enhance the contextuality in the utterance representation, using contrastive learning.
(3) We conduct a series of experiments on two datasets, with four representative CER frameworks.
We further presents an extensive empirical evaluation of our method and a series of in-depth analysis and empirical studies in Section 6.
We will discuss the related work in these two ﬁelds separately below.
We generally divide a typical neural CER framework into two parts: context modeling network and classiﬁer.
2, We design three context replacement modes (1) Emotion-Relevant Modiﬁcation (EM): For a target utter- ance ut, the content for each of its context utterances cðu1; ...; ut/C01; utþ1; ...; unÞwill be replaced with new utter- ances drawn from cross-dialogues in the test data, while the emotion labels are kept the same as those associated to the original utterances, to maintain the emotional relevance of the replacement content.
4.2 Experimental Setup We implement four representative CER models including a real-time method, i.e., AGHMN [9] and three non-real-time methods, i.e., LSTM [5], DialogueRnn [1], and DAG [15].
We adopt a speciﬁc variant of Dia- logueRnn, namely Bi-DialogueRnn (publicly available at github1), which was reported to have the best performance in previous studies.
We used the variants with the best perfor- mance in the actual test, according to the original open- source code (publicly available at github2), namely UniF- BiAGRU for MELD and AGRU-BiCNN for IEMOCAP.
We follow DAG to use the RoBERTa [43] to extract utterance feature, and choose the variants reported with best performance, namely DAG-EEC provide by the original open-source code (publicly available at github3) during our experiments.
We can ﬁnd that most of the CER models are pre- dominately sensitive to the emotion labels of conversational context, rather than the utterance ’ s content itself.
We use multi-task learning to achieve the ultimate goal of semantic semantic context based augmenta- tion of the CER model.
We call the direc- tion in which the dialogue proceeds in chronological order as forward direction, and the reverse as backward direction.
We further incorporate the proposed contrastive learning module (i.e., C3ER) into each baseline model and explore if it can lead to a performance improvement.
We just use the textual feature in both of IEMOCAP and MELD.
We use full conversations as the counting unit of batches.
We record the hyper-parameters of the Bi-C3ER branch for reproducing the experiment result in Tables 3 and 4.
We can draw the following observations: (1) C3ER can improve the classiﬁcation accuracy over all the baseline methods.
We also investigate the inﬂuence of the number of negative samples on the emotion classiﬁca- tion results for the IEMOCAP dataset.
We also designed three different replacement modes, which are detailed in Section 4.
We also conduct the signiﬁcant test under the hypothesis that the CER models without C3ER have a better perfor- mance than the models with C3ER, and the statistical signif- icance result is reported by permutation test with p <0:1.
We will discu ss them separately below.Fig.
We suspect that C3ER can help the CER model capture the structure of a conversation inferred from the semantic context, so as to reduce dependence on the conversational structure.
The contributions of this paper are: 1) We propose an informativeness measure to repre- sent the inconsistency between the estimated labels of unlabeled samples and the true label distribution of labeled samples.
For an unlabeled sample xxxxxxxU j, its estimated label vector is ^yyyyyyyU j; T¼^yU j; 1; ...; ^yU j; jTjhi ¼f1ðxxxxxxxU jÞ; ...; fjTjðxxxxxxxU jÞhi: (1) 2.3 Conditional Label Distribution Model gt To represent the label distribution of the labeled dataset in TasksTdis, we also construct a set of models fgtgt2Tdis.F o ra Task tinTdis, gttakes the labels of the related tasks Trel tas inputs and outputs the conditional label distribution in Task t. In MDEE, we estimate the conditional label distribution of each task from the remaining ones, i.e., Tdis¼T¼f Valence estimation, Arousal estimation, Dominance estimation gand Trel t¼fTn tg.gtadopts k-nearest neighbors (kNN) regressor with k¼5, which takes the estimated labels of an unlabeled sample xxxxxxxU jinTrel t, i.e., ^yyyyyyyU j; Trel t, as inputs, ﬁnds its nearest labeled neighbors in the label spaces of Trel t, and averages their labels in Task tas the conditional task label ~yU j; t, i.e., ~yU j; t¼gt^yyyyyyyU j; Trel t/C18/C19 ¼kNN ^yyyyyyyU j; Trel t/C18/C19; t2Tdis: (2) In SECE, the conditional distributions of the dimensional emotions are computed from the estimated categorical emo- tion probabilities ^yyyyyyyC, i.e., Tdis¼fValence estimation, Arousal estimation, Dominance estimation g, andTrel t¼f Emotion classiﬁcation g. We obtain the conditional task labels of the dimensional emotions from the estimated cate- gorical emotion probabilities ^yyyyyyyCthrough gð^yyyyyyyCÞ.
We employ two different inconsistency calculation functions for MDEE and SECE to demonstrate their ﬂexibility.
We propose inconsistency-based multi-task active learn- ing (IMAL), which selects the unlabeled sample xxxxxxxU qwith the maximum inconsistency and queries for its groundtruth label in all tasks, i.e., q¼arg max j¼1;...NUdj: (7) The pseudo-code of IMAL is shown in Algorithm 1.
fort2T do UsefxxxxxxxL i; yL i; tgNL i¼1to train ft; end fork¼1: Kdo Estimatef^yyyyyyyU j; TgNU j¼1ofXUusing (1); fort2Tdisdo Construct the conditional label estimation function gt usingfyyyyyyyL i; Trel t; yL i; tgNL i¼1; Obtain the conditional task labels f~yU j; tgNU j¼1ofXUusing (2) for MDEE or (4) for SECE; end Compute the inconsistency fdjgNU j¼1using (5) for MDEE or (6) for SECE; Select the most inconsistent sample xxxxxxxU qusing (7); Query for yyyyyyyU q; T, groundtruth labels of xxxxxxxU qin all the tasks; XU XUnxxxxxxxU q, NU NU/C01; XL XL [ðxxxxxxxU q; yyyyyyyU q; TÞ, NL NLþ1; fort2T do UsefxxxxxxxL i; yL i; tgNL i¼1to update ft; end end 2.6 Inconsistency-Based SSL We adopt self-training SSL to exploit the unlabeled samples with high consistency between the estimated labels and the conditional task labels.
We attempted to adopt different inconsistency measures and self-training sample selection rules in MDEE and SECE to verify the ﬂexibility of IMCL.
3.1 Datasets and Feature Extraction We veriﬁed the performance of our proposed IMAL and IMCL in MDEE on three public affective computing data- sets, VAM, IAPS and IEMOCAP, with Valence-Arousal- Dominance dimensional emotion annotations.
We used 46 acoustic features, including nine pitch features, ﬁve duration features, six energy features, and 26 Mel Frequency Cepstral Coefﬁcient (MFCC) features, as in [4], [10], [23].
We used only the audio modality.
3.2 Experimental Setup We compared the performances of the following seven sam- ple selection strategies: 1) Baseline-all (BL-all), which assumes all samples in the data pool are labeled, and uses them to construct the models for each task.
We set the threshold ttin Task tto et=2, where etwas the root mean squared error (RMSE) of ft onXL.
We did not assign pseudo-labels of categori- cal emotion to the unlabeled samples.
We ﬁrst randomly shufﬂed the indices of the speakers, and then added all samples from each indi- vidual speaker according to the shufﬂed order one by one, until the total number of selected samples reached 30% of the original dataset size (947).
We used RMSE and correlation coefﬁcient (CC) to evalu- ate the performance of the regression models, where RMSE directly indicates the prediction error and was our primary performance measure.
We can also observe that among the three single-task AL algorithms, whereas usually ST- tranked ﬁrst in Task t (the task it focused on), it ranked poorly in the other two tasks.
4.3.2 Effect of Threshold ttttttt We also conducted experiments with ttttttt¼feeeeeee; eeeeeee=2; eeeeeee=5g [where each et2eeeeeeewas the training RMSE of the RR model in Task t] while ﬁxing a¼0:5.
We evaluate our proposed multi- modal approach using the Turkish Audio-Visual Bipolar Disorder corpus that we have recently collected and made available to the research community [3], [14], and push the state-of-the-art performance achieved on the corpus so far.
We discuss our results extensively in the light of our quanti- tative ﬁndings, provide insights and point out to challenges in this problem.
We discuss our ﬁndings in Section 5 and provide some ﬁnal remarks.
We caution the reader that the reported accuracies in these works (including the present paper) are not clinical results, but a good indication of the possibilities of automatic analysis approaches.
We use the Google Automatic Speech Recognition (ASR) tool4to convert the interviews to text, and obtain one text segment per task for each interview.
We experiment with both row-level and column-level normalization.
3.3 Feature Selection We use high-dimensional feature sets in our experiments.
We tested the tree feature selection method [54], which is expected to be robust against overﬁtting.
We report experi- mental results with this approach, even though ultimately, it did not yield improved test set results.
We use a radial basis function (RBF) kernel K, as suggested in [58].
We investigate how an automatic system can best extract indica- tors from each modality, and how they complement each other by providing context or including more discrimina- tive information.We ﬁrstly experiment with audio, speech, and text modali- ties separately.
We sample 500 times randomly and ﬁnd the values that maximize the UAR of the ﬁnal fusion model.
We also experiment with early (feature level) fusion meth- ods.
We evaluated only a small number of models on the test set to prevent overﬁtting.
4E XPERIMENTS AND RESULTS We report both unimodal and multimodal experimental results in this section.
We also used original eGeMAPS feature set [33].
We experiment with LIWC, TF-IDF and polarity features (see Section 3.1.2) for the text classiﬁcation.
We then reduce the dimension- ality using PCA (retaining 99% variance) to 49 dimensions, and apply tree-based feature selection.
We use 4F-CV and MM1 scores to select the fusion systems that will be used for the limited test set probes.
We also limit the test set probes to 10, including the unimodal constituents.
We also obtain the test set results of the constituent unimodal models (i.e., eGe- MAPS10, eGeMAPS10 with tree feature selection, eGe- MAPS, LIWC, and FAU) in order to report their MM1 scores on the test set.
We further analyzed the contributions of each modality/ feature set on the fusion performance of the top three sys- tems in the 4F-CV setting with a randomization test.
We observe that within each sys- tem, the contributions of the modalities are similar, however the ranking changes per system.
We report the results with OMSVM at the end of Table 6.
We note that such symptoms are not seen in each patient, and must be treated cautiously.
We then trained models to predict these activities using the constit- uent feature sets in our system.The results of the YMRS item activity recognition experi- ments are summarized in Table 7.
We also see that YMRS5 (Irritability) activ- ity is also a difﬁcult task to generalize, where the multimodal Fig.
We attribute this to the acoustic features that capture arousal that also corre- late with sexual arousal.
We note that this aspect addresses a long standing source of violence on psychiatry nurses [70].
We have performed a comprehensive analysis of fusion of modalities for predict- ing mania levels.
We achieve 64.8% test set UAR on this conﬁguration, which advances the state-of-the-art on the BD dataset.
We note two limi- tations related to this modality, which can be tackled in future studies.
We have, in a preliminary experiment, manu- ally transcribed one task to assess the performance of the automatic translation, and veriﬁed that it was producing comparable results with the manual translation.
We position AGAIN at the intersection of traditional affective computing corpora and datasets with a focus on interactive emotion elicitation.
We argue that the use of video games as interactive elicitors combined with tradi- tional affective labels can also help bridge the gap between AC and games user research.
We focus on self-reported labels to better capture the subjec- tive intricacies of gameplay.
We put careful consideration to create software which is aesthetically pleasing, representative of popular sub-genres of games, can be understood immediately with a basic level of game literacy [51], and produces a coherent and consistent dataset without the need of heavy pre-proc- essing.
We have chosen Rank- Trace as our annotation framework for this dataset.
We remove duplicate values from the dataset, as well as sessions which are either too short (less than 1 minute) or too long (more than 3 minutes) due to software or technical errors during crowdsourcing.
We also prune sessions which have less than 10 annotation points, assuming that the participant was unresponsive.
We apply the cumulative DTW distance as a similarity measure between arousal traces, in order to remove irregular annota- tion patterns; we do not transform any of the signals.
We remove all sessions which fall more than two standard deviations closer to zero from the average cumulative distance (the left tail of the distribution).
We remove all sessions which fall more than two standard deviations away from the average summedTABLE 4 The General Gameplay Features of AGAIN feature description time_passed time counted from the start of the recording score player score input_intensity number of keypresses input_diversity number of unique keypresses idle_time percentage of time spent without input activity inverse of idle_time movement distance travelled + reticle moved (in shooters) bot_count number of bots visible bot_movement bot distance travelled bot_diversity number of unique bots visible object_intensity number of objects of interest object_diversity number of unique objects event_intensity number of events event_diversity number of unique events Fig.
We apply preference learning through a pairwise transforma- tion.
We initialise RFs with their default parameters.
We note that the presented machine learning models are quite preliminary, aiming to showcase a use-case for the dataset along with a proposed cleaning and modelling pipe- line.
We introduce the Sequential Random Deep Q-Network (SRDQN) method to learn a policy for backchannel generation, that explicitly maximizes user engagement.
We suggest the use of batch reinforcement learning (batch-RL) to train a robot for engaging behaviors in an off- line manner.
We achieve this by introducing a reward factor which penal- izes the rewards that result in excessive number of smiles/nods.
We also note that a very recent work [68] uses conser- vative Q-learning as a batch-RL algorithm to learn a back- channel policy that enhances engagement while statistically matching the human laughter generation in dyadic conver- sations.
3M ETHODOLOGY We formulate the problem of backchannel generation in HRI as a Markov decision process (MDP), which we solve with ofﬂine batch-RL using a human-human interaction dataset as the batch of samples.
We represent the user state with a statistical summariza- tion of Mel-Frequency Cepstrum Coefﬁcients (MFCCs) and prosody features from the user ’ s speech.
We work with the IEMOCAP dataset [75], which is designed to analyze expressive human interactions.
3.4 Algorithm: Sequential Random Deep Q-Network (SRDQN) We introduce the Sequential Random Deep Q-Network (SRDQN) as an off-policy batch-RL algorithm for learning a backchannel policy for robot during human-robot inter- action.
The Bellman control equation then takes a form similar to that proposed by deep Q-net- work (DQN) algorithm [27], and is given by Qðst; at; uÞ¼rtþgmax a^Qðstþ1; a; u0Þ: (2) We make the standard assumption that the future rewards are discounted by a factor of g¼0:99per time- step.
We introduce the hyperparameter reward factor (k) for scaling rewards during training.
We address this large deviation of learned policy from the behavior policy by penalizing the rewards.
We use the simple technique of scaling down the rewards near smiles and nods, as given in Equa- tion (4).The value of reward factor kwas selected after a series of experiments such that backchannels were reduced in number, yet without becoming fewer than those in the dataset.
We perform the training ﬁve times for each method for 1500 epochs to address the variation seen in the converged policy when training is repeated.
We use fourdifferent types of evaluation metrics: Bellman residual, backchannel frequency, off-policy policy evaluation and similarity to human behavior.
4.2.2 Backchannel Frequency We also consider the fraction of states of the dataset where the new policy triggers a backchannel.
We use the step-wise weighted importance sampling (step-wise WIS) method to evaluate the learned policies.
We further extend the OPE analysis with two othercommonly used baseline strategies, supervised learning (SL) and mirroring.
We analyze the similarity by observing the statistical metrics of min, max and mean.
We observe even poorer result for smiles with NFQ and DQN policies, with very large chunks of sequen- tial smiles.
5U SERSTUDY We designed a human-robot interaction (HRI) experiment to assess the effectiveness of the learned backchannel policy in keeping the participants engaged.
We aim to evaluate the interaction by noting the engage- ment indicating signals triggered during the interaction (Section 3.2), and thorough a feedback questionnaire ﬁlled by the participants.TABLE 2 Off-Policy Policy Evaluation with Step-Wise Weighted Impor- tance Sampling Estimated Vp Dataset NFQ DQN SL Mirror SRDQN Nods 24.6 24.8 24.2 22.0 26.4 29.8 Smiles 24.6 26.5 20.0 21.2 26.9 27.8TABLE 3 Statistics of Smile & Nod Duration (Sec) Nods Smiles Min Max Mean Min Max Mean Dataset 0.22 7.1 1.20 0.08 5.6 0.92 SL 0.10 1.42 0.42 0.1 4.97 0.55 NFQ 0.10 140 1.50 0.10 120 2.40 DQN 0.10 21 0.84 0.10 160 2.01 SRDQN 0.10 6.3 0.33 0.10 13 0.771846 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL.
We deﬁne a ‘ cool down ’ time of 2 seconds to be used following the execution of each backchannel, during which no action is taken by the robot.
Player1 We should go the forest.
We don ’ t have the tools to kill or capture animals.
We deﬁned a running-mean engagement EðtÞto observe long-term accumulation at time tas EðtÞ¼ht t: (7) where htis the number of CEs until time tin a given session.
We also deﬁned a session engagement as a single scalar to represent engagement in each interaction, represented by EðTÞwhere Tis the session length.
6C ONCLUSION We have proposed a generation model for non-verbal back- channel behaviors of a robot, smiles and nods, to engage humans during HRI.
We have demonstrated the use of recorded human-human interaction data to learn near-opti- mal policies with batch-RL algorithms.
We need not restrict ourselves to the goal of engagement, but multiple design objectives can be aimed.
We should however note that the size of the solution space of an RL problem grows exponentially with each additional feature describing the state [87].
We Robot Conf., Univ.
We believe the open- released dataset could beneﬁt the other research works in the FER community.
We would like to point out that, given the human lan- guage is a living entity, some recent studies [47], [48], [49] propose a variety of emotion classes that defend/revise the Shaver ’ s model [7].
We use the 135 emotion category names as keywords, accompanying several other sufﬁx words such as expression, feeling, and face, to query for web images with matching titles by internet search engine indexing.
We ﬁrst adopt the facial expression embedding model [33] to evaluate the facial images between different emotion categories.
We utilize Directed Hausdorff Distance to measure the one-sided similarity from CitoCj, which can be formu- lated as following: dHðCi; CjÞ¼max nmin mkVn i/C0Vm jk2 2: (1) Notably, this metric is asymmetric as dHðCi; CjÞdoes not necessarily equal to dHðCj; CiÞ, and thus it is suitable for the similarity modeling purpose.
We also illustrate the detailed network structures in the appen- dix section.
4.1 Implementation Detail We randomly split the Emo135 dataset into training, valida- tion, and testing set by 70%; 15%; 15%, respectively.
We implement our training framework based on Pytorch3.
We use a stochastic gradient (SGD) optimizer for optimization and train the entire framework for 100 epochs.
We blindly select examples from each category according to its size.
We believe it would be meaningful to increase the manual evaluation scale in the future.
We show the percentage of the three judgements that each category receives, arranging from “ absolutely agree ” t o “ disagree ”.
We also design a baseline consisting of a pre-trained VGGFace2 model [61] and the same MLP layers as the ResNet-50 baseline.
We use the original prototypical rating results [7] as reference and calculate the Pearson Correlation Coefﬁ- cients (PCC) between each model ’ s output and the original matrix (Table 3).
We conduct thorough evaluations on both the dataset labeling quality and the baseline recognition method.
5.4 Baseline Methods We compare our proposed CNCM with the following groups of baselines: /C15Rule-based methods: CB [46] is also a traditional method that introduces commonsense knowledge into the ECE task to reveal emotion causes.
4E XPERIMENT 4.1 Dataset and Experiment Setting We conduct experiments on the Amazon-reviews bench- mark dataset for cross-domain sentiment classiﬁcation [60] with a few support instances of the target domain.
We analyze the experimental results from four perspectives as follows.
We analyze that not all the provided support features (or instances) can beneﬁt identi- fying the sentiment polarities of the query instances.
We analyze that the domain-invariant fea- tures are scarce in the given few support instances, which limits the performance of sentiment analysis in the target domain.
We observe that they mainly focus on capturing the domain-invariant features but ignore the domain-speciﬁc features.
We analyze that the performance of the supervised learning based base- lines highly depends on large-scale labeled data of target domain.
We analyze that the rela- tional knowledge can effectively enrich the domain-speciﬁc information based on the provided few support instances and beneﬁt the performance of cross-domain sentiment analysis tasks.
We consider that the discrepancy of the embedding spaces from the graph feature encoder and domain-adapted BERT encoder leads to bias in the dis- tance metric, thereby degrading the performance of proto- typical networks.
We analyze that the sentiments of the aspect-opin- ion pairs can be derived through the sentiments of their con- textual aspect-opinion pairs.
4.2.4 Analysis for N-Hop Knowledge Linking Strategy We conduct the comparative experiments between our pro- posed model with 1-hop knowledge linking strategy and that with 2-hop knowledge linking strategy adopted in the Knowledge Graph Construction of Phase 1.
We can observe that the semantic understanding of the domain-spe- ciﬁc (i.e., the target domain) features are signiﬁcant for thecross-domain learning.
The main contributions of the present work are as follows: 1) We propose a novel spatio-temporal deep learning transformer framework for video based micro- expression recognition, which we name Short and Long range relation based Spatio-Temporal Transformer (SLSTT), the structure whereof is summarized in Fig.
2) We use matrices of long-term optical ﬂow, computed in a novel way particularly suited for MER, instead of the original colour images as the input to our net- work.
3) We design a temporal aggregation block to connect spatio-temporal features of spatial relations extracted from each frame by multiple transformer encoder layers and achieve video based MER.
We evaluate our approach on the three well known and popular ME databases, Spontaneous Micro-Expression Corpus (SMIC) [34], Chinese Academy of Sciences Micro- Expression II (CASME II) [35] and Spontaneous Actions and Fig.
We used three LSTM layers in the aggregation block.
We also design a comparative experiment to demon- strate the effectiveness of the LSTM aggregator, the details of which are described in the Section 4.3.2.
We begin with a descrip- tion of the data sets used, follow up with details on the data pre-processing performed, relevant implementation details, and evaluation metrics, and conclude with a report of the results and a discussion of the ﬁndings.
We adopt the Real-time Intermediate Flow Estimation (RIFE) method [61], which uses an end-to-end trainable neural network, IFNet, which speedily and directly estimates the intermediate ﬂows.
We then apply RIFE to interpolate between I0and ^I0:5to get ^I0:25, a n ds oo n .I no u r experiment, we prioritize interpolation in the temporal vicin- ity of the apex frame.
We resize our input images to 384/C2384pixels and split each image into patches with 16/C216pixels, so that the number of patches is 24/C224.
We have tried other options with different number of frame, but it didn ’ t work any better.
We only use long-term optical ﬂow in experiments, as motivated by the arguments dis- cussed in Section 3.1.
4.3.2 Mean Versus LSTM Aggregator We compare our LSTM aggregator with an alternative which uses the simple mean operator for temporal aggrega- tion.
We use accu- racyandmacro F1-score to assess the recognition performance.
4.4 Results and Discussion We compare the performance of the proposed approach with baseline handcrafted feature extraction methods and the most prominent recent deep learning based methods on the widely used micro-expression databases, SMIC-HS, CASME II, and SAMM, described in the previous section, both in the SDE and the CDE settings.
We use transformer encoder layers with multi-head self-attention mechanism to learn spatial rela- tions from visualized long-term optical ﬂow frames and design a temporal aggregation block for temporal relations.
We hypothesize that learning optical ﬂow on face motion data will improve the quality of predicted ﬂow on faces.
We generate optical ﬂow ground truth using facial key-points in the BP4D-Spontaneous dataset.
In doing so, we explore how well a deep network can perform in this tas k. We demonstrate that the pro- posed architecture will work w ell for faces compared and com- pare it to traditional optical ﬂow algorithms.
We use the BP4D-Spontaneous dataset [5] consisting of videos of 41 participants with different facial expressions to generate the ground-truth optical ﬂow between every pair of consecutive frames in the dataset.
We then use this facial optical ﬂow ground truth to train a convolutional autoen- coder based architecture, FlowNetS [6] (specialized for opti- cal ﬂow estimation), to learn optical ﬂow specialized for facial motions, meaning that the motion learned should exhibit local coherency as would be expected on faces.
We also modify the architecture by adding a cyclic loss to help the network reconstruct the latter image in a given image pair using the optical ﬂow predicted by the network.
We argue that adding this reconstruction in the learning frame- work improves the predicted optical ﬂow by guiding it using the structure of the image pairs.
We perform an abla- tion study with different loss functions, and compare the/C15Muhannad Alkaddour and Usman Tariq are with the American University of Sharjah, Sharjah 26666, UAE.
We then complement the structure with a cyclic loss.
We follow this up by a sur- vey of optical ﬂow methods as applied to faces in particular, and how optical ﬂow is used in tasks such as micro-expres- sion detection.
2.2 Optical Flow and Facial Expression Analysis We now discuss various optical ﬂow methods as applied to facial expression analysis, many of which are based on deep networks.
We attempt to learn optical ﬂow from the face movements themselves.
We mention a few implementations of deep networks in facial expression analysis using optical ﬂow.
We introduce a noisy optical ﬂow dataset, that we generate using the motion of sparse facial landmarks.
We then learn a network foroptical ﬂow estimation, specialized for movements induced by facial expressions.
We then complement the structure with a cyclic loss.
We show that our modiﬁed architecture outperforms several other networks used for optical ﬂow estimation.
We use the BP4D-Spontaneous dataset [5], which consists of 41 subjects with 8 video sequences each, containing videos of elicited emotions.
1 shows the overall pipeline for a pair of frames and how they can be used for dataset generation and CNN training.1 We introduce the notation that we ’ ll use throughout this section to generate the optical ﬂow ground truth from the BP4D-Spontaneous dataset [5].
We use the triangulation TTTTTTTk/C01to capture the local motion on every triangle in the face partition from frame fk/C01to frame fk.
We can deﬁne tttttttl; /C3¼tttttttlllllll; 11111113/C21/C0/C1T2R3/C23to be the matrix of homogeneous coordinates of each vertex.
We use the barycentric coordinates to compute the interi- ors of all the triangles in TTTTTTT0, and then learn each afﬁne map AAAAAAAl 0as described above to map all the triangle interiors from TTTTTTT0toTTTTTTT1.
We test all points in this way using a rectangular discrete grid surrounding the triangle.
We only do this to deﬁne the optical ﬂow ﬁeld at each frame, but continue to learn the afﬁne maps on the irregular grids, since we wish t op r e s e r v et h es a m eb a r y c e n t r i cc o o r d i n a t e so b t a i n e di n f0for all frames.
We note that structures inherent only to the second input cannot be reproduced in the warped output, since the warping function only changes pixel locations from the ﬁrst input, and does not contain any learnable parameters.
We denote by ðXi; Xiþ1Þthe pair of successive input frames, where Xi; Xiþ12R384/C2512/C23and YYYYYYYi¼f ðYiÞkg5 k¼1, ^YYYYYYYi¼f ð ^YiÞkg5 k¼1contain the intermediate multi-scale ground-truth and prediction ﬂow ﬁelds respectively, where the elements ðYiÞk, ð^YiÞk2RHk/C2Wk/C22.
We initialized the learn- ing rate aat1e/C04for faces and 5e/C05for FlyingChairs and scheduled similar to [6].
We then used a disjoint set of 3k image pairs from the face dataset as the test set.
We then tested the model, learnt on 15k training image pairs from the face data, separately on the face, FlyingChairs, and Sintel test sets [48].
We report these results in Table 1.
We expect to see an improvement in the ﬂow prediction due to the cyclic loss.
The xiþ1; j; k; ^xiþ1; j; kare values of thejth pixel of Xiþ1; ^Xiþ1at color channel k. We also note that ^Xiþ1is a function of the ﬁrst image of the input pair, Xi, and ^Yi, which is the ﬂow prediction with largest resolu- tion.
We refer to this as Case I.
We refer to this as Case II.
We then tested the trained networks on the test set of 32416 image pairs.
In Case II, the loss function Li 3ðYi; ^YiÞis deﬁned as: Li 3ðYi; ^YiÞ¼1 HWXHW j¼1arctanyyyyyyy/C3 ij/C2^yyyyyyy/C3 ij/C13/C13/C13/C13/C13/C13 2 yyyyyyy/C3 ij/C1^yyyyyyy/C3 ij0 @ 1 A (8) The total loss function is then a weighted sum of the loss functions JðX; ^X; YYYYYYY; ^YYYYYYYÞ¼1 MXM/C01 i¼0½/C211Li 1ðYYYYYYYi; ^YYYYYYYiÞþ/C212Li 2ðXiþ1; ^Xiþ1Þ þ/C213Li 3ðYi; ^YiÞ/C138 (9) We trained the network on only the faces dataset for 14 epochs and 228160 training pairs, with /C211¼0:3, /C212¼0:5, /C213¼0:2, and learning rate 2:5e/C06.
We initialized the weights from the results of Experiment 2 (Case I), to see if there is any improvement in ﬂow prediction after adding L3.
We make use of their labeling for the SMIC dataset.
We crop the faces based on keypoints obtained using the OpenFace 2.0 toolbox [43] for SAMM.
We follow their the recommended approach in [8] to train the STSTNet.
We use the publicly available codeprovided by the authors [8], and thus replicate the exact same network architecture, with a learning rate of 5e /C05and maximum epochs set to 500.
We note that the RGB input images, described in Section 4.4, are resized to a resolution of 28/C228/C23.
We also compute the TVL1 optical ﬂow on SAMM and SMIC, as done in [8], to compare its performance with the optical ﬂow features obtained from other networks.
We will combine the metrics to a single scalar, which we will refer to as the aggregated metric.
We then show the results of the ablation study for the experiments trained on the full dataset, and compare the networks using the average EPE and AAE.
5.1 Results for Ablation Studies We ﬁrst describe the initial results of Exp.
We report the performance in terms of average EPE.
We conjec- ture that this may show the usefulness of our generated dataset to problems that are even unrelated to faces.
5.2 Comparison With Other Networks We now compare the results with other notable optical ﬂow implementations.
We note that the fol- lowing remarks for the remainder of this section are qualita- tive in nature and are based on a very small subset, but nevertheless yield some insight to accompany the statistics from Tables 2 and 3.
We ﬁrst observe the differences in ﬂow predictions among the networks trained on our dataset.
We use the offset values of 10 pixels in all four directions, for cropping.5.3 Results of Micro-Expression Detection We now report on the results of the experiments described in Section 4.4.2 for micro-expression detection.
We note that the F1-scores are typically lower than the pre- cision and recall since these are aggregated metrics, i.e., the F1-score averaged over all F1-scores in the LOSOCV, and is not the harmonic mean of the aggregated precision and recall.
We hypothesize that our method will overcome the per- formance difference in some of the results if we use a denser keypoint tracker during the optical ﬂow training phase to generate the BP4D ground-truth.
ACKNOWLEDGMENTS We thank Ms. Florence Wacheux for her input in the review of the manuscript draft.
We collected labeled vocalizations in real-world settings with eight mv * communicators, with communicative and affective labels provided in-the-moment by a close family member.
We analyze labeling and data collection practices for each participating family, and discuss the classiﬁcation results in the context of our novel real-world data collection protocol.
We show that nonverbal vocalizations can be classiﬁed using audio alone for each individual.
We present evaluation and sampling strategies to work with messy, real-world data with uneven sample distributions and varying background noise.
We implement and evaluate a custom feature set designed for nonverbal vocalizations for mv * individuals.
We also analyze the data collection and labeling practices for each participant, and discuss model performance in the context of how data was collected by each participant.
We experimented with multiple labels while piloting the study but found that asking labelers to desig- nate multiple classes imposed too high of a cognitive load and reduced the overall ﬁdel ity of the marked labels.
We contribute to an improved understanding of non- verbal vocalizations from mv * communicators.
We hope that our study of nonverbal com- munication with mv * communicators will also lead to improved awareness among the community-at-large that Fig.
