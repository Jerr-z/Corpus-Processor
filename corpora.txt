Aspect-Based Sentiment Quantiﬁcation
Vladyslav Matsiiako, Flavius Frasincar , and David Boekestijn
Abstract— In the current literature, many methods have been devised for sentiment quantiﬁcation. In this work, we propose
AspEntQuaNet, one of the ﬁrst methods for aspect-based sentiment quantiﬁcation. It extends the state-of-the-art QuaNet deep
learning method for sentiment quantiﬁcation in two ways. First, it considers aspects and ternary sentiment quantiﬁcation concerning
these aspects instead of binary sentiment quantiﬁcation. Second, it improves on the results of QuaNet with an entropy-based sorting
procedure instead of multisorting. Other sentiment quantiﬁcation methods have also been adapted for ternary sentiment quantiﬁcation
instead of binary sentiment quantiﬁcation. Using the modiﬁed version of the SemEval 2016 dataset for aspect-based sentiment
quantiﬁcation, we show that AspEntQuaNet is superior to all other considered existing methods based on obtained results for various
aspect categories. In particular , AspEntQuaNet outperforms QuaNet often by a factor of 2 on all considered evaluation measures.
Index Terms— Aspect-based sentiment quantiﬁcation, sentiment analysis, sentiment quantiﬁcation
Ç
1I NTRODUCTION
WIDESPREAD usage of the Web has enabled customers’
instantaneous and elaborate sharing of feedback or
reviews, conveying useful information for businesses. This
kind of information could arguably be more reliable than
questionnaires people are often unwilling to answer, while
simultaneously being more readily available.
Considering the speed at which the amount of data (partic-
ularly textual data like user reviews, comments, and forum
posts) on the Web increases, it becomes practically impossible
to analyze all of the data manually. Nevertheless, analysis of
this data remains an important task for both the private and
public sectors. For instance, restaurant chains prefer knowing
as precisely as possible what people think about their new
meals [1], and ﬁnancial analysts could use sentiment retrieved
from Tweets to potentially explain price swings in stocks [2].
It is therefore essential to use algorithms speciﬁcally designed
for extracting and determining the sentiment of texts, which
could afterward be used to create valuable insights.
The described branch of resear ch is called sentiment analy-
sis (SA) [3]. Its main objectiv e is determining the sentiment
towards a certain entity. Wh ile general SA methods aim to
determine the sentiment of an entir e text [4], aspect-based senti-
ment analysis (ABSA) is concerned with identifying the senti-
ment level per aspect (feature) within that text [5]. For example,
the following sentence contains two aspects scenario andacting .
“The scenario of that movie was great, but the
acting could have been better.”In this sentence, the sentiment towards the aspect scenario
is positive, while the sentiment towards acting is negative.
Using the methods of ordinary SA, we would run into the
issue that the review conveys both negative and positive
sentiment. However, by performing the SA task on an
aspect level, we would obtain separate sentiment scores for
each of the involved aspects. Therefore, with the methods of
ABSA, we would be able to obtain more in-depth insights.
For example, knowing particular sentiment scores for each
feature of a certain product could provide useful informa-
tion on how to improve it, and importantly, which parts to
improve.
While for some use cases personal sentiment scores
are important (e.g., in CRM marketing one wants to tar-
get every user separately) [6], there are many cases in
which only aggregate numbe rs matter. For example, for
political campaigns, it is only important to know the
share of the population that supports a certain policy or
candidate, while it makes no difference to know senti-
ment estimates on the individual level [7], [8]. Another
example would be estimating the severity of an epidemic
by means of identifying medical reports with a certain
diagnosed disease [9]. For this reason, there exists a
branch of research that explor es sentiment quantiﬁcation
(SQ) [10].
Until now, despite the abundance of aspect-based senti-
ment classiﬁcation methods, there has been little research
connecting SQ with ABSA [11], [12], [13]. Hence, the nov-
elty of this research lies in bridging the gap between those
two ﬁelds. In particular, it compares the performance of
usual sentiment quantiﬁcation techniques on the aspect
level as well as proposes a novel method, AspEntQuaNet,
based on current state-of-the-art ﬁndings from both SQ
and ABSA. All researched quantiﬁcation methods are also
extended to be applied to terna ry (positive, neutral, and
negative) instead of binary (p ositive and negative) senti-
ment classiﬁcation. Additionally, for the purposes of this
paper, we introduce the term aspect-based sentiment
quantiﬁcation (ABSQ), a ﬁeld at the intersection of SQ and
ABSA./C15The authors are with Econometric Institute, Erasmus University Rotter-
dam, 3062 Rotterdam, PA, Netherlands. E-mail: matsiiako@gmail.com,
{frasincar, boekestijn}@ese.eur.nl.
Manuscript received 8 March 2022; revised 7 October 2022; accepted 23 Octo-
ber 2022. Date of publication 31 October 2022; date of current version 15
November 2022.
(Corresponding author: Flavius Frasincar.)
Recommended for acceptance by E. Cambria.
This article has supplementary downloadable material available at https://doi.
org/10.1109/TAFFC.2022.3218504, provided by the authors.
Digital Object Identiﬁer no. 10.1109/TAFFC.2022.32185041718 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022
1949-3045 ©2022 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See ht _tps://www.ieee.org/publications/rights/index.html for more information.
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:22 UTC from IEEE Xplore.  Restrictions apply. The paper is structured as follows. In Section 2, relevant lit-
erature concerning SQ and ABS Ai sp r o v i d e d .S e c t i o n3o u t -
lines the data used in this pape r. Next, Section 4 presents
detailed explanations for the proposed methods. Following,
results obtained using the inv estigated methods are assessed
in Section 5. Finally, Section 6 provides a summary of the
main conclusions of this paper, as well as suggests directions
for future research. All methods and models researched in
this paper are programmed in the Python programming lan-
guage; source code can be found at https://github.com/vlad-
matsiiako/ABSQ.
2R ELATED WORK
As identiﬁed in the works of [3] and [14], SA is mainly con-
cerned with locating opinion-related parts of texts, deter-
mining the sentiment they convey, and quantifying it.
There are three main levels of sentiment identiﬁcation:
document-level, sentence-leve l, and aspect-level. The ﬁrst
two aim to determine the sentiment level within, respec-
tively, a document or a sentence. When it comes to the
aspect level, both document s and sentences may include
multiple aspects towards which sentiments are expressed.
This paper focuses on the aspect level.
A comprehensive survey of ABSA was provided by [15].
As deﬁned by the authors, its goal is to ﬁnd the sentiment
towards each aspect of a certain entity. More mathemati-
cally, the task of ABSA equates to ﬁnding a quadruple of
ðs; g; h; t Þ[16]. In this quadruple, sdenotes sentiment, g
denotes the target, hdenotes the holder (the subject that is
expressing the sentiment), and tdenotes the time point of
the expressed sentiment. In reality, most methods, includ-
ing the ones proposed in this paper, aim to only identify the
pairðs; gÞ.
The three main processing steps constituting ABSA were
deﬁned by [17] to be identiﬁcation, classiﬁcation, and aggre-
gation. In the ﬁrst step, the pairs of opinions and targets in a
piece of text are identiﬁed. In the classiﬁcation step, a cer-
tain algorithm maps the opinion onto a predetermined set
of sentiment values (positive, neutral, or negative) [18], [19].
Finally, the obtained sentiment values are aggregated per
aspect to generate a certain kind of sentiment report. This
paper focuses on the last step: aggregation.
The strong point of ABSA is its greater utilization of the
information given in a text compared to SA. However, it is
essential to deﬁne the granularity of the utilized texts,
because, for ABSA, it could vary from a piece of text as
small as a sentence to a large document. In this paper, we
aim to carry out ABSA on the sentence level.
It should be mentioned that both sentiments and
aspects can be explicit or implicit. E.g., the sentence “I
expected much more.” has an explicit sentiment (expecta-
tions were not met), but only an implicit aspect since it is
not mentioned in the text what was “expected much
more” of. According to [20], who analyzed a different
dataset of restaurant reviews proposed by [21], implicit
aspect targets do not occur often. For this reason and con-
sidering that the proposed methods rely on identiﬁed
aspects, the possibility of implicit aspects will be ignored
in this paper. On the other hand, implicit sentiments will
not be disregarded.2.1 Sentiment Quantiﬁcation Task
Historically, SA was considered a sentiment task [22]. The
idea was to classify each piece of text separately and then
aggregate the results in a certain way. However, there are
situations in which a different approach, oriented on aggre-
gation, is needed. For instance, when market researchers try
to estimate the share of the population that likes a newly-
released product, they do not care about each individual
separately, but rather care only about aggregate results. It
has been argued in various examples that the idea of classi-
ﬁcation and further aggregation gives a suboptimal perfor-
mance [23], [24], [25]. Therefore, it is worth investigating
whether it is possible to design algorithms that outperform
simple classiﬁcation. This area of research is called Senti-
ment Quantiﬁcation (SQ) and was ﬁrst introduced by [10].
At ﬁrst sight, optimizing for classiﬁcation seems equal to
optimizing for quantiﬁcation. This is not true. The intuition
behind the fact that simple classiﬁcation returns suboptimal
performance on the aggregate level is implicit in the mea-
sure that is used for evaluating performance. To illustrate
this, we look at the F1score, considered to be a common
measure for classiﬁcation algorithms
F1¼2/C1TP
2/C1TPþFPþFN; (1)
where TPis the number of true positive predictions, FPrep-
resents the number of false positives, FNis the number of
false negatives, and TNequals the number of true negatives.
Consider a sample of 100 reviews with binary sentiments, for
which the true share of positives versus negatives is 70:30.
Compare one classiﬁer obtaining the results TP¼50;F P ¼
20;T N ¼10;F N ¼20, with another classiﬁer obtaining dif-
ferent results TP¼70;F P ¼10;T N ¼20;F N ¼0. The latter
model achieves the better F1score of 0.9333, while the former
achieves a score of only 0.7143. However, for the SQ task, the
latter model gives an estimate of 80:20 positive versus nega-
tive sentiments, while the former model returned the ideal
answer: 70:30.
In mathematical notation, this means that to optimize for
SQ tasks, it is desirable to reduce jFP/C0FNjrather than
ðFPþFNÞ. For this reason, multiple other measures that
could compensate for the inaccuracy of a classiﬁer were pro-
posed in [26] and will be discussed further in this paper.
Moreover, as discussed by [23], adjusting the classiﬁers for
the purposes of SQ requires only a limited amount of train-
ing data compared to the size of the datasets one would
need to improve the accuracy of the actual classiﬁers.
2.2 Development of Quantiﬁcation Methods
One of the initial works in the ﬁeld of quantiﬁcation was
published by [27]. The author was essentially the ﬁrst to
argue that in real-world situations, the assumption of iden-
tical characteristics between training and testing data is not
necessarily true. In this work, he proposes an iterative pro-
cedure based on the “expectation-maximization” algorithm
for increasing the likelihood of the new data. Additionally,
a statistical test was developed to determine whether the
initial class distribution is different from the real-world
observations.MATSIIAKO ET AL.: ASPECT-BASED SENTIMENT QUANTIFICATION 1719
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:22 UTC from IEEE Xplore.  Restrictions apply. The next big leap in the direction of quantiﬁcation was
made by [28]. In his research, three methods were named
and proposed: “Classify and Count,” its adjusted variant,
and “Mixture Model”. The Mixture Model was found to
outperform the others. However, since the ﬁrst two meth-
ods have been much more widely used in academic litera-
ture, we will consider “Classify and Count” and its adjusted
variant as baselines in this paper.
The previous line of research was further continued by
the work proposed in [23] and [29]. Here, the author
presents a new branch of quantiﬁcation called “cost
quantiﬁcation”. The idea is that in the case of attributed
cost available and relevant to the problem (e.g., estimating
t h ea m o u n to ft i m ep e rd a yi tt a k e sf o rs u p p o r ta g e n t st o
reply to all the questions), cost quantiﬁcation might be pre-
ferred to ordinary quantiﬁcation.
Since then, there have been multiple different directions in
which the previous research developed. [30] explored the use
of probability estimations of the classiﬁer while applying an
additional scaling factor similar to the one developed by [27].
In later research, these methods are called “Probabilistic Clas-
sify and Count” and “Probabilistic Adjusted Classify and
Count,” and will thus be referred to as such in this paper.
[31] have proposed one of the ﬁrst methods that would
not be based on classiﬁcation to derive a quantiﬁer. Instead,
the authors designed a decision tree-based method opti-
mized directly for quantiﬁcation. Their results proved this
method to be the new state-of-the-art at that time.
Additionally, [32] have explored the direction of ordinal
text sentiment quantiﬁcation. The authors present OQT, a
tree-based method, which they use to quantify the data
based on ﬁve different sentiment classes (on a scale from
very negative tovery positive ). Even though the authors
obtained promising results, there has been little following
research in this domain. In this paper, we have opted for ter-
nary sentiment quantiﬁcation, with sentiment classes posi-
tive, neutral, and negative.
Based on the SVM for Multivariate Performance
Measures ( SVM perf) developed by [33], [34] developed an
SVM-based method able to optimize for multivariate loss
functions created speciﬁcally for the quantiﬁcation task.
When released, their method outperformed all previously
existing ones and became the new state-of-the-art in SQ.
Eventually, [35] came up with a new quantiﬁcation-spe-
ciﬁc method, based on neural networks, which the authors
called “QuaNet”. Tested on an SQ text dataset, it was
proven to substantially outperform all existing methods.
This method will be used as a starting point for the quantiﬁ-
cation task and further advanced in the work described in
this paper. To the best of our knowledge, our work is one of
the ﬁrst to adapt SQ methods for ABSQ.3D ATA
This section describes the datasets used in this paper. In Sec-
tion 3.1, the datasets, their structure, some basic characteris-
tics, and the employed preprocessing steps are discussed.
Subsequently, Section 3.2 provides important insights into
the datasets with regards to the distribution of data among
aspect categories and sentiment classes of review sentences.
3.1 SemEval 2016 Dataset
The ABSA dataset used in this paper is taken from the
SemEval competition held in 2016. In particular, we use
task 5 of SemEval 2016 [36]. This domain dataset is consid-
ered an obvious choice for ABSA and is used in many
papers researching this subject [37]. Additionally, there is
also a dataset from task 12 of SemEval 2015 [38].
Both datasets contain restaurant reviews. Reviews are
split into sentences, each of which might convey multiple
sentiments depending on the number of target words
within the sentence. Each opinion has a corresponding tar-
get and target category (FOOD#QUALITY, SERVICE#GEN-
ERAL, etc.), as well as a polarity (positive, neutral, or
negative). The distribution of reviews’ opinions among vari-
ous polarity classes is given in Table 1 [18].
To preprocess the dataset, which is in XML format, simi-
lar procedures as in the work of [39] are utilized. Most
importantly, we delete those observations where the target
is implicit (equal to NULL ).NULL values represent less than
10% of all observations and therefore represent a compara-
tively rare category. Additionally, according to [20], such
implicit targets appear quite infrequently in general.
3.2 Data Insights
Besides the distribution of reviews among sentiment clas-
ses, it is also of high importance to understand the distribu-
tion among aspect categories. The distributions for the
training and test data are provided in Table 2. The ﬁrst four
dominant categories were selected for the evaluation of the
results in this paper. With data from each of these catego-
ries, we will also train an aspect-speciﬁc QuaNet model,
called AspEntQuaNet. Combined, the aforementioned cate-
gories represent approximately 80% of the training data and
around 78% of the test data. Other categories were takenTABLE 1
Distribution in Percentages of Review Sentences Among Senti-
ment Classes
2015 2016
Pos. Neu. Neg. Pos. Neu. Neg.
Training 72.43 3.20 24.36 70.2 3.80 26.0
Test 53.72 5.32 40.96 74.3 4.90 20.80TABLE 2
Distribution in Percentages of Review Sentences Among
Aspect Categories
Aspect Category Training Test
FOOD#QUALITY 40.77 43.54
SERVICE#GENERAL 17.24 16.46
AMBIENCE#GENERAL 12.03 9.08
RESTAURANT#GENERAL 9.85 8.92
FOOD#STYLE_OPTIONS 6.12 7.85
FOOD#PRICES 3.73 3.38
RESTAURANT#MISCELLANEOUS 2.61 2.77
DRINKS#QUALITY 2.34 3.38
DRINKS#STYLE_OPTIONS 1.7 1.69
RESTAURANT#PRICES 1.38 0.77
LOCATION#GENERAL 1.17 1.54
DRINKS#PRICES 1.06 0.621720 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:22 UTC from IEEE Xplore.  Restrictions apply. out of consideration for testing purposes for the reason that
on their own, these categories do not represent a large
enough part of the data to draw meaningful conclusions.
For the four categories under consideration, we also pro-
vide the distribution of review sentences among sentiment
classes (positive, neutral, or negative). In Table 3, we see the
distribution of sentiment classes for the observations in the
training and test data. All four distributions are substan-
tially different from each other, even though they have simi-
lar underlying features (e.g., neutral sentiment is always the
least represented).
Table 3 shows that for some aspect categories, distribu-
tions between training and test data are quite similar. For
SERVICE#GENERAL, we ﬁnd a total absolute difference
(TAD, the sum of absolute differences in the true prevalen-
ces of each sentiment class between training and test data)
equal to 4.54%; for RESTAURANT#GENERAL, we ﬁnd a
TAD of 5.48%. However, this is not the case for the other
two aspect categories. Namely, for FOOD#QUALITY, we
obtain a TAD of 24.46%, and for AMBIENCE#GENERAL,
there we ﬁnd a TAD equal to 26.92%. The differences in
these distributions for the training and test data are not
insigniﬁcant. Therefore, it will be particularly interesting to
observe the performance of the chosen methods on the test
data for these categories, once trained to optimality.
4M ETHODOLOGY
This section provides an extensive explanation of the meth-
ods researched and proposed in this paper. First, Section 4.1
describes the classiﬁcation methods and corresponding loss
functions. Following, Section 4.2 discusses the quantiﬁca-
tion methods that are built on classiﬁcation algorithms.
4.1 Classiﬁcation Methods
This section provides the necessary methodology for the
classiﬁcation methods proposed in this paper, as well as
describes how to move from classiﬁcation to quantiﬁcation.
First, Section 4.1.1 discusses the LCR-Rot-hop++ neural net-
work architecture designed for ABSA. Then, Section 4.1.2
goes into detail about which loss functions should be
applied for quantiﬁcation purposes.
4.1.1 LCR-Rot-Hop++ Neural Network
To utilize any of the quantiﬁcation methods that are investi-
gated in this paper, we ﬁrst need to incorporate a classiﬁer.
For this purpose, we will utilize the deep learning compo-
nent of a state-of-the-art Hybrid Approach for Aspect-BasedSentiment Analysis (HAABSA) which was ﬁrst developed
by [40] and further improved by [18] (under the name of
HAABSA++).
It has to be noted that considering the fact that for Qua-
Net we will need to obtain document embeddings and sort
them in the order of increasing probabilities (as will be dis-
cussed in detail in Section 4.2.6), we can only use the second
part of HAABSA++ for this method (LCR-Rot-hop++).
Therefore, to ensure a fair comparison with the other meth-
ods, we will omit the ontology part completely.
LSTM with attention has become a standard practice and
starting point for the ABSA task [39], [41]. LCR-Rot is a
model proposed by [42] with the intention of allowing the
opinionated expressions to interchange information with
their contexts on both sides of the target. In the research
work of [40], this model is further extended under the name
of LCR-Rot-hop by adding repeated attention.
[18] extends this model even further under the name of
LCR-Rot-hop++ by experimenting with various types of
word embeddings and adding hierarchical attention. In
fact, the largest disadvantage of LCR-Rot-hop is that for the
computation of target2context and context2target vectors
only the local information is used. Hierarchical attention is
meant to solve exactly this issue. By design, it is supposed
to represent the input sentence on a higher abstraction level.
Additionally, we have to notice that in the research work
of [18], there are four ways of implementing hierarchical
attention. As an outcome of various experiments, Method 4
(in which the weighting of attention procedure is employed
one by one at each iteration of the rotatory attention mecha-
nism for the pairs of intermediate context and target vec-
tors) was determined to perform the best. For that reason, it
will be the only method utilized in this research work.
Multiple components of LCR-Rot-hop++ will be needed
in this paper. The ﬁrst and most important is the last layer
of this model. This layer should be extracted and used as an
input for one of the quantiﬁcation methods, as it can be con-
sidered a representation (embedding) of a document (opin-
ion with respect to each aspect target). In fact, this layer
consists of the left and right representations, while each of
those consists of two vectors (in the case of the right part: rr
andrtr; the dimensions of both are 1x600). In turn, each of
the vectors is built upon bi-directional LSTMs which means
that their size is doubled (initially the size is 1x300). In the
end, we end up with 1x2400 vectors which are to be used as
input for quantiﬁcation methods.
Next to that, after experiments with word embeddings
by [18], their research found BERT word embeddings to
perform the best. BERT word embeddings are a type of con-
textual word embeddings [43]. Since we seek to identify the
possible best model and build upon this in terms of estimat-
ing prevalences, only the BERT word embedding will be
used for the purposes of this paper.
4.1.2 Loss Function for Quantiﬁers
As previously argued in Section 2.1, loss functions that are
commonly used for SA are not always suitable for SQ. In fact,
until recently there has not been extensive research about the
ideal evaluation measure for SQ. [26] proposed a collection of
8 properties (some of them mutually exclusive) which wouldTABLE 3
Distribution in Percentages of Sentiment Classes Among the
Chosen Aspect Categories
Training Test
Aspect Category Pos. Neu. Neg. Pos. Neu. Neg.
FOOD#QUALITY 73.99 3.53 22.48 85.51 4.24 10.25
SERVICE#G.156.48 2.47 41.05 54.21 2.80 42.99
AMBIENCE#G.178.07 5.70 16.23 91.53 5.08 3.39
RESTAURANT#G.178.69 1.64 19.67 77.59 0.00 22.41
1G. denotes the GENERAL subcategory.MATSIIAKO ET AL.: ASPECT-BASED SENTIMENT QUANTIFICATION 1721
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:22 UTC from IEEE Xplore.  Restrictions apply. need to hold in order for the evaluation measure to be suitable.
During extensive analysis, the authors determine that no sin-
gle measure is in line with all t he proposed properties. How-
ever, Absolute Error (AE) and Relative Absolute Error (RAE)
were found to meet the most requirements (6 out of 8).
Interestingly, AE and RAE are considered the simplest
evaluation measures for quantiﬁcation. The idea behind AE
is to calculate the absolute difference between the true and
predicted quantiﬁcation. RAE extends AE by expressing the
difference between the true and predicted quantiﬁcation in
terms of true quantiﬁcation. The formulas for these error
measures are provided in (2) and (3)
AEðp;^pÞ¼1
jCjX
c2Cj^pðcÞ/C0pðcÞj; (2)
RAE ðp;^pÞ¼1
jCjX
c2Cj^pðcÞ/C0pðcÞj
pðcÞ; (3)
where Cis the set of all sentiment classes.
It should be noted that RAE is undeﬁned in case the true
quantiﬁcation is equal to 0. For this reason, [26] proposes
the use of a certain type of additive smoothing
psðcÞ¼j/C15þpðcÞj
/C15jCjþP
c2CpðcÞ; (4)
where /C15¼1
2jsjand sis the set of opinions. However, even
with such smoothing, RAE may remain numerically unsta-
ble in case of extreme true or predicted prevalences [35].
Another commonly used evaluation measure is Kull-
back-Leibler Divergence (KLD) [35], [44], [45]. While devel-
oped much earlier, KLD was ﬁrst used for SQ only in [28]. It
is computed using (5)
KLD ðp;^pÞ¼X
c2CpðcÞlogpðcÞ
^pðcÞ: (5)
The ideal scenario is to have KLD ¼0.I nt h a tc a s e ,b o t hd i s -
tributions (predicted and original) coincide. Similar to RAE,
KLD is undeﬁned when ^pðcÞ¼0;w eu s et h es a m es m o o t h i n g
technique as shown in (4), although this could also still result
in numerical instability of the smoothed value.
4.2 Quantiﬁcation Methods
This section elaborates on each of the ﬁve various quantiﬁca-
tion methods reviewed in this paper. Section 4.2.1 discusses
the method “Classify and Count” followed by the explana-
tion of “Adjusted Classify and Count” in Section 4.2.2. Fur-
ther, Section 4.2.3 presents the adaptation of these methods
for the case of three sentiment classes. Next, Section 4.2.4 dis-
cusses the probabilistic version of those methods. It is fol-
lowed by the adaptation of these methods for three
sentiment classes in Section 4.2.5. Section 4.2.6 describes the
methodology behind QuaNet. Last, Section 4.2.7 provides
the adapted QuaNet for three sentiment categories and with
entropy-based sorting, named EntQuaNet.
4.2.1 Classify and Count
Considered to be the simplest method for the SQ task, the
idea of “Classify and Count (CC)” is to classify each data
point separately and then compute the share of populationthat is estimated to be of a certain sentiment. This can be
achieved using the formula below
^pCC
cðDÞ¼jfx2DjhðxÞ¼cgj
jDj¼TPc
hþFPc
h
jDj; (6)
where hð/C1Þis the hard classiﬁcation function, subscript h
denotes hard predictions, and cis a sentiment class.
Essentially, this can be viewed as sentiment classiﬁcation
with further aggregation. It is obvious that the perfect classi-
ﬁer is also the perfect quantiﬁer, but the opposite is not
always the case.
Although we have used three sentiment classes in this
paper instead of two, this does not form a problem for CC.
The only difference is that the formula, written out, is
slightly more extensive
^pCC3
1ðDÞ¼jfx2DjhðxÞ¼1gj
jDj¼T1hþF1h
jDj
¼T1hþF1=2hþF1=3h
jDj: (7)
In the above equation, T1his the number of correctly pre-
dicted observations of the ﬁrst category, F1=2his the num-
ber of predicted ﬁrst category observations when the true
category is the second one, F1=3his the number of predicted
ﬁrst category observations when the true category is the
third one. Furthermore, in (7) we denote the sentiment clas-
ses positive, neutral, and negative by 1, 2, and 3, respec-
tively. Without loss of generality, the formula has been
written out for the ﬁrst category.
4.2.2 Adjusted Classify and Count
Considering the formula of CC in (6), it can be seen that it
gives optimal estimates when FP¼FN. Obviously, this is
rarely the case, which is why an adjusted method was pro-
posed in [23], [28]. The concept behind “Adjusted Classify
and Count (ACC)” is slightly more intricate, as it involves
adjusting for the number of false positives versus false nega-
tives if one or the other prevails. In fact, provided the “true
positive rate” and “false positive rate” are known, we could
adjust the share to obtain the optimal result. The idea can be
seen in the following equation
^pACC
cðDÞ¼^pCC
cðDÞ/C0cfprh
ctprh/C0cfprh; (8)
wherectprh¼TPh
TPhþFNhandcfprh¼FPh
FPhþTNh, and we have
omitted superscript cdue to verbosity. The derivation for
this formula is provided in Appendix A, which can be
found on the Computer Society Digital Library at http://
doi.ieeecomputersociety.org/10.1109/TAFFC.2022.3218504.
Essentially, it shows that in the case of known true positive
and false positive rates (which in reality are estimated from
the training data), we can obtain the true prevalence of a
certain class.
Considering that the true rates ctprhandcfprhare
unknown in a real-life scenario, we need to estimate them
from our data. According to [34], in ACC, one should esti-
mate these rates from the training data via the process of1722 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:22 UTC from IEEE Xplore.  Restrictions apply. k-fold cross-validation, or by using a hold-out validation
dataset.
A potential problem with ACC is that it might return val-
ues that are outside the [0, 1] range. This happens because
ctprhandcfprhare not always optimal for the test data. For
this reason, [23] introduced a clipping procedure that
adjusts values higher than 1 to 1, and values lower than 0 to
0.
4.2.3 Adjusted Classify and Count for Three Sentiment
Classes
For the purposes of our paper, the formula of ACC given in
(8) should again be generalized to the case of three senti-
ment classes. [46] and [25] have previously provided the
general equations with which it is theoretically possible to
derive the formulas of adjusted classiﬁers for any number
of sentiment classes. However, to the best of our knowledge,
the exact formulas for the case of three classes were not pro-
vided in any of the previous research work. Therefore, the
formula that would be optimal for the purposes of this
paper should be written out in order to be used for the case
of ternary sentiment classiﬁcation.
The idea of obtaining this formula is analogous to that for
ACC. The starting point is the following two equations
Pð^1Þ¼Pð^1j1Þ/C1Pð1ÞþPð^1j2Þ/C1Pð2ÞþPð^1j3Þ/C1Pð3Þ;(9)
Pð^2Þ¼Pð^2j1Þ/C1Pð1ÞþPð^2j2Þ/C1Pð2ÞþPð^2j3Þ/C1Pð3Þ:(10)
After rewriting these equations we obtain the following
three formulas that represent the adjusted prevalence esti-
mations
Pð1Þ¼Pð^1Þ/C0ðdf1=2r/C0df1=3rÞ/C1ðPð^2Þ/C0df2=3rÞ
bt2r/C0df2=3r/C0df1=3r
ct1r/C0ðdf1=2r/C0df1=3rÞ/C1ðdf2=1r/C0df2=3rÞ
bt2r/C0df2=3r/C0df1=3r; (11)
Pð2Þ¼Pð^2Þ/C0ðdf2=1r/C0df2=3rÞ/C1ðPð^1Þ/C0df1=3rÞ
bt1r/C0df1=3r/C0df2=3r
ct2r/C0ðdf2=1r/C0df2=3rÞ/C1ðdf1=2r/C0df1=3rÞ
bt1r/C0df1=3r/C0df2=3r; (12)
Pð3Þ¼1/C0Pð1Þ/C0Pð2Þ; (13)
wherect1r¼T1h
T1hþF2=1hþF3=1h,df2=1r¼F2=1h
T1hþF2=1hþF3=1h,df3=1r¼
F3=1h
T1hþF2=1hþF3=1h,ct2r¼T2h
F1=2hþT2hþF3=2h,df1=2r¼F1=2h
F1=2hþT2hþF3=2h,
df3=2r¼F3=2h
F1=2hþT2hþF3=2h,ct3r¼T3h
F1=3hþF2=3hþT3h,df1=3r¼
F1=3h
F1=3hþF2=3hþT3h,df2=3r¼F2=3h
F1=3hþF2=3hþT3h.T h el e t t e r rin these
formulas stands for ‘rate’. Derivations for these formulas are pro-
vided in Appendix B, available in the online supplemental
material.
For the scenario of three sentiment classes, there still exists
a problem of the output values being outside the [0, 1] range.
However, in this case, simple clipping as proposed by [23]
does not work since multiple numbers may be above one or
below zero. For this reason, we propose a two-step clipping
algorithm. Initially, if the lowest quantiﬁcation estimate is
lower than zero, it is made zero and then subtracted from the
other two quantiﬁcation estimates. And at the second step,each estimated prevalence is divided by the obtained sum of
prevalences (meaning their sum is rescaled to one).
4.2.4 Probabilistic CC and ACC
CC and ACC use binary classiﬁcation models for generating
predicted values. The majority of such classiﬁers (as is also
the case with LCR-Rot-hop++) are also able to output the
values in terms of posterior probabilities PrðcjxÞ. Addition-
ally, considering that posterior probabilities represent a con-
ﬁdence level of certain classiﬁcation outcome, which
potentially represents more information, it might be beneﬁ-
cial to create probabilistic versions of CC and ACC [35].
This can be done via substituting counts TPb,TNb,FPb,FNb
by their soft counts TPs¼P
x2c;DPrðcjxÞ,TNs¼P
x2/C22c;D1/C0
PrðcjxÞ,FPs¼P
x2/C22c;DPrðcjxÞ,FNs¼P
x2c;D1/C0PrðcjxÞ.
Using all of the above, “Probabilistic Classify and Count”
(PCC) can be deﬁned as follows:
^pPCC
cðDÞ¼P
x2DPrðcjxÞ
jDj¼TPsþFPs
jDj: (14)
Initially, this method was deemed as non-working
by [23], [28], while in later research, it was shown to per-
form moderately well [47].
At the same time, “Probabilistic version of Adjusted
Classify and Count” (PACC) has the formula
^pPACC
c ðDÞ¼^pPCCcðDÞ/C0bfprs
btprs/C0bfprs; (15)
wherectprs¼TPs
TPsþFNsandcfprs¼FPs
FPsþTNs. It was ﬁrst
designed by [30] under the name of “Scaled Probability
Average,” and has the idea of substituting variables from
(8) to their soft versions (using probabilities instead of
counts) as described above.
4.2.5 Probabilistic CC and ACC for Three Sentiment
Classes
The versions of PCC and PACC for ternary sentiment classi-
ﬁcation is only different because it has different compo-
nents. This way, T1s¼P
x21;DPrð1jxÞ,T2s¼P
x22;DPrð2jxÞ,
T3s¼P
x23;DPrð3jxÞ,F1=2s¼P
x22;DPrð1jxÞ,F1=3s¼
P
x23;DPrð1jxÞ,F2=1s¼P
x21;DPrð1jxÞ,F2=3s¼P
x23;D
Prð2jxÞ,F3=1s¼P
x21;DPrð3jxÞ,F3=2s¼P
x22;DPrð3jxÞ.
At the next step, the true and false positive rates are
computed as in Section 4.2.3. Then, using the calculated
rates, adjusted quantiﬁcation values are computed using
(11), (12), and (13). Additionally, since the problem of
estimates going above one or below zero is also typical
for PACC, the same clipping procedure is applied as pro-
posed in Section 4.2.3.
4.2.6 QuaNet
The deep learning method for binary quantiﬁcation predic-
tion was ﬁrst proposed by [35]. QuaNet requires document
embeddings and a predicted prevalence score for one of the
sentiment classes in the case of binary data. This input is
extended for our proposed EntQuaNet for n-aryMATSIIAKO ET AL.: ASPECT-BASED SENTIMENT QUANTIFICATION 1723
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:22 UTC from IEEE Xplore.  Restrictions apply. quantiﬁcation, which additionally requires predicted preva-
lences for all sentiment classes, as well as an entropy score.
The architecture of EntQuaNet, based largely on the archi-
tecture of QuaNet, is provided in Fig. 1.
For QuaNet, the ﬁrst step comprises the classiﬁcation of
the given data (e.g., positive or negative) using LCR-Rot-
hop++ as discussed in Section 4.1.1. As a result, we obtain
the probabilities of each aspect opinion to belong to a certain
sentiment class, along with their embeddings (concatena-
tions of rl,rtl,rtr,rr). Afterwards, these two elements are
concatenated into one single vector to obtain jDjpairs of
type [ PrðcjxÞ;~x]. Additionally, these pairs are sorted in
increasing order of PrðcjxÞ(in the binary quantiﬁcation
case, it does not matter which class is chosen for sorting)
before being passed to a bi-LSTM layer. According to [35],
the sorting step is needed for QuaNet to leverage the
ordered sequence of PrðcjxÞand spot the transition point
between negative and positive documents.
Concurrently, using the classiﬁed data from the initial
step, we calculate prevalences ^pCC
cðDÞ,^pACC
cðDÞ,^pPCC
cðDÞ,
^pPACC
c ðDÞ, as well as rates ctprs,cfprs,ctnrs,dfnr s. These eight
values are then combined in a single vector before being
concatenated to the hidden state of the bi-LSTM. The
obtained vector is passed through the second main compo-
nent of QuaNet ( nfully connected layers, with ReLU activa-
tion functions). The idea is that QuaNet could adjust the
quantiﬁcation embedding of bi-LSTM with the help of
quantiﬁcation-related statistics obtained using LCR-Rot-
hop++. Finally, the quantiﬁcation embedding is passed
through a softmax function to obtain the prevalences
^pQuaNet
c ðDÞ.
The reason for choosing these particular quantiﬁcation-
related statistics is that they do not require much computa-
tional power. Moreover, extensive ablation experiments
were performed by [35], which conﬁrmed the assumption
that all inputs in the last step of QuaNet improve its
performance.
Considering the EntQuaNet structure in Fig. 1, it
should be noted that the QuaNet-based models will be
trained separately from LCR-Rot-hop++ in a two-step
approach. This two-step approach entails that we will
ﬁrst train the LCR-Rot-hop++ classiﬁcation model. Then,
u s i n gt h eo b t a i n e do p i n i o ne m b e d d i n g sa sw e l la st h e
predicted probabilities, these are used as training input
for the QuaNet part.4.2.7 QuaNet for Three Sentiment Classes
To adjust the QuaNet architecture for the case of three senti-
ment classes, some changes were made. In the explanations
and in Fig. 1, sentiment classes are denoted 1, 2, and 3,
instead of positive, neutral, and negative.
First, we concatenate to the document embeddings all
three probabilities (each corresponding to a certain senti-
ment class) instead of only one. Next to that, we concatenate
the value of entropy calculated from the three given proba-
bilities. We employ the following entropy formula
Entropy ¼/C0X
c2C^pðcÞlog^pðcÞ: (16)
We then sort the document embedding obtained with
LCR-Rot-hop++ from lowest to highest entropy. Essentially,
this corresponds to sorting from the easiest to hardest senti-
ment choices. In the case of low entropy, there is a clear pre-
diction; when entropy is high, the prediction can be
considered to be less certain.
In general, we argue that in our case, entropy-based sort-
ing is better at treating minority data categories because it
takes into account all probabilities at once. On the other
hand, probability-based sorting tends to disregard the
minority categories as the classiﬁcation is initially decided
on the majority category. Only when the probabilities of
belonging to the majority category are equal (which is in
our case a rare event), does probability-based sorting con-
sider the minority categories. It is known that the sentiment
of customers’ reviews is predominantly positive [48], lead-
ing to unbalanced distributions like those found in this
paper. As such, we consider entropy-based sorting to be a
ﬁtting option for the sentiment quantiﬁcation task under
consideration.
As explained in the previous section, the last hidden state
of the bi-LSTM and a more extensive set of 21 statistics
(instead of eight for the binary quantiﬁcation case) are
concatenated and passed through the fully connected layers
with ReLU activation functions as shown in Fig. 1. Nine of
the statistics come directly from the classiﬁed data (T1 -
prevalence of correctly predicted ﬁrst category observa-
tions, F1/2 - prevalence of wrongly predicted ﬁrst category
observations when the true value is the second category,
etc.). The other 12 values are prevalence predictions for
each of the three categories generated by the aforemen-
tioned quantiﬁcation methods: CC, PCC, ACC, and PACC.
Fig. 1. The architecture of the EntQuaNet quantiﬁcation system for three sentiment classes.1724 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:22 UTC from IEEE Xplore.  Restrictions apply. This way, quantiﬁcation results of the QuaNet-based mod-
els are built upon four simpler models. Considering the
advancements with the proposed entropy-based sorting
and the extension of three sentiment categories, we call this
new model EntQuaNet.
Additionally, to make sure EntQuaNet indeed improves
upon QuaNet for the ABSQ task in this paper, we train the
ordinary QuaNet on the whole training set without includ-
ing the value of entropy as a separate variable. Because we
consider three probabilities instead of two, it is unclear
which probability should be chosen for sorting. For the pur-
poses of this paper, it was decided to apply multi-sorting
based on the two most common sentiment classes. Hence, it
was ﬁrst sorted on the probability of opinions belonging to
the positive class. Then, in case the probabilities of at least
two opinions belonging to the positive class were equal, we
employed a similar sorting based on the probability of opin-
ions belonging to the negative class (second-most common).
It is pointless to include the last class in the multi-sorting
procedure: if the ﬁrst two probabilities are the same for any
two sentiments, the third probability of belonging to the
neutral class is equal as well.
To account for the ABSQ task and to see that the aspect
restriction of the training sample improves the performance
of the quantiﬁers, we select the four most prevalent aspect
categories (FOOD#QUALITY, SERVICE#GENERAL, AMBI-
ENCE#GENERAL, RESTAURANT#GENERAL), and train
separate EntQuaNets for each of them. These aspect-speciﬁc
variants will be referred to as AspEntQuaNet.
In our implementation of various QuaNets for three senti-
ment classes, LSTM cells contain nhidden dimensions. These
dimensions range from 128 to 512; the optimal dimension
varies depending on the model and is to be found with a Ran-
domized Grid Search (RGS) procedure. As a result, this archi-
tecture gives an output with dimensions 1/C22n,a st h e
utilized LSTM is bidirectional. Afterwards, this output is
concatenated with the 21 additi onal quantiﬁcation statistics
that were discussed above in detail. Essentially, this gives the
vector of size 1/C2ð2nþ21Þ. Subsequently, the resulting vector
is passed through three or fou r dense layers with a varying
number of dimensions (also d etermined via the RGS, and
ranging from 64 to 512). Each of these dense layers has a
ReLU activation function as well as a 0.3 dropout layer (found
to be the most suitable option for the majority of the models
via the RGS). At the end, there i s a layer of size three with a
softmax activation function (each of the output values corre-
sponds to the quantiﬁcation of a certain sentiment class).
To train the models, it is important to understand how the
training and test datasets are created. In our case, for each of
the aspect categories as well as the full dataset, we create sub-
datasets by sampling without replacement the opinions from
the total or category datasets. The size of these subdatasets is
determined for each of the aspect categories separately. In this
paper, we chose this size as approximately 40% of the test
dataset size. This percentage was chosen to make sure that the
subdatasets are sufﬁciently di fferent from each other, while at
the same time ensuring they are of large enough size for the
models to achieve stable test results after training. As such, the
subdatasets for FOOD#QUALITY comprise 150 opinions; for
SERVICE#GENERAL, 60 opinions; for AMBIENCE#GEN-
ERAL and RESTAURANT#GENERAL, 30 opinions. For eachof these subdatasets, we calculate the corresponding quantiﬁ-
cations of size 1/C23.
The optimal number of subdatasets within the training
datasets, for each of the aspects and models, was found
using an RGS procedure. For this reason, depending on the
speciﬁcation, the number of subdatasets inside the training
dataset may range from 1700 to 10000. For testing, we have
opted to determine the performance over a constant set of
100 randomly generated subdatasets. This allows us to
obtain more accurate and stable performance, as well as
fairer comparison between the models.
Additionally, taking into account that the RAE and KLD
may become numerically unstable in case the true preva-
lence of one of the categories is equal to zero, we make sure
there is at least one opinion from every sentiment class in
each of the subdatasets. However, this could not be done for
the test dataset of the RESTAURANT#GENERAL aspect cat-
egory, since the true prevalence of neutral sentiments is zero.
5R ESULTS
In this section, we discuss and compare results obtained from
all previously mentioned ABSQ models and methods. Sec-
tion 5.1 provides the training process and optimal hyperpara-
meter conﬁgurations of the QuaNet models. Following,
Sections 5 and 5.3, respectively , show the quantiﬁcation results
for the largest aspect category (FOOD#QUALITY) and for the
remaining smaller aspect categories (SERVICE#GENERAL,
AMBIENCE#GENERAL, and RESTAURANT#GENERAL).
5.1 Hyperparameter Optimization
The performance of machine learning models depends greatly
on the conﬁguration of its hyper parameters. Hence, to opti-
mize performance, we employ an RGS to ﬁnd the optimal con-
ﬁguration of hyperparameters. RGS has been proven to
generate models of at least the same quality as those discov-
ered by Grid Search in a fraction of the time [49].
For a fair comparison of results, we compared the models
from RGS on a validation dataset comprising 20% of the train-
ing dataset from each of the aspect categories. Considering the
characteristics for each of the m odels, displayed in Table 4, we
have identiﬁed the generally be st-performing optimizer for
this problem to be Adam and the best-performing loss func-
tion to be Mean Absolute Error (MAE), calculated over the
constant number of test subdatas ets. Additionally, for most of
the models, a range of four to seven training epochs was found
to be optimal. More training ep ochs were required to reach
optimality in the case of larger datasets.
The best results for AspEntQuaNet were achieved after
16 epochs in terms of the MAE, and 19 epochs when
assessed on KLD. After these points, training set losses con-
tinue to decrease while losses for the validation set start to
increase. Such overﬁtting behavior is common for deep
learning models and shows the importance of ﬁnding the
right tradeoff between in-sample and out-of-sample perfor-
mance. We have selected 16 training epochs for this model
(MAE was the best loss function for the QuaNet-based
models).
For example, AspEntQuaNet was trained speciﬁcally on
the FOOD#QUALITY aspect category, on 5000 subdatasets
of the training dataset. Inside the model were four stackedMATSIIAKO ET AL.: ASPECT-BASED SENTIMENT QUANTIFICATION 1725
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:22 UTC from IEEE Xplore.  Restrictions apply. dense layers of dimensions 512-256-128-64 (in said order),
and it was trained using an Adam optimizer with a batch
size of 32, and the MAE loss function. Losses were evalu-
ated and averaged over 100 random test samples.
Table 5 shows the optimal conﬁgurations of hyperpara-
meters for each of the considered models. For the majority of
t h e s em o d e l s ,t h eo p t i m a lb a t c hs i z et r a i n e do no u rd a t aw a s
determined to be 32. Also in the majority of the cases, four
dense layers were superior to thr ee. Additionally, as already
mentioned above, smaller models required signiﬁcantly less
computation time and reached optimality in a smaller number
of epochs. Lastly, the most common number of subdatasets
was either 4000 or 5000, with only two models giving the best
results for either 2200 or 10000 subdatasets.
5.2 Results for the FOOD#QUALITY Aspect
Category
This section reports results for the largest aspect category by
the number of opinions. Table 6 displays results obtained by
applying all models to FOOD#QUALITY. The test dataset
for this aspect category comprised 283 review opinions.
The ﬁrst important observation is that ACC and PACC per-
form the worst when evaluated by the KLD measure, and are
worse than ordinary CC in terms of both AE and RAE. This
can be explained by the following two arguments. First, con-
sider the training and testing data distribution: in bothdatasets, there is always a category that contains less than 6%
of observations; in fact, in 50% of the cases this number is less
than 3%. Second, as discussed previously, ACC and PACC
c a ni nt h e o r yr e t u r np r o b a b i l i t y values that are lower than
zero or higher than one (for this reason, [23] proposed a clip-
ping procedure for binary data, which we extended to be
usable for ternary data). During the process of obtaining the
results it was discovered that, when faced with unbalanced
distributions, ACC and PACC m ay overshoot below zero and
above one so much that the clipping procedure changes the
quantiﬁcation estim ates completely.
The issue with this clipping pr ocedure is illustrated in the
following example. We have randomly selected a subdataset
of the test dataset for the FOOD#QUALITY aspect category.
First, we have run CC on it and obtained the following predic-
tions for, respectively, positive, negative, and neutral classes:
[0.893333, 0.106667, 0]. For ACC, we obtain for this subdataset:
[0.978551, 0.11480645, -0.09335745]. Since one of the values is
lower than zero, the quantiﬁcations are rescaled using the clip-
ping procedure described in Section 4.2.3. This gives us
[0.837381, 0.162619, 0] as a quantiﬁcation output. A similar sit-
uation occurs with PACC, where the quantiﬁcation output is
clipped from [1.037080, 0.145764, -0.182844] to [0.787794,
0.212206, 0], changing the output considerably.
This problem is bolstered by the fact that in the majority
of the cases, there is not enough training data to estimate all
the probabilities and rates mentioned in Sections 4.2.3 andTABLE 4
Set of Hyperparameters Used in the Grid Search
Characteristic Domain Set
Optimizer Adam, RmsProp, SGD
Loss Function Mean Absolute Error, KLD
#Epochs up to 25
#Subdatasets 1700, 2200, 3000, 4000, 5000, 7000, 10000
#Dense Layers 3, 4
Dense layer
dimensions512-256-128-64, 512-256-128, 256-128-64
Batch size 8, 16, 32, 64
TABLE 5
Optimal Conﬁguration of Hyperparameters for Each of the Considered Models
Aspect Category Model #Epochs #Subdatasets #Layers Layer dimensions Batch size
FOOD#QUALITYQuaNet 11 5000 3 512-256-128 32
EntQuaNet 10 5000 4 512-256-128-64 32
AspEntQuaNet 16 5000 4 512-256-128-64 32
SERVICE#GENERALQuaNet 5 5000 4 512-256-128-64 32
EntQuaNet 5 4000 4 512-256-128-64 16
AspEntQuaNet 7 10000 3 256-128-64 32
AMBIENCE#GENERALQuaNet 5 5000 4 512-256-128-64 32
EntQuaNet 4 5000 4 512-256-128-64 32
AspEntQuaNet 4 4000 4 512-256-128-64 32
RESTAURANT#GENERALQuaNet 4 4000 4 512-256-128-64 32
EntQuaNet 3 4000 4 512-256-128-64 32
AspEntQuaNet 3 2200 4 512-256-128-64 16TABLE 6
Results of the Quantiﬁcation Method Evaluated on the Largest
Aspect Category, FOOD#QUALITY (Best Performances in Bold)
Model AE RAE KLD
CC 0.036000 0.420337 0.261060
ACC 0.069849 0.754300 0.295884
PCC 0.030851 0.400516 0.043322
PACC 0.062101 0.506329 0.311720
QuaNet 0.114636 0.731996 0.095213
EntQuaNet 0.093901 0.652151 0.072461
AspEntQuaNet 0.023564 0.235014 0.0134821726 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:22 UTC from IEEE Xplore.  Restrictions apply. 4.2.5. Hence, especially for the smaller-sized categories,
these coefﬁcients may be skewed.
As expected, QuaNet, which was trained on the whole
training dataset with a multi-sorting based on probabilities,
performed worse than EntQuaNet for which the opinions’
sorting was done based on the entropy values calculated
using all three probabilities. In fact, there is approximately a
20% difference across all the evaluation measures.
However, it comes as a surprise that both QuaNet and
EntQuaNet are signiﬁcantly worse than CC in terms of AE
and RAE. By contrast, when evaluated on KLD, they sur-
pass ordinary CC by around three times. This can be
explained by the fact that QuaNet-based models have been
shown to be the best at predicting the prevalence of the
minority data class. Hence, the value inside the logarithm of
the KLD measure (see (5) in Section 4.1.2) does not get as
high as when the prediction for the category is equal to zero
(which happens often with CC, ACC, and PACC). The fact
that KLD punishes more for errors in smaller data catego-
ries (ceteris paribus) is its particular feature.
The second-best method for this aspect category was
found to be PCC. This is largely explained by the fact that it
is much more ﬂexible than CC for the minority classes. In
fact, when CC falsely misses the minority category, the
probabilistic classiﬁer assigns a (small) probability to it. So,
even when CC has predicted zero prevalence for the minor-
ity category, its probabilistic counterpart always has a small
number assigned to that category, which is most times
closer to the true prevalence than a zero prediction.
AspEntQuaNet came out as the best method for this
aspect category. In particular, it surpassed the general Qua-
Net models by roughly three to seven times across all per-
formance measures and outperformed ordinary CC by
almost 20 times when evaluated on KLD. These results
highlight the added value of both entropy-based sorting
and training the model on separate aspect categories.
5.3 Results for SERVICE#GENERAL and
Remaining Categories
This section discusses the results obtained for the other
(smaller) aspect categories. Table 7 displays the results of all
researched models evaluated on the SERVICE#GENERAL
aspect category, comprising 107 observations in the test
data.
In this case, we observe that PACC again underper-
forms on all three metrics. This time, however, contrary
t ot h ec a s ef o rF O O D # Q U A L I T Y ,A C Cg i v e sb e t t e rq u a n -
tiﬁcation performance than both CC and PACC, althoughimprovements are marginal. In turn, PCC gives preva-
lence estimations of competitive quality according to the
evaluation measures, since it manages to accurately esti-
mate the minority category.
Again, entropy-based sorting generates better results
than multisorting on the ﬁrst two probabilities. This is sup-
ported by the fact that EntQuaNet surpasses the perfor-
mance of normal QuaNet on all three evaluation measures.
For this aspect category, all QuaNet-based models out-
perform ordinary CC. Additionally, AspEntQuaNet became
the best method according to AE and KLD. It was only mar-
ginally surpassed by EntQuaNet when evaluated on RAE.
Because they are great at predicting the prevalence of the
minority category, QuaNet-based models performed partic-
ularly well when evaluated on KLD.
Similar results were found for the remaining smaller
aspect categories AMBIENCE#GENERAL and RESTAU-
RANT#GENERAL, which comprise 59 and 58 observations
in the test data, respectively. For the former category,
AspEntQuaNet outperformed the other models on all per-
formance measures. For the latter category, an absence of
the neutral sentiment class in the test dataset (see Table 3)
resulted in numerical instability for the RAE performance
measure. In this case, we found ordinary CC to perform
best according to RAE; for the AE and KLD performance
measures, AspEntQuaNet proved most effective once more.
Additionally, EntQuaNet performed better than normal
QuaNet for both categories on almost all performance meas-
ures, again highlighting the added beneﬁt of sorting docu-
ments on entropy rather than multisorting on the ﬁrst two
probabilities.
6C ONCLUSION
In this article, we proposed EntQuaNet, an aspect-based
sentiment quantiﬁcation (ABSQ) method built upon LCR-
Rot-hop++, a state-of-the-art classiﬁcation architecture for
the task of aspect-based sentiment analysis. It extends Qua-
Net, a state-of-the-art method for sentiment quantiﬁcation,
in two ways. It is the ﬁrst ABSQ method explicitly deﬁned
for the quantiﬁcation of three sentiment classes (positive,
neutral, negative), instead of two (positive, negative). Addi-
tionally, it uses a novel entropy-based sorting procedure
instead of multisorting on probability estimates. Entropy-
based sorting was argued to be better at handling minority
data categories and resulted in better performance than
multisorting for all investigated aspect categories and per-
formance measures.
AspEntQuaNet, the aspect-speciﬁc variant of EntQua-
Net, consistently outperformed the other methods on
almost all evaluation measures. We conclude that AspEnt-
QuaNet can be successfully a pplied to ternary ABSQ, and
often outperforms all existing methods by at least a factor
of 2.
Further research could investigate other sorting methods.
Currently, input document embeddings are ordered via
probability entropy sorting. We would like to investigate
other dimensionality reduction techniques such as PCA or
the use of autoencoders to represent the three probabilities
in one value and sort the document embeddings on that
value.TABLE 7
Results of the Quantiﬁcation Method Evaluated on the SERV-
ICE#GENERAL Aspect Category (Best Performances in Bold)
Model AE RAE KLD
CC 0.075444 0.468466 0.267874
ACC 0.065944 1.447930 0.262462
PCC 0.042534 0.254345 0.030829
PACC 0.091526 1.291486 0.094069
QuaNet 0.057297 0.261275 0.031820
EntQuaNet 0.030276 0.187150 0.015957
AspEntQuaNet 0.029821 0.188882 0.013353MATSIIAKO ET AL.: ASPECT-BASED SENTIMENT QUANTIFICATION 1727
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:22 UTC from IEEE Xplore.  Restrictions apply. REFERENCES
[1] Y. M. Aye and S. S. Aung, “Sentiment analysis for reviews of res-
taurants in myanmar text,” in Proc. 18th IEEE/ACIS Int. Conf.
Softw. Eng., Artif. Intell., Netw. Parallel/Distrib. Comput. , 2017,
pp. 321–326.
[2] V. S. Pagolu, K. N. Reddy, G. Panda, and B. Majhi, “Sentiment
analysis of twitter data for predicting stock market movements,”
inProc. Int. Conf. Signal Process., Commun., Power Embedded Syst. ,
2016, pp. 1345–1350.
[3] B. Liu, Sentiment Analysis: Mining Opinions, Sentiments, and Emo-
tions, Cambridge, U.K.: Cambridge Univ. Press, 2020.
[4] E. Cambria, D. Das, S. Bandyopadhyay, and A. Feraco, A Practical
Guide to Sentiment Analysis , Berlin, Germany: Springer, 2017.
[5] A. Nazir, Y. Rao, L. Wu, and L. Sun, “Issues and challenges of aspect-
based sentiment analysis: A comprehensive survey,” IEEE Trans.
Affective Comput. , vol. 13, no. 2, pp. 845–863, Second Quarter 2020.
[6] V.-D. P /C21av/C21aloaia, E.-M. Teodor, D. Fotache, and M. Danile ¸t,
“Opinion mining on social media data: Sentiment analysis of user
preferences,” Sustainability , vol. 11, no. 16, 2019, Art. no. 4459.
[7] D. J. Hopkins and G. King, “A method of automated nonparamet-
ric content analysis for social science,” Amer. J. Political Sci. ,
vol. 54, no. 1, pp. 229–247, 2010.
[8] J. Ramteke, S. Shah, D. Godhia, and A. Shaikh, “Election result
prediction using twitter sentiment analysis,” in Proc. Int. Conf.
Inventive Comput. Technol. , 2016, pp. 1–5.
[9] S. Baccianella, A. Esuli, and F. Sebastiani, “Using micro-docu-
ments for feature selection: The case of ordinal text classiﬁcation,”
Expert Syst. Appl. , vol. 40, no. 11, pp. 4687–4696, 2013.
[10] A. Esuli and F. Sebastiani, “Sentiment quantiﬁcation,” IEEE Intell.
Syst., vol. 25, no. 4, pp. 72–75, 2010.
[11] G. Brauwers and F. Frasincar, “A survey on aspect-based senti-
ment classiﬁcation,” ACM Comput. Surv. , 2022.
[12] W. Zhang, X. Li, Y. Deng, L. Bing, and W. Lam, “A survey on
aspect-based sentiment analysis: Tasks, methods, and challenges,”
2022, arXiv:2203.01054 .
[13] A. Nazir, Y. Rao, L. Wu, and L. Sun, “Issues and challenges of aspect-
based sentiment analysis: A comprehensive survey,” IEEE Trans.
Affective Comput. , vol. 13, no. 2, pp. 845–863, Second Quarter 2022.
[14] W. Medhat, A. Hassan, and H. Korashy, “Sentiment analysis algo-
rithms and applications: A survey,” Ain Shams Eng. J. , vol. 5, no. 4,
pp. 1093–1113, 2014.
[15] K. Schouten and F. Frasincar, “Survey on aspect-level sentiment
analysis,” IEEE Trans. Knowl. Data Eng. , vol. 28, no. 3, pp. 813–830,
Mar. 2016.
[16] B. Liu, “Sentiment analysis and opinion mining,” Synth. Lectures
Hum. Lang. Technol. , vol. 5, no. 1, pp. 1–167, 2012.
[17] M. Tsytsarau and T. Palpanas, “Survey on mining subjective
data on the web,” Data Mining Knowl. Discov. ,v o l .2 4 ,n o .3 ,
pp. 478–514, 2012.
[18] M. M. Tru ¸scǎ, D. Wassenberg, F. Frasincar, and R. Dekker, “A
hybrid approach for aspect-based sentiment analysis using deep
contextual word embeddings and hierarchical attention,” in Proc.
20th Int. Conf. Web Eng. , 2020, pp. 365–380.
[19] B. Liang, H. Su, L. Gui, E. Cambria, and R. Xu, “Aspect-based
sentiment analysis via affective knowledge enhanced graph
convolutional networks,” Knowl.-Based Syst. , vol. 235, 2022,
Art. no. 107643.
[20] N. Dosoula, R. Griep, R. den Ridder, and R. Slangen, “Detection of
multiple implicit features per sentence in consumer review data,”
inProc. 12th Int. Baltic Conf. Databases Inf. Syst. , 2016, Art. no. 289.
[21] G. Ganu, N. Elhadad, and A. Marian, “Beyond the stars: Improv-
ing rating predictions using review text content,” in Proc. 12th Int.
Workshop Web Databases , 2009.
[22] B. Liu and L. Zhang, ”A survey of opinion mining and sentiment
analysis” in Mining Text Data , Berlin, Germany: Springer, 2012,
pp. 415–463.
[23] G. Forman, “Quantifying counts and costs via classiﬁcation,” Data
Mining Knowl. Discov. , vol. 17, no. 2, pp. 164–206, 2008.
[24] W. Gao and F. Sebastiani, “Tweet sentiment: From classiﬁcation to
quantiﬁcation,” in Proc. IEEE/ACM Int. Conf. Adv. Soc. Netw. Anal.
Mining , 2015, pp. 97–104.
[25] A. Moreo and F. Sebastiani, “Tweet sentiment quantiﬁcation: An
experimental re-evaluation,” 2020, arXiv:2011.08091 .[26] F. Sebastiani, “Evaluation measures for quantiﬁcation: An axiom-
atic approach,” Inf. Retrieval J. , vol. 23, no. 3, pp. 255–288, 2020.
[27] M. Saerens, P. Latinne, and C. Decaestecker, “Adjusting the out-
puts of a classiﬁer to new a priori probabilities: A simple
procedure,” Neural Comput. , vol. 14, no. 1, pp. 21–41, 2002.
[28] G. Forman, “Counting positives accurately despite inaccurate
classiﬁcation,” in Proc. 16th Eur. Conf. Mach. Learn. ,2 0 0 5 ,
pp. 564–575.
[29] G. Forman, ”Quantifying trends accurately despite classiﬁer error
and class imbalance,” in Proc. 12th ACM SIGKDD Int. Conf. Knowl.
Discov. Data Mining , 2006, pp. 157–166.
[30] A. Bella, C. Ferri, J. Hern /C19andez-Orallo, and M. J. Ramirez-Quin-
tana, “Quantiﬁcation via probability estimators,” in Proc. 10th
IEEE Int. Conf. Data Mining , 2010, pp. 737–742.
[31] L. Milli, A. Monreale, G. Rossetti, F. Giannotti, D. Pedreschi, and
F. Sebastiani, “Quantiﬁcation trees,” in Proc. 13th Int. Conf. Data
Mining , 2013, pp. 528–536.
[32] G. Da San, W. MartinoGao, and F. Sebastiani, “Ordinal text
quantiﬁcation,” in Proc. 39th Int. ACM SIGIR Conf. Res. Develop.
Inf. Retrieval , 2016, pp. 937–940.
[33] T. Joachims, “A support vector method for multivariate perfor-
mance measures,” in Proc. 22nd Int. Conf. Mach. Learn. , 2005,
pp. 377–384.
[34] A. Esuli and F. Sebastiani, “Optimizing text quantiﬁers for multi-
variate loss functions,” ACM Trans. Knowl. Discov. Data , vol. 9,
no. 4, pp. 1–27, 2015.
[35] A. Esuli, A. Moreo Fern /C19andez, and F. Sebastiani, “A recurrent
neural network for sentiment quantiﬁcation,” in Proc. 27th ACM
Int. Conf. Inf. Knowl. Manage. , 2018, pp. 1775–1778.
[36] M. Pontiki et al., “SemEval- 2016 task 5: Aspect based senti-
ment analysis,” in Proc. 10th Int. Workshop Semantic Eval. , 2016,
pp. 19–30.
[37] Y. Ma, H. Peng, and E. Cambria, “Targeted aspect-based senti-
ment analysis via embedding commonsense knowledge into an
attentive LSTM,” in Proc. 32nd AAAI Conf. Artif. Intell. , 2018,
pp. 5876–5883.
[38] M. Pontiki, D. Galanis, H. Papageorgiou, S. Manandhar, and
I. Androutsopoulos, “SemEval-2015 task 12: Aspect based senti-
ment analysis,” in Proc. 9th Int. Workshop Semantic Eval. , 2015,
pp. 486–495.
[39] Y. Wang, M. Huang, X. Zhu, and L. Zhao, “Attention-based LSTM
for aspect-level sentiment classiﬁcation,” in Proc. Conf. Empir.
Methods Natural Lang. Process. , 2016, pp. 606–615.
[40] O. Wallaart and F. Frasincar, “A hybrid approach for aspect-based
sentiment analysis using a lexicalized domain ontology and atten-
tional neural models,” in Proc. 16th Eur. Semantic Web Conf. , 2019,
pp. 363–378.
[41] J. Zeng, X. Ma, and K. Zhou, “Enhancing attention-based LSTM
with position context for aspect-level sentiment classiﬁcation,”
IEEE Access , vol. 7, pp. 20462–20471, 2019.
[42] S. Zheng and R. Xia, “Left-center-right separated neural network
for aspect-based sentiment analysis with rotatory attention,”
2018, arXiv:1802.00892 .
[43] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-
training of deep bidirectional transformers for language under-
standing,” in Proc. Annu. Conf. Assoc. Comput. Linguistics: Hum.
Lang. Technol. , 2018, pp. 4171–4186.
[44] A. Sanyal, P. Kumar, P. Kar, S. Chawla, and F. Sebastiani,
“Optimizing non-decomposable measures with deep networks,”
Mach. Lear. , vol. 107, no. 8, pp. 1597–1620, 2018.
[45] P. Gonz /C19alez, A. Casta ~no, N. V. Chawla, and J. J. D. Coz, “A review
on quantiﬁcation learning,” ACM Comput. Surv. , vol. 50, no. 5,
pp. 1–40, 2017.
[46] W. Gao and F. Sebastiani, “From classiﬁcation to quantiﬁcation in
tweet sentiment analysis,” Soc. Netw. Anal. Mining , vol. 6, no. 1,
pp. 1–22, 2016.
[47] L. Tang, H. Gao, and H. Liu, “Network quantiﬁcation despite
biased labels,” in Proc. 8th Workshop Mining Learn. Graphs , 2010,
pp. 147–154.
[48] N. Hu, J. Zhang, and P. A. Pavlou, “Overcoming the J-shaped dis-
tribution of product reviews,” Commun. ACM , vol. 52, no. 10,
pp. 144–147, 2009.
[49] J. Bergstra and Y. Bengio, “Random search for hyper-parameter
optimization,” J. Mach. Learn. Res. , vol. 13, no. 2, pp. 281–305, 2012.1728 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:22 UTC from IEEE Xplore.  Restrictions apply. Vladyslav Matsiiako received the bachelor’s
degree in econometrics and operations research
and economics and business economics from
Erasmus University Rotterdam, the Netherlands,
and the bachelor’s degree in applied mathemat-
ics from Dnipro State University, Ukraine. Cur-
rently, he is working toward the master’s degree
in operations research and information engineer-
ing with Cornell Tech, USA. His research inter-
ests include machine learning, deep learning, Big
Data, and business intelligence.
Flavius Frasincar is an assistant professor in
information systems with Erasmus University Rot-
terdam, the Netherlands. He has published in
numerous conferences and journals in the areas of
databases, web information systems, personaliza-
tion, machine learning, and the semantic web. He
is a member of the editorial boards of the Interna-
tional Journal of Web Engineering and Technology ,
Information Processing & Management, Decision
Support Systems, and Computational Linguistics
in the Netherlands Journal , and co-editor-in-chief
of the Journal of Web Engineering .
David Boekestijn received the bachelor’s degree
in econometrics and operations research from Eras-
mus University Rotterdam. He is currently working
toward the master’s degree in quantitative ﬁnance
(econometrics and management science) with the
Erasmus University Rotterdam, and the master’s
degree in artiﬁcial intelligence with the University of
Amsterdam. As a research assistant, he focuses on
machine learning approaches for text mining.
"For more information on this or any other computing topic,
please visit our Digital Library at www.computer .org/csdl.MATSIIAKO ET AL.: ASPECT-BASED SENTIMENT QUANTIFICATION 1729
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:22 UTC from IEEE Xplore.  Restrictions apply. Towards Contrastive Context-Aware
Conversational Emotion Recognition
Hanqing Zhang and Dawei Song
Abstract— Conversational Emotion Recognition (CER) aims at classifying the emotion of each utterance in a conversation. For a target
utterance, its emotion is jointly determined by multiple factors, such as conversation topics, emotion labels and intra/inter-speaker
inﬂuences, in the conversational context of it. Then an important research question arises: can the effects of these contextual factors be
sufﬁciently captured by the current CER models? To answer this question, we carry out an empirical study on four representative CER
models by a context-replacement methodology . The results suggest that these models either exhibit a label-copying effect, or rely
heavily on the intra/inter-speaker dependency structure within the conversation, but do not make a good use of the semantics carried by
the conversational context. Thus, there is a high risk that they overﬁt certain single factors, yet lacking a holistic understanding of the
semantic context. To tackle the problem, we propose a semantic-guided contrastive context-aware CER method, namely C3ER, to
augment/regularize a backbone CER model, which can be any neural CER framework. Speciﬁcally , C3ER takes the hidden states of
utterances from the CER model as input, extracts the contrast pairs consisting of relevant and irrelevant utterances to the
conversational context of a target utterance, and uses contrastive learning to establish a soft semantic constraint between the target
utterance and its context. It is then jointly trained with the main CER model, forcing the model to gain a semantic understanding of the
context. Extensive experimental results show that C3ER can signiﬁcantly boost the accuracy and improve the robustness of the
representative CER models.
Index Terms— Conversational emotion recognition, conversational context, semantic constraint, contrastive learning
Ç
1I NTRODUCTION
CONVERSATIONAL emotion recognition (CER) is aimed at
utterance-level emotion classiﬁcation for a conversa-
tion. It has attracted an increasing attention from both aca-
demia and industry in recent years. Effective CER is crucial
for building advanced dialogue systems that would become
more empathetic and engaging by taking into account user’s
emotional state [1], [2]. In addition, the practical demand of
CER is growing in many application domains, e.g., online
health care, education and legal trails [3].
Compared with traditional sentence/document-level emo-
tion recognition, the main challenge of CER lies in the fact that
CER is governed by different factors of context, such as topic,
interlocutors’ personality, intra/inter-personal dependencies,
argumentation logic, viewpoint, and intent, etc [4]. Generally
speaking, for a target utterance to be classiﬁed, the utterances
before and after it in the conversation can be regarded as its
“conversational context”. The conversational context can con-
tain different aspects that inﬂuence CER from different per-
spectives. In this paper, we divide them into three types: (1)the emotion labels of context utterances, (2) the semantics car-
ried by the actual content of utterances (e.g., topic or dialogue
intent), and (3) the relationship between speakers, i.e., intra/
inter-speaker inﬂuence. For the convenience of presentation,
we refer the ﬁrst two collectively to as semantic context, and
the third as structure context.
With the advance of deep learning techniques, neural
CER models have achieved certain performance break-
throughs. Poria et al. [5] proposed various early-stage neu-
ral CER models based on the long short-term memory
(LSTM) structures, to capture the conversational contextual
information and get utterance-level representation for emo-
tion classiﬁcation. After then, numerous neural CER meth-
ods have emerged. Most of them are dedicated to building a
more solid utterance representation to better model the
impact of conversational context. More concretely, they
treat the utterances in a conversation as a sequence, and uti-
lize the sequence models commonly used in Natural Lan-
guage Processing (NLP), such as recurrent neural networks
(RNN) [6], [7], [8], [9], Transformer [10], [11], [12], and
GCN [13], [14], [15], to aggregate the conversational context
of each target utterance to get its ﬁnal vector representation.
Some recent studies [17], [18] pointed out that the neural
networks such as the RNN and Transformer, are hard to fully
capture conversational dynamics. As for the ﬁeld of CER,
Ghosal et al. [19] also found that certain CER models exhibit a
“label copying” effect, i.e., an effect of mimicking the affective
states of the context utterances when classifying the target
utterance, rather than understanding the actual semantics of
the context. For example, the models in effect tend to “copy”
the major emotion label of the conversational context as the
predicted label of the target utterance, or directly “copy” the/C15The authors are with the School of Computer Science & Technology,
Beijing Institute of Technology, Beijing 100811, China.
E-mail: {zhanghanqing, dwsong}@bit.edu.cn.
Manuscript received 24 November 2021; revised 10 September 2022; accepted
3 October 2022. Date of publication 10 October 2022; date of current version
15 November 2022.
This work was supported in part by Natural Science Foundation of Beijing
under Grant 4222036 and in part by Huawei Technologies under Grant
TC20201228005.
(Corresponding author: Dawei Song.)
Recommended for acceptance by C.-C. Lee.
Digital Object Identiﬁer no. 10.1109/TAFFC.2022.3212994IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022 1879
1949-3045 ©2022 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See ht _tps://www.ieee.org/publications/rights/index.html for more information.
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:22 UTC from IEEE Xplore.  Restrictions apply. emotion transfer patterns (e.g., a negative emotion in a con-
versation tends to appear after the emotion “anger”) in train-
ing dataset. However, the conclusion in [19] was limited to
the a simple LSTM-based model and lacks a systematic analy-
sis of more representative and state-of-the-art CER models.
In order to ﬁll this gap, we conduct an empirical study on
four representative CER methods to further explore what
the models actually learn from the conversational context,
which are reported in more detail in Sections 4 and 6.6. We
ﬁnd that in general the current CER frameworks fall short
in understanding of the semantics of a conversation. On the
one hand, LSTM- and RNN-based CER frameworks trend
to overﬁt the label patterns of conversational context, and
show the “label copying” effect. Speciﬁcally, for each target
utterance, we replace its context utterances with different
content that yet carry the same emotions as the original
ones. We found that such replacement can hardly affect the
classiﬁcation accuracy of the model. However, the perfor-
mance degraded dramatically, when the replacement con-
tent is carried different emotion labels from the original
context utterances so that the label patterns of the conversa-
tional context are destroyed. This suggests that the models
are overly sensitive to the emotion labels instead of the
actual semantics of the conversational context. On the other
hand, the GCN-based CER models are not label-copying,
and instead they rely more on the intra/inter-speaker
dependency structure within a conversation. As a result,
the performance decreased sharply when the structural con-
text of a conversation is missing. In summary, the represen-
tative CER models studies tend to ﬁt certain single aspects
of context, i.e., the label-copying or structure-dependency
effects, yet lacking a holistic understanding of the semantic
context. This limits the accuracy and robustness of the CER
models to some extend.
To alleviate the problems mentioned above, we further
propose a semantic-guided context-enhanced mechanism to
regularize a CER model and facilitate a more effective
understanding of conversational context. The intuition is
that, in most cases, the semantic context in a conversation
tends to be consistent, i.e., an utterance can be predicted by
its preceding utterances to a certain extent [18]. As illus-
trated in Fig. 1, the utterances u1tou3are about the topic of
“name”, which is semantically consistent with u4. In this
sense, we call u4as ”context-relevant”. From a deep learn-
ing model’s perspective, given the context utterances u1:3
and with the utterance-level representations generated by a
CER framework, if a model can correctly predict u4by dis-
tinguishing the context-relevant utterance u4from the ran-
domly sampled “context-irrelevant” candidates, then the
CER framework as more context-aware. Heuristically, a
model that fully perceive conversational context would be
more helpful for utterance-level emotion analysis. Let us
take semantic context in term of topic as an example.
Under the topic of “funerals”, the utterance is more
inclined to a negative emotion; yet the emotion of the
same utterance content tend to be positive when the
topic is “weddings”.
In this paper, we incorporate the above ideas into a con-
trastive learning scheme, and propose a contrastive context-
aware CER method, namely C3ER, to augment a CER
framework. Speciﬁcally, the representation for a sequenceof utterances can be extracted from any existing CER model.
We then map the historical information for a target utter-
ance onto a compact latent semantic space, which can be
regarded as a summary of its historical context. Then we
take the target utterance itself and its subsequent ones
within a certain proximity as the context-relevant (positive)
samples, and randomly sample context-irrelevant utteran-
ces from other conversations in the dataset (termed as
“cross-dialogues”) as the negative samples, to construct
contrast pairs. Meanwhile, the above contrast pair construc-
tion process can also be performed in the opposite direction
of the utterance sequence. Finally, contrastive learning is
employed, with the objective to make each target utterance’s
representation semantically closer to positive samples but
away from the negative samples in the semantic space. This
implicitly achieves the goal that each utterance in a conver-
sation can be predicted by its conversational context. The
contrast learning objective could be regarded as a soft
semantic constraint for CER and is added to emotion classi-
ﬁcation task for jointly training, allowing a standard CER
framework to better capture the useful semantic informa-
tion in the conversational context for more effective utter-
ance-level emotion recognition.
We conduct experiments on two datasets (i.e., IEMO-
CAP [20] and MELD [16]), with four representative CER
frameworks in two scenarios including real-time and non-real-
time. Experimental results show that C3ER can signiﬁcantly
improve the accuracy of the CER frameworks. Perturbation
tests based on replacement of context for each target utterance
further reveal that C3ER can help the label copy CER frame-
work ﬂexibly deal with perturbations, and avoid overﬁtting
label patterns. As for the non-label copy CER method, it also
can help them capture more useful semantic information and
reduce dependence on the structural context of a conversation,
thus improving the robustness of the frameworks.
In a nutshell, our main contributions are summarized as
follows:
(1) We motivate and explore the problem of semantic
understanding the conversational context in CER. To the best
of our knowledge, this is the ﬁrst work to systematicallyFig. 1. A toy example of conversational context. The actual content of a
conversation ( left-hand side ) is a snippet from the MELD dataset [16].
Assume u4is the target utterance that is to be classiﬁed. According to its
historical utterances u1tou3, we can infer that the conversation is talking
about “ name ”, instead of “ camera ”, “coffee ”o r“ TV” . Therefore, u4is con-
text-relevant. The right-hand side of the ﬁgure shows some context- irrel-
evant utterances which are randomly sampled from cross-dialogues and
irrelevant to the historical context of u4.1880 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:22 UTC from IEEE Xplore.  Restrictions apply. investigate the inﬂuence of semantic context on different CER
frameworks, which stimulate the reconsideration of whether
the existing CER models make a good use of conversational
context and really understand the its semantics, so as to
develop more robust and efﬁcient CER methods in the future.
(2) We propose a pluggable approach, namely C3ER, to
enhance the contextuality in the utterance representation,
using contrastive learning. C3ER can be ﬂexibly used to reg-
ularize any existing CER framework during the model
training phrase without participating in the inference stage,
allowing the CER model to have a holistic understanding of
the semantic context.
(3) We conduct a series of experiments on two datasets,
with four representative CER frameworks. The results show
that the proposed C3ER improves the effectiveness and
robustness of the CER frameworks, by alleviating the prob-
lem of over-ﬁtting the label patterns for the “label-copying”
CER models and strengthening the understanding of
semantic context for the “non-label-copying” CER models.
The remainder of this paper is organized as follows: Sec-
tion 2 gives a brief literature review on conversational emo-
tion recognition and contrastive learning. Section 3 recaps
the preliminaries of CER. And then, we choose four repre-
sentative CER methods as baselines to conduct a empirical
study in Section 4, and describe the details of the proposed
method in Section 5. We further presents an extensive
empirical evaluation of our method and a series of in-depth
analysis and empirical studies in Section 6. Finally, we con-
clude the paper in Section 7 and discuss the promising
directions for future research in Section 8.
2R ELATED WORK
Our work is closely related to two areas: Conversational
Emotion Recognition and Contrastive Learning. We will
discuss the related work in these two ﬁelds separately
below.
Conversational Emotion Recognition . In recent years, most
state-of-the-art (SOTA) CER models are based on deep
learning. Early work uses LSTM to capture contextual infor-
mation of conversations to improve the performance [5]. In
order to further model the context, memory networks that
were previously used in question answering [21], have also
been adapted for emotional reasoning. The core idea is to
treat an utterance as a query and its context as a document,
and then perform multi-hop inferences to recognize the cor-
responding utterance’s emotion [6], [9], [22]. Later, various
context factors that are important emotion recognition have
been explored, such as factors about the speakers (e.g.,
inter-speaker inﬂuence, intra-speaker inﬂuence, personality,
etc. ) [10], [11], [13], [14], [15], external knowledge [23], [24],
and conversational topic [25], [26]. Inspired by the cognitive
theory of emotion, Hu et al. [27] design multi-turn reason-
ing modules to extract and integrate emotional clues. These
factors can be extracted through various neural networks
such as RNN, GCN, Transformer, etc., and have continu-
ously improved the CER performance. Ma et al. [28] pro-
pose multi-view network (MVN), a real-time CER method,
which explores the emotion representation from word- and
utterance-level views. A parameter-efﬁcient method, Bidi-
rectional emotional recurrent unit(BiERU) is proposed tomodel the conversational context Li et al. [29]. Compared
with sentence-level or document-level emotion recognition,
conversational emotion recognition are more sensitive to
conversational context modeling. However, existing CER
methods pay more attention to heuristically incorporating
emotional factors into the network model to improve the
performance, rather than exploring what the models actu-
ally learn from the conversational context and developing a
solution to capture that. This paper is the ﬁrst attempt to ﬁll
this gap.
Contrastive Learning . The concept of learning useful pat-
terns from contrast pairs has been extensively studied in the
literature of contrastive learning [30], [31], [32] and more
recently has been applied to a range of applications. Con-
trastive visual representation learning has been shown to
achieve the equivalent effectiveness of supervised learning
methods by constructing contrast samples and learning
visual representation in a self-supervised fashion [33], [34].
Oord et al. [35] uses a probabilistic contrastive loss to learn
sequence representations in a latent space, and the experi-
mental results demonstrate that the approach is able to learn
useful representations and achieve a strong performance on
several domains. Logeswaran and Lee [36] use the contrast
pairs to learn sentence representation, for which two contig-
uous sentences are considered as positive pairs, and the sen-
tences from other documents are regarded as negative pairs.
Cheng et al. [37] use contrastive learning to eliminate bias
(such as gender prejudice and racial prejudice) in text repre-
sentations generated by pre-trained language models. Fur-
thermore, an adversarial perturbation method [38] is
proposed for contrastive learning to solve the problem of
exposure bias in text generation, with aim to make positive
samples have a higher likelihood and negative samples
have a lower likelihood by adding perturbations. Xiong
et al. [39] propose an approximate nearest neighbor nega-
tive contrastive learning (ANCE) approach for dense
retrieval, and its main contribution is to select hard training
negatives globally from the entire corpus. As contrastive
learning does not rely on labeled data in most case, it has
been successfully applied in an increasing number of ﬁelds.
In the ﬁeld of conversation modeling, there are also some
related works using contrastive learning. Cai et al. [40] pro-
pose a group-wise contrastive dialogue learning approach
by maximizing positive responses and minimizing negative
responses, to tackle the low-diversity problem of dialogue
generation. Wu et al. [41] propose a contrastive objective
function to simulate the response selection task, and incor-
porate it into the BERT pre-trained language model for task-
oriented dialogue systems (TOD-BERT). Liu et al. [42] pro-
poses two topic-aware contrastive learning objectives,
namely coherence detection and sub-summary generation
objectives to implicitly model the topic change and handle
the information scattering problem for the dialogue summa-
rization task. How to construct the contrast sample pairs is
the key for contrastive learning. Compared with the appli-
cation in images, the construction of positive samples for
text is more challenging. In addition, simple contrast pairs
that are easy to distinguish have brought about limited
gains. Thus applying contrastive learning to natural lan-
guage processing tasks is still an open problem to be further
explored.ZHANG AND SONG: TOWARDS CONTRASTIVE CONTEXT-AWARE CONVERSATIONAL EMOTION RECOGNITION 1881
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:22 UTC from IEEE Xplore.  Restrictions apply. To the best of our knowledge, we are the ﬁrst to use con-
trastive learning in CER. Different from the existing CER
models which mainly focus on developing new neural net-
work structures for performance improvement, we create a
semantic context modelling method using contrastive learn-
ing to enhance a CER model, which can be incorporated
into and jointly learnt with any existing CER framework to
improve the classiﬁcation accuracy and robustness.
3P RELIMINARIES OF CER
3.1 Task Deﬁnition
The task of conversational emotion recognition is estab-
lished in a dialogue scenario and aims at an utterance-level
emotion classiﬁcation. Assume that a conversation lis com-
posed of a sequence of utterances fujgTl
j¼1, where Tlis the
number of utterances in the conversation. The goal is to
design and train a model to predict the emotion ejof each
utterance uj, where ejbelongs to a ﬁnite set of emotion
labelsE. In general, the whole conversation can be used as
context when classifying ej, while for a real-time scenario,
only the historical utterances u1:j/C01can be used as context
for emotion inference.
3.2 Typical Neural CER Frameworks
The C3ER could be regarded as a soft semantic constraint
for an existing CER framework. In order to better illustrate
our method, we ﬁrst introduce the general structure of typi-
cal Neural CER Frameworks. We generally divide a typical
neural CER framework into two parts: context modeling
network and classiﬁer. The former uses various sequence
modeling networks such as RNN, memory network and
GCN, to aggregate conversational context for each utterance
ut. Formally, it takes a target utterance utas input, and out-
puts its emotional hidden state ht
ht¼CERðutÞ; (1)
where ht2Rdwith dimensionality d.CER could be the part
in any CER framework before the classiﬁcation head layer.
A typical classiﬁer is composed of a fully connected layer
and softmax, which are then used to predict the target utter-
ance’s emotion label. The formulation is as follows:
^yt¼softmax WhtþbðÞ ; (2)
where W2RjEj/C2d,b2RjEj, andjEjis the number of emotion
classes. Typically, cross-entropy is used as the loss function,
and the loss for the target utterance utcan be computed as
LeðutÞ¼/C0XjEj
e¼1ye
tlog ^ye
t/C0/C1
; (3)
where ^ye
tis the logit output of the CER model, and ye
tis the
one-hot vector of the target label for the emotion class e.
4A NEMPIRICAL STUDY OF REPRESENTATIVE
CER M ODELS
4.1 Context Replacement Methods
Because of the black-box characteristics of neural networks,
it is difﬁcult to directly ﬁgure out what existing neural CERmodels have learned from conversational context informa-
tion. Instead, we propose a method based on context-
replacement to indirectly infer what a model has learned
from the conversational context by changing the input of
the model. Concretely, we aim to observe the classiﬁcation
performance of the trained CER models under the setting of
deliberately modiﬁed conversational context of each target
utterance. To do so, we have designed three different con-
text replacement methods. The ﬁrst is to replace each target
utterance’s conversational context with different content yet
keeping their original emotional labels ( EM), in order to test
the model’s sensitivity to the actual content of the conversa-
tional context. The second is to replace the context with
utterances randomly sampled from other conversations
(RM), and another is the extreme case of RM, which repla-
ces conversational context for each target utterance with the
ones which have exact different emotion labels and conver-
sational context( AM). As such both content and emotion
labels of the context are modiﬁed, in order to test the mod-
el’s sensitivity to the emotion labels associated to the con-
versational context.
Formally, for each target utterance utin a conversation,
its bi-directional context utterances in the past and future
are modiﬁed with certain constraints while utis kept
unchanged. As the illustrated in Fig. 2 , We design three
context replacement modes
(1)Emotion-Relevant Modiﬁcation (EM) : For a target utter-
ance ut, the content for each of its context utterances
cðu1;...;ut/C01;utþ1;...;unÞwill be replaced with new utter-
ances drawn from cross-dialogues in the test data, while the
emotion labels are kept the same as those associated to the
original utterances, to maintain the emotional relevance of
the replacement content. (2) Random Modiﬁcation (RM) : Sim-
ilarly, we need to replace the conversational context for
each target utterance. However, the replacement utterances
are randomly sampled from the test set, without consider-
ing whether or not their emotion labels are related to the
original contextual utterances’. (3) Altering Modiﬁcation
(AM) : As a more extreme case of RM, we replace conversa-
tional context for each target utterance with the ones which
have exact different emotion labels and conversational
context.Fig. 2. The diagram of conversational context modiﬁcation operation.
Assume u3is regarded as target utterance in a conversation, its history
and future utterances are replace with certain constraints. The EM mode
represents utis replaced with u0
tthat has the same emotional label but
different content. The RM mode represents utis replaced with u0
tthat
only has different content regardless of its emotional labels. The AM
mode represents utis replaced with u0
tthat has both different content
and emotional labels.1882 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:22 UTC from IEEE Xplore.  Restrictions apply. 4.2 Experimental Setup
We implement four representative CER models including a
real-time method, i.e., AGHMN [9] and three non-real-time
methods, i.e., LSTM [5], DialogueRnn [1], and DAG [15].
BC-LSTM [5]. This is one of the most classic early CER
models, which uses LSTM to capture the conversational
contextual information and get utterance-level representa-
tion for emotion classiﬁcation.
DialogueRnn [8]. It is one of the most typical CER
approaches, in which the attention mechanism and RNNs
are used to track the conversational global state and
speaker-dependent state. We adopt a speciﬁc variant of Dia-
logueRnn, namely Bi-DialogueRnn (publicly available at
github1), which was reported to have the best performance
in previous studies.
AGMHN [9]. It is a SOTA framework using Memory Net-
work for the real-time setting of CER. In this model, an atten-
tion gated hierarchical memory network is used to better
extract the utterance features under the condition without
the future context. We used the variants with the best perfor-
mance in the actual test, according to the original open-
source code (publicly available at github2), namely UniF-
BiAGRU for MELD and AGRU-BiCNN for IEMOCAP.
DAG [15]. It is the SOTA framework with the best reported
performance so far. A directed acyclic neural network is used
to model the structure information within a conversation,
that is, the relationship between speakers. It achieves the
SOTA performance in CER up to now. We follow DAG to use
the RoBERTa[43] to extract utterance feature, and choose the
variants reported with best performance, namely DAG-EEC
provide by the original open-source code (publicly available
at github3) during our experiments.
Then, we test the performance of these models, under the
different context replacement modes, on the widely used
IEMOCAP dataset [20] which contains dyadic conversations
among 10 speakers and is proved to sensitive to conversa-
tional context [23].
4.3 Results
The experimental results, in term of F1 scores, are shown in
Table 1. We can ﬁnd that most of the CER models are pre-
dominately sensitive to the emotion labels of conversational
context, rather than the utterance’s content itself. More spe-
ciﬁcally, the performance of BC-LSTM, DialogueRnn and
AGHMN drops a lot when label patterns of conversational
context are destroyed (under the RM andAM operation).
However, when these label patterns are maintained (underthe EM operation), the performance of those models
decreases very little. Hence, they tend to learn the “label
copying” patterns.
The only exception is DAG. Its performance does not
plummet when the label patterns are destroyed, showing
that DAG has almost no “label copy” effect. DAG is a
graph-based model to capture conversational structure, it
naturally has a structure-dependence. The particularity of
this model will be discussed in more detail in Section 6.6.
This experiment reveals quantitatively that the represen-
tative CER models tend to focus on the single aspect of emo-
tion labels or dialogue structure, but lack a holistic
understanding of the semantics of conversational context.
While these individual aspects are important for CER, there
is a risk of overﬁtting. Then a research problem arises: can
these neural CER models be semantic-regularized for
improved effectiveness and robustness?
In the next sections, we will address this problem by pro-
posing a semantic-enhanced regularization method to aug-
ment the CER models via contrastive learning and joint
training.
5C ONTRASTIVE CONTEXT -AWARE CER (C3ER)
5.1 Overall Framework
The overall framework of C3ER is shown in Fig. 3. It is plug-
gable and in principle could be injected to any existing neu-
ral CER model. Speciﬁcally, it consists of four major
components. First, C3ER extracts a sequence of hidden
states generated by an existing CER framework for emotion
classiﬁcation. Then a set of contrast pairs that capture the
context-aware semantic patterns in a conversation are con-
structed, based on which a contrastive loss is developed to
serve as a semantic-enhanced regularization. Finally, theTABLE 1
Conversational Context Replacement Experiment
Methods/C15
Operation No operation EM / decline RM / decline AM / decline
BC-LSTM[5] 57.3 54.6 / 2.7 # 40.5 / 16.8# 34.3 / 23.0#
DialogueRnn[8] 62.2 60.0 / 2.2 # 35.1 / 27.1# 30.5 / 31.7#
AGHMN[9] 59.1 54.0 / 5.1 # 31.1 / 28.0# 25.7 / 33.4#
DAG[15] 67.3 66.0 / 1.3 # 63.2 / 4.1# 62.3 / 5.0#
Fig. 3. The overall framework of C3ER. It can be injected into any CER
framework (left-hand), and contains 4 components: Emotional sequence
extraction, Contrast pair construction, Contrastive learning, and Joint
training.1.https://github.com/SenticNet/conv-emotion
2.https://github.com/wxjiao/AGHMN
3.https://github.com/shenwzh3/DAG-ERCZHANG AND SONG: TOWARDS CONTRASTIVE CONTEXT-AWARE CONVERSATIONAL EMOTION RECOGNITION 1883
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:22 UTC from IEEE Xplore.  Restrictions apply. contrast loss is added to the existing CER’s emotion classiﬁ-
cation loss for joint training.
Emotional Sequence Extraction . For better ﬂexibility, C3ER
directly takes a sequence of hidden states from a backbone
CER model, which is described as Equation 1 for every con-
versation, as its inputs. This allows C3ER to share the neural
network structure of the CER model before the classiﬁcation
layer, forcing the network model to learn a more contextual
emotional representation.
Contrast Pair Construction . This part is the core process of
C3ER. Based on the assumption that consistency of semantic
context tends to be maintained in the conversational ﬂow,
we construct positive (relevant) and negative (irrelevant)
contrast samples from both forward and reverse directions.
Those contrast pairs aim to capture semantic patterns from
the conversational context, in order for the C3ER model to
better understand and comply with the semantic contexts of
a conversation.
Contrastive Learning . In this part, we design a contrastive
loss to push a target utterance’s context semantically closer
to its relevant utterances but away from irrelevant ones.
This mechanism explicitly establishes a soft semantic con-
straint between the utterance and its context, pushing a
CER model have a better understanding of the conversa-
tional context.
Joint Training . We use multi-task learning to achieve the
ultimate goal of semantic semantic context based augmenta-
tion of the CER model. The contrastive loss is combined
with the loss of emotion classiﬁcation for joint training to
improve the performance of CER.
More technical details of these key components are pre-
sented in the following subsections.
5.2 Emotional Sequence Extraction
The proposed C3ER is a semantic context based enhance-
ment for a backbone CER model, which in principle can be
any neural CER model, e.g., bc-LSTM [5] and Dialo-
gueRnn [8]. For a full utilization of the backbone CER mod-
el’s ability and ensuring the adaptability of the C3ER
method, we directly take the utterance-level hidden
sequence from the CER model, as input to C3ER. In other
words, we injecting the C3ER module onto the classiﬁcation
head of the CER model. This could avoid the adjustment ofnetwork structure to a great extent. In addition, our method
shares parameters with the CER’s context modeling mod-
ule, so that the two tasks can be integrated and interact with
each other to achieve the purpose of mutual promotion.
Speciﬁcally, given a conversation composed of a sequence
of utterances l¼½u1;u2...:un/C138, where nis the number of
turns in a conversation, it is fed into the CER model to get the
emotional hidden state H¼½ h1;h2...:hn/C138according to the
Equation (1). Next, contrast pairs will be constructed based
on the obtained utterance-level representation.
5.3 Contrast Pair Construction
Constructing the contrast pairs is at the core of our method.
The basic idea is that, for the conversational context of a tar-
get utterance, we can identify its relevant utterances as posi-
tive examples and its irrelevant utterances as negative
example. The contrast pairs thus contain contextual seman-
tic patterns that can enhance the semantic contextuality of
the utterance’s emotional representation.
As illustrated in Fig. 4, given a sequence of utterance rep-
resentations h1:nextracted from an existing CER framework,
the historical conversational context for hkrefers to its his-
torical utterances h1:k/C01in the same dialogue, encoded by a
historical context encoder. The relevant utterances are the
intra-dialogue utterances adjacent to hkwithin certain dis-
tance at the same direction, and irrelevant ones refer to
utterances which are sampled from cross-dialogues in the
dataset. Heuristically, the same goes for constructing from
the opposite direction of a conversation, and the counter-
part of the historical context encoder is a future context
encoder.
Formally, given the sequence of emotional representation
H¼½ h1;h2...:hTl/C138, contrast pairs are generated for every
target utterance’s emotional representation hk, where k2
f1;2...Tlg. For a target utterance’s hidden state hk, we con-
struct contrast pairs from two directions. We call the direc-
tion in which the dialogue proceeds in chronological order
as forward direction, and the reverse as backward direction.
Positive Pairs . For the forward direction, we use a forward
GRU/C131/C131/C131!model to encode the historical utterances of hkas the
context representation
c!
k¼GRU/C131/C131/C131!ð½h1;h2;...hk/C01/C138Þ; (4)Fig. 4. Contrast pair construction process in C3ER.1884 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:22 UTC from IEEE Xplore.  Restrictions apply. It is natural to assume that the most relevant to the con-
text c!
kare the w-nearest neighboring utterances in the for-
ward direction. Therefore we construct the context and its
relevant utterances as positive pairs, formalized as
P!
k¼fpi;pi2ðc!
k;hkþwÞg; (5)
where P!
kdenotes the set of positive pairs in the forward
direction, and kþw/C20Tl.
Similarly to the above procedure, we use a backward GRU /C131/C131/C131
to encode the future context of the target utterance hk,a n d
obtain a backward direction context representation c 
k.T h e n ,
the corresponding backward positive pairs set P 
kis con-
structed. Now, we get all positive pairs for the target utterance
Pk¼fP!
k;P 
kg: (6)
Negative Pairs . Considering that different dialogues have
different backgrounds, topics and dialogue intents, a natu-
ral idea is to directly use the utterances from different dia-
logues (i.e., cross-dialogues) as reference negative samples.
Therefore, based on the contextual representation c!
kand
c 
kdescribed earlier, we can obtain irrelevant utterances
randomly sampled from the cross-dialogues to construct
negative pairs, formalized as
N!
k¼fni;ni2ðc!
k;^h0Þg; (7)
N 
k¼fnj;nj2ðc 
k;^hÞg; (8)
where ^h0and ^hare the utterances randomly sampled from
some other cross-dialogues. The negative pairs of the target
utterance can be represented as
Nk¼fN!
k;N 
kg: (9)
The positive pairs Pkand negative pairs Nkcontain
semantically relevance and irrelevance information about
the conversational context of a target utterance. They can be
put together to form contrast pairs, which allow the model
to perceive up and down at a conversation. For each utter-
ance’s hidden emotional representation hk, its contrast pairs
can be represented as
Dk¼fNk;Pkg: (10)
In this section, given the hidden emotional sequences
extracted from the backbone CER model, we construct a series
of context-irrelevant/relevant contrast pairs for each utterance.
In the next section, we will use them within the contrastive
learning setting to regularize the CER model and force the
model to learn the characteristi cs of these semantic patterns.
5.4 Contrastive Learning
As illustrated in Fig. 5, given a contrast pair sample, we ﬁrst
design a matching network to calculate two semantic
matching scores, one for the positive pair (i.e., between the
context representation and the relevant utterance) and the
other for the negative pair (i.e., between the context repre-
sentation and the irrelevant utterance), and then build a
contrastive loss based on the scores. Concretely, for every
target utterance’s hidden emotional representation hk,i ti saccompanied by a set of contrast pairs Dk¼fPk;Nkg. Given
a pair dj¼ð~cj;~ujÞselected from Dk, we ﬁrst concatenate the
vector representations of a contrast sample, and then a MLP
(Multi-layered Perceptron) network is used to compute the
matching scores for the pair
oj¼mlpð½~cj/C14~uj/C138Þ: (11)
In our method, we use a 2-layer MLP, which inputs the
concatenated vector and output a real value as the matching
score. The sigmoid function is used to normalize the score
to convert the matching scores into the interval [-1,1]
sj¼sigmoidðojÞ: (12)
Intuitively, the goal of our proposed approach is to give
higher matching scores for the positive samples, and lower
matching scores for those negative samples. Accordingly,
for the target utterance hk, its contrastive loss is given by
LcðDkÞ¼/C01
jPkjXjPkj
i¼1sþ
i
/C01
jNkjXjNkj
j¼1ð1/C0s/C0
jÞ; (13)
where sþ
iands/C0
jrepresent the matching scores for a posi-
tive pair from Pkand a negative pair from Nkrespectively.
jPkjdenotes the number of positive sample pairs in Dk, and
jNkjis that of the negative pairs.
5.5 Joint Training
Compared with the original CER framework, the C3ER
enhanced CER has an additional context encoding module
composed of a GRU and a matching network. The parts of
context modeling and emotion classiﬁer are shared with the
original CER framework. The entire network’s loss function
integrates the emotion classiﬁcation loss and the contrast
loss. The former is deﬁned as in Equation (3), and the con-
trastive loss is added to the original CER model for joint
training in a form that is similar to a regularizationFig. 5. The schematic diagram of the contrastive learning in C3ER. The
contrast samples are feed into a MLP network to calculate the matching
scores. And the contrast loss is applied to give higher matching scores
for the positive contrast sample, and lower matching scores for the con-
trast negative sample. In practice, the MLP for calculating matching
scores for positive and negative samples is parameter-sharing.ZHANG AND SONG: TOWARDS CONTRASTIVE CONTEXT-AWARE CONVERSATIONAL EMOTION RECOGNITION 1885
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:22 UTC from IEEE Xplore.  Restrictions apply. LðuÞ¼1PL
l¼1TlXTl
t¼1ðLeðutÞþ/C21LcðDtÞÞ; (14)
where udenotes the set of parameters of C3ER, Tlis the num-
ber of utterances in the lth conversation, and Lis the total
number of conversations in the training set. LeðuiÞis the loss
function that is described in Equation (3). LcðDtÞis the con-
trasitve loss for C3ER module, and /C21is a coefﬁcient which
determines the intensity of semantic context-enhancement.
6E XPERIMENTS
6.1 Datasets
As shown in Table 2, two commonly used benchmarking
datasets: IEMOCAP [20] and MELD [16], are chosen to eval-
uate our proposed method. IEMOCAP contains videos of
dyadic conversations among 10 speakers under diverse sce-
narios. MELD is a multi-party conversation dataset crawled
from the Friends TV series. Both datasets are originally for
multi-modal dialogue emotion classiﬁcation. Since this
work focuses on the modeling of the conversational context,
only textual modality in the original data is used.
6.2 Baselines
Consistent with Section 4, four typical CER models are chosen
as baselines, including two classic early methods (namely BC-
LSTM [5] and DialogueRnn [8]) and two SOTA methods with
leading performance so far, namely AGMHN [9] and
DAG [15].4They are described in Section 4.
We further incorporate the proposed contrastive learning
module (i.e., C3ER) into each baseline model and explore if
it can lead to a performance improvement.
For a fair comparison, the experiments with DialogueRnn
and bc-LSTM use the pre-trained utterance features provided
by the authors of DialogueRNN [8] which is available at
github1. We just use the textual feature in both of IEMOCAP
and MELD. As for AGMHN [9] and its variants, we also fol-
low the data pre-processing method provided in the original
source code of the model2. In the same way, we use the pre-
trained features provided by the open source3for DAG [15].
6.3 Experimental Settings
Precision (P), Recall (R) and weighted F-score (F1) are used
as performance measures to evaluate the single-category
emotion classiﬁcation. Then the overall performance of eachcomparative model is evaluated by Accuracy (Acc) and
weighted F-score.
With respect to the modeling direction (i.e., forward or
backward) of the contrast pair construction process, we con-
sider two variations for C3ER: namely Uni-C3ER and Bi-
C3ER. The former means to construct contrast pairs only
from the forward direction, while the latter is from both
directions simultaneously. In addition, we conduct the sig-
niﬁcant test for each CER model with/without the Bi-C3ER
module on IEMOCAP and MELD, the statistical signiﬁcance
result is reported by permutation test with p< 0:1.
The hyper-parameters of the baseline CER models are
mainly obtained by referring to the source codes and academic
papers. The speciﬁc parameters are determined by taking the
best results actually measured under different seeds. On this
basis, our proposed contrastive module (C3ER) is added, and
its external hyper-parameters are obtained through a grid
search. We use full conversations as the counting unit of
batches. The range of parameters /C21is [0.001,0.01,0.1,1.0,10],
and the size of negative samples is varied in the range between
1 and batch size minus 1. During our experiments, we only
choose 8 and 16 as batch size. We record the hyper-parameters
of the Bi-C3ER branch for reproducing the experiment result
in Tables 3 and 4. All the parameters are reported in order bs-
LSTM, DialogueRnn, AGMHN, and DAG. For IEMOCAP
dataset, /C21¼½10:0;1:0;0:01;0:1/C138, batch size is [8,16,16,8] and
size of negative samples is [5,9,8,1]; For the MELD dataset, /C21¼
½1:0;1:0;1:0;10/C138, the batch size of all baselines is set to 8, and
size of negative samples is [5,4,3,5].
6.4 Experimental Results
Tables 3 and 4 show the experimental results on IEMOCAP
and MELD dataset respectively. We can draw the following
observations:
(1) C3ER can improve the classiﬁcation accuracy over all the
baseline methods. On IEMOCAP, it improves the performance
of the original AGMHN and DialogueRnn by 3.4%-4.8%. This
result proves that the semanti c context of an utterance in the
conversation is important for CER. The improvement on the
IEMOCAP dataset is more signiﬁcant than MELD, which is
consistent with the conclusion from previous works [23] due
to the nature of data. Compared with IEMOCAP, the MELD
dataset has a shorter dialogue length, and the context has less
inﬂuence on emotion classiﬁcation.
(2) Given the improvement over DAG, we have achieved
a new SOTA performance on both IEMOCAP and MELD.
DAG was the SOTA model according to the reported results
so far. After injecting C3ER into DAG, the classiﬁcation
accuracy performance is further improved by about 1.6% on
IEMOCAP and 1.3% on MELD, achieving a new SOTA per-
formance. It proves the superior effectiveness of C3ER.
(3) Among the variants of C3ER, the effectiveness of Bi-
C3ER is better than Uni-C3ER. According to the experimen-
tal results, the performance of using Uni-C3ER is between
the use of Bi-C3ER and the original methods in the most
cases, which indicates that the bidirectional contrast pairs
can provide more useful semantic information for model
training and thus lead to more gain in performance. In par-
ticular, this phenomenon would be beneﬁcial for the real-
time CER scenario, because our method only operates in theTABLE 2
The Statistics of the Used Datasets
Dataset #dialogues #utterances Avg. #turns
IEMOCAPtrain/val 120 5,810 48.4
test 31 1,623 52.4
MELDtrain 1,039 9,989 9.6
dev 114 1,109 9.7
test 280 2,610 9.3
“#” Represents the number of corresponding items.
4.We also implemented the DialogueGCN [13], but we ﬁnd it easy
to collapse during the model training phrase, so we do not include it as
our baselines.1886 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:22 UTC from IEEE Xplore.  Restrictions apply. training phase and it can effectively utilize bidirectional dia-
logue information during the model training, but still car-
ries on the single direction in the reasoning stage.
(4) The performance of C3ER on bc-LSTM is weaker than
that of the other three more complex CER frameworks. Spe-
ciﬁcally, the improvements over bc-LSTM on both datasets
are less than 1.6%, which is less than the best improvement
performance (close to 5%) on DialogueRnn and AGMHN.
This phenomenon shows that the proposed C3ER module
also relies on the complexity of the backbone model to acertain extent, because the conversational context modeling
itself is a important and challenging task.
6.5 Model Analysis
Coefﬁcient /C21for Contrast Loss . we explore how the regulariza-
tion coefﬁcient /C21affects the emotion classiﬁcation perfor-
mance. The range of /C21is½1e/C02;1e/C01;0;1e1;1e2/C138. As shown in
Fig. 6, the performance trends of DialogueRnn and AGHMN
on both datasets are consistent. With the increase of /C21, F1-TABLE 3
Performances on IEMOCAP Dataset
Model Happy Sad Neural Angry Excitd Frustrated Avg
P R F1 P R F1 P R F1 P R F1 P R F1 P R F1 Acc F1
bc-lstm 41.0 34.7 37.6 69.4 66.5 67.9 51.6 53.9 52.7 60.0 67.1 63.3 66.4 56.2 60.9 56.2 62.2 59.0 57.8 57.9
+Uni-C3ER 37.3 26.4 30.9 67.3 69.8 68.5 53.8 53.1 53.5 60.1 64.7 62.3 63.4 67.2 65.3 57.2 58.3 57.7 58.2 57.8
+Bi-C3ER 37.7 41.7 39.6 68.8 71.0 69.9 53.5 53.6 53.6 66.9 59.4 62.9 66.2 52.5 58.6 58.2 67.0 62.3 58.7{58.7{
1.6%"1.4%"
DialogueRnn 52.4 30.6 38.6 86.9 64.9 74.3 54.9 57.8 56.3 69.0 64.1 66.5 69.6 74.2 71.8 56.4 70.3 62.6 63.1 62.9
+Uni-C3ER 53.6 25.7 34.7 53.6 25.7 83.7 59.1 60.9 60.0 68.2 60.6 64.2 64.9 89.6 64.9 61.8 53.5 57.4 65.2 64.0
+Bi-C3ER 55.9 26.4 35.8 89.7 78.4 83.7 63.7 58.1 60.8 64.1 68.2 66.1 65.1 90.3 75.6 59.2 61.4 60.3 66.1{65.2{
4.8%"3.7%"
AGMHN 50.3 53.9 52.0 81.6 66.9 73.5 49.6 58.6 53.7 69.5 61.8 65.4 75.6 55.9 64.2 56.1 65.1 60.1 60.8 61.3
+Uni-C3ER 50.9 38.46 43.8 81.5 64.9 72.3 50.1 58.1 53.8 82.1 56.5 66.9 76.2 62.5 68.6 54.8 73.5 62.8 61.9 61.7
+Bi-C3ER 55.0 38.5 45.3 80 63.7 70.9 54.0 64.1 58.6 74.6 60.6 66.9 72.8 69.9 71.3 58.1 68.0 62.6 63.4{63.4{
4.3%"3.4%"
DAG 43.0 36.4 39.4 83.7 77.6 50.5 63.6 76.6 69.5 71.7 64.1 67.7 70.8 64.9 67.7 68.9 69.8 69.4 68.1 67.9
+Uni-C3ER 48.8 42.0 45.1 79.1 80.4 79.8 65.7 74.2 69.7 69.6 64.7 67.1 73.1 68.2 70.6 69.1 68.8 68.9 68.9 68.8
+Bi-C3ER 50.0 41.3 45.2 78.0 81.2 79.6 66.4 74.5 70.2 68.2 68.2 68.2 70.9 69.2 70.1 71.6 67.0 69.2 69.2{68.9{
1.6%"1.5%"
It shows the overall performance of the four baselines and two variants Uni-C3ER and Bi-C3ER, which we added on the basis. {represents the Bi-C3ER achieves
signiﬁcant improvements over the baselines among the average Acc and F1 metrics.
TABLE 4
Performances on MELD Dataset
Model Sadness Neural Angry Surprise Fear Disgust Joy Avg
P R F1 P R F1 P R F1 P R F1 P R F1 P R F1 P R F1 Acc F1
bc-lstm 30.2 7.7 12.3 72.0 81.6 76.5 37.5 42.3 39.8 46.2 51.9 48.9 ------ 50.7 54.0 52.3 59.3 56.3
+Uni-C3ER 42.3 5.3 9.4 70.8 83.6 76.7 39.2 39.1 39.1 44.2 49.8 46.8 - - - - - - 50.6 55.0 52.7 59.6 56.0
+Bi-C3ER 36.5 11.1 17.0 71.9 82.2 76.7 40.0 37.1 38.5 44.6 53.3 48.6 - - - - - - 50.1 56.7 53.2 59.8 56.8
0.8%"0.9%"
DialogueRnn 25.9 13.5 17.7 71.2 81.4 76.0 46.1 35.7 40.2 44.4 49.1 46.6 - - - - - - 48.1 58.7 52.9 59.3 56.4
+Uni-C3ER 26.5 13.9 18.2 70.9 81.8 76.0 45.4 44.1 44.7 45.9 50.2 48.0 - - - - - - 51.7 52.5 52.1 59.8 57.1
+Bi-C3ER 26.4 15.4 19.5 71.3 83.3 76.7 43.1 41.0 41.8 45.0 50.9 47.7 - - - - - - 54.0 50.5 52.2 60.0{57.2{
1.2%"1.4%"
AGMHN 36.5 18.8 24.8 73.7 77.6 75.6 42.6 40.6 41.5 53.5 49.5 51.4 13.6 12 12.8 12.9 11.8 12.3 50.3 60.7 55.0 59.4 58.4
+Uni-C3ER 32.2 22.1 26.2 71.5 81.8 76.3 50.3 28.1 36.1 46.0 61.6 52.7 16.0 8.0 10.7 7.1 1.5 2.5 52.6 55.2 53.9 60.2 57.8
+Bi-C3ER 30.5 28.9 29.6 72.6 79.9 76.1 42.9 43.8 43.3 55.7 45.6 50.1 19.5 16.0 17.6 29.4 7.4 11.8 55.9 54.5 55.2 60.3{59.2{
1.5%"1.4%"
DAG 43.4 30.3 35.7 77.0 77.4 77.2 57.1 42.0 48.4 49.7 67.3 57.1 21.2 28.0 24.1 43.5 25.0 31.8 57.5 66.4 61.7 63.9 63.3
+Uni-C3ER 49.3 32.2 39.0 76.9 79.2 78.0 56.7 44.1 49.6 49.9 68.0 57.5 20.0 12.0 15.0 52.4 16.2 24.7 55.4 65.9 60.2 64.6 63.6
+Bi-C3ER 46.9 32.7 38.5 76.9 78.8 77.8 57.4 43.8 49.7 50.3 68.1 57.8 25.0 16.0 19.5 61.9 19.1 29.2 55.5 66.4 60.5 64.7{63.8{
1.3%"0.8%"
The content is the same as Table 3. “-” indicates that the classiﬁcation performance of the model is close to 0 in corresponding items, since MELD datase t have
only a very small samples of “fea” and “disgust.”ZHANG AND SONG: TOWARDS CONTRASTIVE CONTEXT-AWARE CONVERSATIONAL EMOTION RECOGNITION 1887
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:22 UTC from IEEE Xplore.  Restrictions apply. score shows a trend of increasing ﬁrst and then decreasing.
This is in line with our expectation: when /C21is too small, the
model only focuses on the emotion classiﬁcation task, so that
our C3ER module has little impact on the emotion classiﬁca-
tion task; when /C21is too large, the main task of the model will
be ignored; therefore a moderate value tends to achieve a bet-
ter effectiveness of emotion classiﬁcation. When the value of
/C21is 1.0, DialogueRnn achieves the best enhancement effect
on IEMOCAP and MELD. As for AGHMN, the best values of
/C21is 1.0 and 0.01, respectively.
However, as shown in Table 5, it is interesting to see that
the performance of DAG is quite different from Dialo-
gueRnn and AGHMN. For DAG, the contrast loss coefﬁ-
cient /C21has relatively less effect on performance of emotion
recognition than DialogueRnn and AGHMN. Even when
the/C21¼100, the decrease of F-scores is less than 4.0% for
MELD and less than 7.0% for IEMOCAP. This may indicate
that DAG itself have modeled well the context information
which is relevant to emotion classiﬁcation, and then con-
text-enhanced module have a strong consistency with the
optimization objective of DAG.
Size of Negative Samples . We also investigate the inﬂuence
of the number of negative samples on the emotion classiﬁca-
tion results for the IEMOCAP dataset. In order to better con-
trol the variables, we select the nearest 3 utterances adjacent
to the target as the positive samples, and set the batch size
values for all CER methods to 8. Negative samples are
obtained from other cross-dialogues in the same batch that
are not in the same conversation as the target utterance. The
number of conversations used to construct negative samples
ranges from 1 to 7. The test results are shown in Table 6.
Both methods achieve the best performance when the size
of negative sample is 4. This indicates that when the number
of positive samples is ﬁxed, the selection of matching nega-
tive cases is conducive to better results, and it is not appro-
priate to have too many or too few negative samples. A
similar trend also holds for the MELD dataset.6.6 Perturbation Test
In order to ﬁgure out where the performance gain of the
C3ER comes from and quantify the inﬂuence of C3ER on
the backbone CER framework’s performance, we carry out
a series of perturbation tests on IEMOCAP dataset, which is
known more sensitive to contextual information.
6.6.1 Test Setting
Since it is hard to quantify the context semantic consistency
between target utterance and its conversational context, we
follow the context-replacement approach used in our empir-
ical study Section 4, to indirectly validate the performance
of C3ER through perturbation tests by replacing the conver-
sational context of the target utterance under different con-
text-replacement modes.
More concretely, the main idea of the perturbation test is to
modify the context of the target utterance in the test data, and
then ﬁnd out the performance change of the trained model.
When the trained CER models classify the target utterance in a
conversation, its conversational context is replaced with cross-
dialogue content, which can indirectly reﬂect the how the CER
model deal with its context. We also designed three different
replacement modes, which are detailed in Section 4. The EM
Mode maintains the original label patterns, and the RM and
AM is designed to destroy the patterns inversely, AM is a
more extreme case of RM. The performance degradation of dif-
ferent models under those settings can indirectly indicate how
they react to the inﬂuence of “label copy” patterns. Whether
the C3ER is able to signiﬁcantly improve the performance of
CER models under the different perturbation settings, will
serve as a strong evidence to prove the function of C3ER.
During the experiments, the checkpoints for each model
are chosen by training with same hyper-parameters, instead
of the grid search. Furthermore, we chose 5 different ran-
dom seeds to test results and report the average F1-score.
We also conduct the signiﬁcant test under the hypothesis
that the CER models without C3ER have a better perfor-
mance than the models with C3ER, and the statistical signif-
icance result is reported by permutation test with p< 0:1.
6.6.2 Results and Analysis
The perturbation test results are shown in Table 7. According to
the observations we have made in the empirical analysis of the
baseline models (see Section 4), we divide them into two types,
called Label Copying Approaches (i.e., BC-LSTM, DialogueRnn
and AGHMN) and Non-label Copying Approaches (i.e., DAG)
respectively. We will discu ss them separately below.Fig. 6. Inﬂuence of contrast loss coefﬁcient /C21on performance of different
models (DialogueRnn & AGHMN) and datasets (MELD & IEMOCAP).
TABLE 5
Inﬂuence of Contrast Loss Coefﬁcient /C21on Performance of DAG
[15] Between Different Datasets (MELD & IEMOCAP)
Dataset/C21
1e/C031e/C021e/C011e01e11e2
IEMOCAP 68.1 68.2 68.9 67.2 65.3 61.9
MELD 63.7 63.6 63.7 63.5 63.8 60.0
The F-scores of the methods are reported.TABLE 6
The Inﬂuence of the Number of Negative Samples on the Emo-
tion Classiﬁcation Performance
Method#dialogue
123456 7
DialogueRnn+C3ER 63.5 63.7 64.0 65.0 64.0 64.9 64.5
AGHMN+C3ER 60.6 60.8 60.5 62.4 60.9 61.1 60.8
DAG+C3ER 68.5 68.2 68.2 68.9 68.6 68.3 68.5
F1-scores is used as evaluation index. “#dialogue” represents the number of
conversation selected as negative samples.1888 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:22 UTC from IEEE Xplore.  Restrictions apply. Label Copying Approaches. BC-LSTM, DialogueRnn and
AGHMN show similar trends under the different settings
regardless of whether C3ER is injected. When the EM perturba-
tion is applied, the performance of the baseline methods with
the C3ER module added almost do not decrease, and the
decrease is slight when C3ER is not added. In case of the RM
and AM perturbation, the F1-sc ores of BC-LSTM, DialogueRnn
and AGHMN are reduced sharply, and the performance dete-
riorates more in AM than RM mode. After adding the C3ER
module, the performance degra dation is eased signiﬁcantly.
And signiﬁcant test further conﬁrms the above trend.
Therefore, C3ER is like a regularization module, helping
label-copying CER frameworks handle conversational con-
text information more ﬂexibly and at the same time enhance
their robustness. For the baseline models that tend to learn
the “label copying” patterns, the performance after the emo-
tion-relevant perturbation will be less degraded, while the
random perturbation will destroy the label pattern, result-
ing in a sharp performance decline. On the contrary, for a
context-aware model where C3ER is injected, in the case of
emotion-related context perturbation, i.e., EM mode, the
model can make reasonable use of label patterns to maintain
a high classiﬁcation accuracy. When the utterance’s context
is randomly or entirely replaced with both content- and
emotionally irrelevant information (i.e., RM and AM mode),
the C3ER method could help CER model reduce depen-
dency on the label pattern, so as to maintain classiﬁcation
performance to some extent.
Non-Label Copying Approaches. For the baseline DAG [15],
we observe an interesting phenomenon that the perturbation
no matter under EM, RM, AM operation, has less signiﬁcant
inﬂuence on its emotion classiﬁcation performance com-
pared to other three baselines. This indicates that DAG has
little “label copy” effect compared to the other three models.
More experimental results of DAG with respect to extra
modes of perturbation on structure and content are reported
in the Table 8. According to the principles and framework
structure of DAG, we know that DAG mainly focuses on
the construction of structural information of a conversation,
that is, the relationship between speakers established by a
directed acyclic graph. Therefore, in order to ﬁgure out
where the gains come from, we do a further ablation experi-
ment and design 3 different modes to break the conversa-
tional context for DAG. The “content” mode is regarded as
the destruction of actual content of target utterance’sconversational context, which is same with “RM” mode
described in Section 4.1. For the “structure” mode, we take
randomly replace the edges constructed by the DAG and
speaker information as the destruction of the structural con-
text of the dialogue. The third mode (i.e., structure+content)
is a mixture of the above two.
The experimental results are shown in Table 8. In case
where both of the structure and actual content information
of conversational context are broken, the F-scores of the
model with or without C3ER is injected shows no signiﬁcant
difference. When only the structural information of a dia-
logue is destroyed, the model with C3ER injected have a sig-
niﬁcant improvement. We suspect that C3ER can help the
CER model capture the structure of a conversation inferred
from the semantic context, so as to reduce dependence on
the conversational structure. Hence, DAG with C3ER can
maintain high classiﬁcation accuracy even when the struc-
tural context of a dialogue is destroyed. This phenomenon
also supports the conjecture in Section 6.5.
In a summary, we have conducted a quantitative analysis
of how different CER models use the conversational contex-
tual information. The experimental results prove that our
C3ER method has a certain degree of versatility, which can
help understand various types of conversational context
and improve the backbone CER model’s performance. For
the label-copying CER methods, C3ER mainly aims to avoid
over-ﬁtting the label patterns. For non-label copying meth-
ods(i.e., DAG), it trends to act as a semantic enhancement
module in helping the CER model to reduce dependence on
the conversational structure.
6.7 Case Study
In order for a more intuitive understanding of the effect of
our method, we select an example conversation from IEMO-
CAP as a case, to show how C3ER inﬂuences its attention in
comparison with DialogueRnn, which correspond to a large
performance improvement. Fig. 7 shows the emotion classiﬁ-
cation results and attention visualization of the selected case.
In this conversation, 13 out of 23 utterances are labeled with
emotion “frustrated”. DialogueRnn exhibits a label-copy
effect, which directly classiﬁes almost all utterances into
“frustrated” to obtain the highest classiﬁcation accuracy.
When C3ER is added, the performance of the model has been
improved. Not only the prediction is correct for most of theTABLE 7
Comparison of Different Models’ F1-Scores Performance and
Corresponding Signiﬁcance Test Under the Perturbation Test
MethodsOperation
No operation EM RM AM
BC-LSTM 57.3 54.6 40.5 34.3
BC-LSTM+C3ER 57.6{56.8{47.2{40.7{
DialogueRnn 62.2 60.0 35.1 30.5
DialogueRnn+C3ER 63.9{62.2{52.2{41.1{
AGHMN 59.1 54.0 31.1 25.7
AGHMN+C3ER 60.3{56.2{36.5{31.3{
DAG 67.3 66.0, 63.2 62.3
DAG+C3ER 68.5{67.4 63.0 62.4
{represents the C3ER achieves signiﬁcant improvements over the baselines.TABLE 8
Comparison of F1-Scores Performance for DAG [15] With/With-
out C3ER Module Under Different Modes of Conversational
Context Modiﬁcation
OperationModel
DAG DAG+C3ER
No Operation 67.3 68.5{
Structure 63.1 65.5{
Content 63.2 63.0
Structure+Content 62.9 63.5
“Structure” represents the structural information of a conversation is broken;
and “Content” represents the actual content of a target utterance is replaced;
“Structure+Content” means two operations are performed simultaneously. {
represents the C3ER achieves signiﬁcant improvements over the baselines.ZHANG AND SONG: TOWARDS CONTRASTIVE CONTEXT-AWARE CONVERSATIONAL EMOTION RECOGNITION 1889
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:22 UTC from IEEE Xplore.  Restrictions apply. “frustrated” utterances, but a considerable part of “angry”
and “neutral” utterances are also classiﬁed correctly.
Speciﬁcally, we analyze the attention distribution of the
two models (i.e., DialogueRnn with and without C3ER) in
recognizing the last utterance of the conversation. Dialo-
gueRnn without C3ER incorrectly classiﬁes the emotion of
“angry” as “frustrated”. Its attention is more local, focusing
more on the continuous utterance with main emotion label
“frustrated”. After adding the C3ER module, the model
seems to be able to capture more longer-term dependencies,
focusing most of the attention on the 8th and 21st utterances
marked with the text as shown in Fig. 7, thereby achieving
correct classiﬁcation. It is interesting that the utterances
which the C3ER-enhanced model mainly focuses on have a
similar language expression in addition to the fact that their
emotions are all “angry”. This indicates that the C3ER-
enhanced model conducts a context-aware emotional rea-
soning by synthesizing context and the utterances whose
expressions are similar to its own.
7C ONCLUSION
In this paper, we ﬁsrt conduct an extensive empirical inves-
tigation of the effect of conversational context on the perfor-
mance of conversational emotion recognition models. The
results reveal that the representative CER models we have
studied tend to overﬁt certain individual aspects of context,
e.g., the emotion labels and intra/inter-speaker structures,
but lack a holistic understanding of the conversational con-
text, specially semantic context.
To solve this problem, we proposed a semantic-guided
regularization method, namely C3ER, which can be inte-
grated into a baseline CER model via contrastive learning
and joint training. C3ER is based on contrastive learning in
a self-supervised manner. It extracts the context-relevant/
irrelevant utterances from intra/cross-dialogues for each
utterance context as contrast pairs. Then contrastive learn-
ing is employed to establish the explicit semantic connection
between utterances and their conversational context. This
mechanism can be injected into a CER framework and
jointly trained with emotion classiﬁcation through multi-task learning, forcing the CER model to perform an emo-
tional reasoning from the perspective of context under-
standing. Extensive experiments demonstrate that the idea
of explicitly adding context understanding constraints to
CER models is helpful to improve the classiﬁcation accuracy
and robustness.
8F UTURE WORK
(1) Building a comprehensive CER framework that can
i n t e g r a t em u l t i p l ei n ﬂ u e n c ef a c t o r s .C o n v e r s a t i o n a le m o -
tion recognition is a task largely affected by multiple fac-
tors such as semantic contex t (e.g., content emotion,
dialogue topic, intent, etc.) and conversational structure
(e.g., inter-speaker inﬂuenc e, intra-speaker inﬂuence,
etc.). In the future, we will es tablish more quantitative
analysis mechanisms for each inﬂuencing factor, rather
than just heuristic exploration. Based on the ﬁndings, we
can construct an holistic emotion decision-making frame-
work that considers a range of factors to improve the per-
formance of CER.
(2) Constructing more hard contrast samples to promote
contextual learning. How to construct a more effective set of
contrast pairs is the key to contrastive learning. In the
future, it is promising to construct more complex and indis-
tinguishable contrast sample pairs based on the characteris-
tics of conversational emotion recognition, to further
improve the CER model’s understanding of conversational
context.
(3) Incorporating external commonsense knowledge to
conversational emotion recognition. Commonsense knowl-
edge is also an important factor affecting the emotion of the
conversation beyond the conversational context explored in
our paper. Studying the relationship and a seamless integra-
tion between exploiting the commonsense knowledge and
understanding the conversational context is an interesting
topic that deserves a more in-depth further investigation.
REFERENCES
[1] H. Zhou, M. Huang, T. Zhang, X. Zhu, and B. Liu, “Emotional
chatting machine: Emotional conversation generation with inter-
nal and external memory,” in Proc. 32nd AAAI Conf. Artif. Intell.,
30th Innov. Appl. Artif. Intell., 8th AAAI Symp. Educ. Adv. Artif.
Intell. , 2018, pp. 730–739.
[2] J. Wang, X. Sun, M. Wang, and M. Wang, “Emotional conversa-
tion generation with bilingual interactive decoding,” IEEE Trans.
Comput. Soc. Syst. , vol. 9, no. 3, pp. 818–829, Jun. 2022.
[3] S. Xing, S. Mai, and H. Hu, “Adapted dynamic memory network
for emotion recognition in conversation,” IEEE Trans. Affective
Comput. , vol. 13, no. 3, pp. 1426–1439, 3rd Quart. 2022.
[4] S. Poria, N. Majumder, R. Mihalcea, and E. H. Hovy, “Emotion
recognition in conversation: Research challenges, datasets, and
recent advances,” IEEE Access , vol. 7, pp. 100 943–100 953, 2019.
[5] S. Poria, E. Cambria, D. Hazarika, N. Majumder, A. Zadeh, and
L. Morency, “Context-dependent sentiment analysis in user-gen-
erated videos,” in Proc. 55th Annu. Meeting Assoc. Comput. Linguis-
tics, 2017, pp. 873–883.
[6] ICON: Interactive Conversational Memory Network for Multi-
modal Emotion Detection, Brussels, Belgium.
[7] HiGRU: Hierarchical Gated Recurrent Units for Utterance-Level
Emotion Recognition, Minneapolis, Minnesota.
[8] N. Majumder, S. Poria, D. Hazarika, R. Mihalcea, A. F. Gelbukh,
and E. Cambria, “DialogueRNN: An attentive RNN for emotion
detection in conversations,” in Proc. 33rd AAAI Conf. Artif. Intell.,
31st Innov. Appl. Artif. Intell. Conf., 9th AAAI Symp. Educ. Adv. Artif.
Intell. , 2019, pp. 6818–6825.
Fig. 7. Visualization of the DialogueRnn’s attentions under the different
settings whether C3ER is injected. The lines “#Pred” represents the
emotion classiﬁcation results of DialogueRnn with C3ER, “Pred” is that
of without C3ER, and between them is the ground-truth labels. The
attention values are from the models when predicting the emotion of
23th utterance marked with a dashed box in the dialogue.1890 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:22 UTC from IEEE Xplore.  Restrictions apply. [9] W. Jiao, M. R. Lyu, and I. King, “Real-time emotion recognition
via attention gated hierarchical memory network,” in Proc. 34th
AAAI Conf. Artif. Intell., 32nd Innov. Appl. Artif. Intell. Conf., 10th
AAAI Symp. Educ. Adv. Artif. Intell. , 2020, pp. 8002–8009.
[10] J. Li, Z. Lin, P. Fu, Q. Si, and W. Wang, “A hierarchical trans-
former with speaker modeling for emotion recognition in con-
versation,” CoRR , 2020.
[11] DialogXL: All-in-one XLNet for multi-party conversation emotion
recognition, vol. 35.
[12] Z. Lian, B. Liu, and J. Tao, “CTNet: Conversational transformer
network for emotion recognition,” IEEE ACM Trans. Audio Speech
Lang. Process. , vol. 29, pp. 985–1000, Jan. 2021.
[13] DialogueGCN: A Graph Convolutional Neural Network for Emo-
tion Recognition in Conversation,” Hong Kong, China.
[14] D. Zhang, L. Wu, C. Sun, S. Li, Q. Zhu, and G. Zhou, “Modeling
both context- and speaker-sensitive dependence for emotion
detection in multi-speaker conversations,” in Proc. 28th Int. Joint
Conf. Artif. Intell. , 2019, pp. 5415–5421.
[15] W. Shen, S. Wu, Y. Yang, and X. Quan, “Directed acyclic graph
network for conversational emotion recognition,” in Proc. 59th
Annu. Meeting Assoc. Comput. Linguistics 11th Int. Joint Conf. Natu-
ral Lang. Process. , 2021, pp. 1551–1560.
[16] MELD: A Multimodal Multi-Party Dataset for Emotion Recogni-
tion in Conversations, Florence, Italy.
[17] C. Sankar, S. Subramanian, C. Pal, S. Chandar, and Y. Bengio, “Do
neural dialog systems use the conversation history effectively? An
empirical study,” in Proc. 57th Conf. Assoc. Comput. Linguistics ,
2019, pp. 32–37.
[18] C. Hao, L. Pang, Y. Lan, F. Sun, J. Guo, and X. Cheng, “Ranking
enhanced dialogue generation,” in Proc. 29th ACM Int. Conf. Inf.
Knowl. Manage. , 2020, pp. 465–474.
[19] D. Ghosal, N. Majumder, R. Mihalcea, and S. Poria, “Utterance-
level dialogue understanding: An empirical study,” CoRR , 2020.
[20] IEMOCAP: Interactive emotional dyadic motion capture data-
base, vol. 42.
[21] A. Bordes, N. Usunier, S. Chopra, and J. Weston, “Large-scale
simple question answering with memory networks,” CoRR , 2015.
[22] S. Xing, S. Mai, and H. Hu, “Adapted dynamic memory network
for emotion recognition in conversation,” IEEE Trans. Affective
Comput. , vol. 13, no. 3, pp. 1426–1439, 3rd Quart. 2022.
[23] D. Ghosal, N. Majumder, A. F. Gelbukh, R. Mihalcea, and S. Poria,
“COSMIC: Commonsense knowledge for emotion identiﬁcation
in conversations,” in Proc. Conf. Empir. Methods Natural Lang. Pro-
cess., 2020, pp. 2470–2481.
[24] P. Zhong, D. Wang, and C. Miao, “Knowledge-enriched trans-
former for emotion detection in textual conversations,” in Proc.
Conf. Empir. Methods Natural Lang. Process. 9th Int. Joint Conf. Natu-
ral Lang. Process. , 2019, pp. 165–176.
[25] J. Wang et al., “Sentiment classiﬁcation in customer service dia-
logue with topic-aware multi-task learning,” in Proc. 34th AAAI
Conf. Artif. Intell., 32nd Innov. Appl. Artif. Intell. Conf., 10th AAAI
Symp. Educ. Adv. Artif. Intell. , 2020, pp. 9177–9184.
[26] L. Zhu, G. Pergola, L. Gui, D. Zhou, and Y. He, “Topic-driven and
knowledge-aware transformer for dialogue emotion detection,” in
Proc. 59th Annu. Meeting Assoc. Comput. Linguistics, 11th Int. Joint
Conf. Natural Lang. Process. , 2021, pp. 1571–1582.
[27] D. Hu, L. Wei, and X. Huai, “DialogueCRN: Contextual reasoning
networks for emotion recognition in conversations,” in Proc. 59th
Annu. Meeting Assoc. Comput. Linguistics, 11th Int. Joint Conf. Natu-
ral Lang. Process. , 2021, pp. 7042–7052.
[28] H. Ma, J. Wang, H. Lin, X. Pan, Y. Zhang, and Z. Yang, “A multi-
view network for real-time emotion recognition in conversations,”
Knowl.-Based Syst. , vol. 236, 2022, Art. no. 107751.
[29] W. Li, W. Shao, S. Ji, and E. Cambria, “BiERU: Bidirectional emo-
tional recurrent unit for conversational sentiment analysis,” Neu-
rocomputing , vol. 467, pp. 73–82, 2022.
[30] R. Hadsell, S. Chopra, and Y. LeCun, “Dimensionality reduction
by learning an invariant mapping,” in Proc. IEEE Comput. Soc.
Conf. Comput. Vis. Pattern Recognit. , 2006, pp. 1735–1742.
[31] M. Gutmann and A. Hyv €arinen, “Noise-contrastive estimation of
unnormalized statistical models, with applications to natural
image statistics,” J. Mach. Learn. Res. , vol. 13, pp. 307–361, 2012.
[32] J. Y. Zou, D. J. Hsu, D. C. Parkes, and R. P. Adams, “Contrastive
learning using spectral methods,” in Proc. Adv. Neural Inf. Process.
Syst., 2013, pp. 2238–2246.[33] K. He, H. Fan, Y. Wu, S. Xie, and R. B. Girshick, “Momentum con-
trast for unsupervised visual representation learning,” in Proc.
IEEE/CVF Conf. Comput. Vis. Pattern Recognit. , 2020, pp. 9726–9735.
[34] T. Chen, S. Kornblith, M. Norouzi, and G. E. Hinton, “A simple
framework for contrastive learning of visual representations,” in
Proc. 37th Int. Conf. Mach. Learn. , 2020, pp. 1597–1607.
[35] A. van den Oord, Y. Li, and O. Vinyals, “Representation learning
with contrastive predictive coding,” CoRR , 2018.
[36] L. Logeswaran and H. Lee, “An efﬁcient framework for learning
sentence representations,” in Proc. 6th Int. Conf. Learn. Representa-
tions, 2018.
[37] P. Cheng, W. Hao, S. Yuan, S. Si, and L. Carin, “FairFil: Contras-
tive neural debiasing method for pretrained text encoders,” in
Proc. 9th Int. Conf. Learn. Representations , 2021.
[38] S. Lee, D. B. Lee, and S. J. Hwang, “Contrastive learning with
adversarial perturbations for conditional text generation,” in Proc.
9th Int. Conf. Learn. Representations , 2021.
[39] L. Xiong et al., “Approximate nearest neighbor negative contras-
tive learning for dense text retrieval,” in Proc. 9th Int. Conf. Learn.
Representations , 2021.
[40] H. Cai et al., “Group-wise contrastive learning for neural dialogue
generation,” in Proc. Conf. Empir. Methods Natural Lang. Process. ,
2020, pp. 793–802. [Online]. Available: https://doi.org/10.18653/
v1/2020.ﬁndings-emnlp.70
[41] C.-S. Wu, S. C. Hoi, R. Socher, and C. Xiong, “TOD-BERT: Pre-
trained natural language understanding for task-oriented dia-
logue,” in Proc. Conf. Empir. Methods Natural Lang. Process. , 2020,
pp. 917–929.
[42] J. Liu et al., “Topic-aware contrastive learning for abstractive dia-
logue summarization,” CoRR , 2021.
[43] Y. Liu et al., “RoBERTa: A robustly optimized BERT pretraining
approach,” CoRR , 2019.
Hanqing Zhang received the BE degree from
Harbin Engineering University, Harbin, China, in
2017, and the ME degree from the Information
System and Security and Countermeasures
Experimental Center , Beijing Institute of Technol-
ogy, in 2020. Now he is working toward the PhD
degree with the School of Computer Science &
Technology , Beijing Institute of Technology . His
current research interests include affective com-
puting, controlled text generation and emotional
dialogue generation.
Dawei Song received the PhD degree from The
Chinese University of Hong Kong, in 2000, and
became a full professor with The Robert Gordon
University, U.K., in 2008. He is currently a profes-
sor in computer science with the Beijing Institute
of Technology, China. He has also worked as a
professor with The Open University U.K., and
Tianjin University, China. His research interest
has been focused on information retrieval and
natural language processing, especially novel
models and computational methods to support
intelligent access, retrieval and understanding of complex and multi-
modal information in a way that is compatible with human cognitive infor-
mation processing. He has published more than 200 research papers,
including those on prestigious journals such as ACM Transactions on
Information Systems and IEEE Transactions on Knowledge and Data
Engineering , and top conferences in natural language processing and
artiﬁcial intelligence such as SIGIR, WWW , ACL, AAAI, IJCAI and
NeurIPS.
"For more information on this or any other computing topic,
please visit our Digital Library at www.computer .org/csdl.ZHANG AND SONG: TOWARDS CONTRASTIVE CONTEXT-AWARE CONVERSATIONAL EMOTION RECOGNITION 1891
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:22 UTC from IEEE Xplore.  Restrictions apply. Inconsistency-Based Multi-Task Cooperative
Learning for Emotion Recognition
Yifan Xu , Yuqi Cui, Xue Jiang, Yingjie Yin, Jingting Ding,
Liang Li, and Dongrui Wu ,Senior Member, IEEE
Abstract— Emotion recognition is an important part of affective computing. Human emotions can be described categorically or
dimensionally . Accurate machine learning models for emotion classiﬁcation and estimation usually depend on a large amount of annotated
data. However , label acquisition in emotion recognition is costly: obtaining the ground-truth labels of an emotional sample usually requires
multiple annotators’ assessments, which is expensive and time-consuming. To reduce the labeling effort in multi-task emotions recognition,
the paper proposes an inconsistency measure that can indicate the difference between the labels estimated from the feature space and
the label distribution of labeled dataset. Using the inconsistency as an indicator of sample informativeness, we further propose an
inconsistency-based multi-task cooperative learning framework that integrates multi-task active learning and self-training semi-supervised
learning. Experiments in two multi-task emotion recognition scenarios, multi-dimensional emotion estimation and simultaneous emotion
classiﬁcation and estimation, were conducted under this framework. The results demonstrated that the proposed multi-task active learning
framework outperformed several single-task and multi-task active learning approaches.
Index Terms— Active learning, semi-supervised learning, multi-task learning, cooperative learning, emotion recognition
Ç
1I NTRODUCTION
AFFECTIVE computing aims to endow machines the ability
to recognize, interpret, and synthesize human affects for
harmonious human-machine interaction [1]. Emotion recog-
nition is an important part of affective computing. It attempts
to infer human emotions from various forms of inputs, e.g.,
facial expressions [2], gestures [3], speech [4], or physiologi-
cal signals [5].
Human emotions can be described both categorically and
dimensionally. Compared with intuitive categorical repre-
sentations, such as Ekman’s six basic emotions [6], dimen-
sional representations are more suitable for characterizing
continuous and ﬁne-grained emotions. Commonly used
dimensional emotion spaces include the Valence-Arousal 2D
space [7] and the Valence-Arousal-Dominance 3D space [8].
This paper considers two multi-task emotion recognition
scenarios: multi-dimensional emotion estimation (MDEE),
and simultaneous emotion classiﬁcation and estimation
(SECE). In MDEE, we consider emotion estimation in the 3DValance-Arousal-Dominance space. SECE considers further
emotion classiﬁcation, in addition to dimensional emotion
estimation.
Collecting unlabeled affective samples is usually easy
(e.g., videos and speeches can be easily recorded), but acquir-
ing their labels is costly and time-consuming, due to the
ambiguity and subjectiveness of emotions. Labeling affective
samples in multi-task emotion recognition, where labels in
different tasks need to be determined simultaneously, is par-
ticularly challenging. Active learning (AL) [9], [10] and semi-
supervised learning (SSL) [11] are commonly used remedies.
AL uses different strategies to estimate the usefulness of
unlabeled samples and selects the best ones to query for their
labels; thus, better learning performance can be achieved
from a small number of labeled samples. It has been used in
both emotion classiﬁcation and regression. Muhammad and
Alhamid [12] selected samples with large entropy (low clas-
siﬁcation conﬁdence) for labeling in facial emotion classiﬁca-
tion. Zhang et al. [13] selected samples with medium
uncertainty in support vector machine for labeling in speech
emotion classiﬁcation, and dynamically allocated annotators
for each sample until the user-speciﬁed annotation agree-
ment level was met. Han et al. [14] transformed single
dimensional emotion regression into a positive-negative
binary classiﬁcation problem, and selected the samples with
high uncertainty in the classiﬁcation model to annotate their
dimensional labels. This approach helps improve the correla-
tion coefﬁcient of the regression model. Abdelwahab and
Busso [15] evaluated the performance of an uncertainty-
based AL algorithm and three greedy sampling-based ones
(GSx, GSy and iGS in [16]) in valence and arousal estimation
using deep neural networks, and demonstrated that greedy
sampling in the feature space (GSx) can achieve both higher
concordance correlation coefﬁcient and lower variance. Wu
and Huang [10] extended two greedy sampling-based/C15Yifan Xu, Yuqi Cui, Xue Jiang, and Dongrui Wu are with the Key Labora-
tory of the Ministry of Education for Image Processing and Intelligent
Control, School of Artiﬁcial Intelligence and Automation, Huazhong Uni-
versity of Science and Technology, Wuhan 430074, China.
E-mail: {yfxu, yqcui, xuejiang, drwu}@hust.edu.cn.
/C15Yingjie Yin and Liang Li are with the Ant Group, World Financial Center,
Beijing 100024, China. E-mail: {gaoshi.yyj, double.ll}@antgroup.com.
/C15Jingting Ding is with the Ant Group, Hangzhou 310023, China.
E-mail: yimou.djt@antgroup.com.
Manuscript received 18 March 2022; revised 9 July 2022; accepted 5 August 2022.
Date of publication 9 August 2022; date of current version 15 November 2022.
This research was supported in part by CCF-AFSG Research Fund under
Grant RF20210007 and in part by Technology Innovation Project of Hubei
Province of China under Grant 2019AEA171.
(Corresponding author: Dongrui Wu.)
Recommended for acceptance by C. Busso.
Digital Object Identiﬁer no. 10.1109/TAFFC.2022.3197414IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022 2017
1949-3045 ©2022 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See ht _tps://www.ieee.org/publications/rights/index.html for more information.
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:24 UTC from IEEE Xplore.  Restrictions apply. single-task AL algorithms (GSy and iGS in [16]) to multi-task
AL in MDEE, by considering the diversity of the three affect
primitives simultaneously. It has been veriﬁed [10] that iGS
is more efﬁcient than expected model change maximization
[17] and query-by-committee (QBC) [18] in AL for regres-
sion, and the multi-task version [16] further improve its per-
formance in MDEE. Jiang et al. [19] used rank combination
[20] that weights the AL ranks of all single tasks to select the
most beneﬁcial samples to label in SECE.
AL only selects a small number of samples to query for
their labels. The remaining large amount of unlabeled sam-
ples in the data pool also contain rich information, which
can be exploited through SSL. For example, self-training
SSL ﬁrst uses the model trained in the previous iteration to
temporally label the samples, and then identiﬁes those with
high conﬁdence and assigns pseudo-labels to them. Zhang
et al. [21] proposed a cooperative learning approach that
combines self-training and AL in speech emotion recogni-
tion. Experiments on two binary classiﬁcation datasets veri-
ﬁed the effectiveness of cooperative learning and its multi-
view and mixed-view variants.
However, in dimensional emotion regression, it is difﬁ-
cult to directly compute the conﬁdence of the outputs in the
regression model and employ self-training like in emotion
classiﬁcation. This paper proposes an inconsistency mea-
sure that can indicate the difference between the labels esti-
mated from the feature space and the conditional label
distribution of the labeled samples, which only depends on
the relationship between the label spaces of different tasks.
The inconsistency can be viewed as an informativeness indi-
cator: samples with large inconsistency can increase the
label diversity of the labeled dataset.
Consider MDEE ﬁrst. Given an emotional sample that is
predicted to have low valence, high arousal, and low domi-
nance, e.g., a sample with fearemotion, we can calculate its
label inconsistency in the Dominance dimension using the
labeled dataset, based on its estimated labels in the other
two dimensions. First, we identify the labeled samples that
have similar Valence and Arousal values with this sample
and check their Dominance labels. Assume most of these
samples have high Dominance, e.g., with anger emotion.
Then, the given sample with low Dominance is inconsistent
with the label distribution of these similar samples, and can
thus increase the label diversity in Dominance. Similarly,
we can obtain its label inconsistency with the labeled data-
set in Valence and Arousal. Aggregating the label inconsis-
tency in all three dimensions, we obtain the sample’s total
inconsistency with the labeled dataset.
For SECE, we measure the label inconsistency differently,
since additional categorical labels are available. More speciﬁ-
cally, the dimensional label distributions are estimated from
the categorical labels, unlike in MDEE where the conditional
label distribution of each dimension is estimated from the
remaining two dimensions. For example, consider a sample
which is estimated to have surprise emotion, i.e., high
Valence, medium Arousal and medium Dominance. Assume
that most of the labeled samples having the same categorical
label with the given sample have high Valence, high Arousal
and medium Dominance. Then, the given sample is inconsis-
tent with the category-conditional label distribution in
Arousal but consistent in Valence and Dominance.Based on this informativeness measure, we further pro-
pose an inconsistency-based multi-task cooperative learning
(IMCL) framework that integrates AL and SSL. Speciﬁcally,
IMCL ﬁrst computes the inconsistency of the unlabeled sam-
ples in the tasks where conditional label distribution can be
estimated from other tasks, and integrates them into the total
inconsistency. Then, it selects the most inconsistent sample
to query for its label (i.e., it uses AL to select the most infor-
mativeness sample to manually label), and assigns pseudo-
labels to samples with low inconsistency (i.e., it uses self-
training SSL to label the high-conﬁdent samples, which are
consistent with the current label distribution), to enlarge the
labeled training set. The samples with manual annotations
and pseudo-labels are subsequently combined and utilized
to update corresponding task models. The overall ﬂowchart
of IMCL in a two-task dimensional emotion estimation appli-
cation is illustrated in Fig. 1.
The contributions of this paper are:
1) We propose an informativeness measure to repre-
sent the inconsistency between the estimated labels
of unlabeled samples and the true label distribution
of labeled samples.
2) Based on the inconsistency measure, we further pro-
pose IMCL, a multi-task cooperative learning frame-
work that integrates AL and SSL.
3) Experiments on two speech datasets and one image
dataset veriﬁed that our proposed IMCL can effec-
tively select valuable samples for annotation and uti-
lize unlabeled samples.
The remainder of the paper is organized as follows: Sec-
tion 2 introduces the framework of our proposed IMCL
approach. Section 3 describes the datasets and implementa-
tion details of IMCL in MDEE and SECE in the experiments.
Section 4 compares the performance of IMCL with other AL
approaches in MDEE and SECE, and discusses the results.
Section 5 draws conclusions and points out some future
research directions.
2I NCONSISTENCY -BASED MULTI-TASK
COOPERATIVE LEARNING (IMCL)
This section introduces our proposed IMCL framework.
2.1 Problem Setting
Assume the Task Set Tcontains more than one tasks in emo-
tion recognition. In MDEE, T¼f Valence estimation, Arousal
estimation, Dominance estimation g, and in SECE,T¼f Emo-
tion classiﬁcation, Valence estimation, Arousal estimation,
Dominance estimation g. The data pool consists of a small
number of labeled samples XL¼fðxxxxxxxL
i;yyyyyyyL
i;TÞgNL
i¼1and a large
number of unlabeled samples XU¼fxxxxxxxU
jgNU
j¼1,w h e r e xxxxxxxi2
Rd/C21is a d-dimensional feature vector, and yyyyyyyi2RjTj/C2 1its
jTj-dimensional label vector corresponding to the tasks in T.
2.2 Emotion Recognition Model ft
For each task t2T, we train a ridge regression (RR) model
for emotion regression, or a logistic regression (LR) model
for emotion classiﬁcation, ftðxxxxxxxÞ,o nfðxxxxxxxL
i;yL
i;tÞgNL
i¼1,u s i n gt h e
original features as inputs to estimate the labels of the
unlabeled samples.2018 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:24 UTC from IEEE Xplore.  Restrictions apply. For an unlabeled sample xxxxxxxU
j, its estimated label vector is
^yyyyyyyU
j;T¼^yU
j;1;...; ^yU
j;jTjhi
¼f1ðxxxxxxxU
jÞ;...; fjTjðxxxxxxxU
jÞhi
: (1)
2.3 Conditional Label Distribution Model gt
To represent the label distribution of the labeled dataset in
TasksTdis, we also construct a set of models fgtgt2Tdis.F o ra
Task tinTdis,gttakes the labels of the related tasks Trel
tas
inputs and outputs the conditional label distribution in Task t.
In MDEE, we estimate the conditional label distribution of
each task from the remaining ones, i.e., Tdis¼T¼f Valence
estimation, Arousal estimation, Dominance estimation gand
Trel
t¼fTn tg.gtadopts k-nearest neighbors ( kNN) regressor
with k¼5, which takes the estimated labels of an unlabeled
sample xxxxxxxU
jinTrel
t, i.e., ^yyyyyyyU
j;Trel
t, as inputs, ﬁnds its nearest
labeled neighbors in the label spaces of Trel
t, and averages
their labels in Task tas the conditional task label ~yU
j;t, i.e.,
~yU
j;t¼gt^yyyyyyyU
j;Trel
t/C18/C19
¼kNN ^yyyyyyyU
j;Trel
t/C18/C19
;t2Tdis: (2)
In SECE, the conditional distributions of the dimensional
emotions are computed from the estimated categorical emo-
tion probabilities ^yyyyyyyC, i.e.,Tdis¼fValence estimation,
Arousal estimation, Dominance estimation g, andTrel
t¼f
Emotion classiﬁcation g. We obtain the conditional task
labels of the dimensional emotions from the estimated cate-
gorical emotion probabilities ^yyyyyyyCthrough gð^yyyyyyyCÞ. Each ele-
ment ^yein^yyyyyyyC2RjEj/C21denotes the estimated probability for
Emotion ein the emotion category set E(an example is E¼
fangry, happy, excited, sad, frustrated g, as in our experi-
ments in the next section). gð^yyyyyyyCÞcomputes the average
dimensional emotion values for each emotion category onthe labeled samples, and multiplies them with the corre-
sponding emotion classiﬁcation probabilities in ^yyyyyyyU
C.
Speciﬁcally, for an emotion category ein SECE (e.g.,
happy), let yL
i;v,yL
i;aandyL
i;dbe the valence, arousal and dom-
inance values of xxxxxxxL
i, respectively; then, the average dimen-
sional values are
hhhhhhhe¼1
PNL
i¼1yi;eXNL
i¼1yL
i;e/C1yL
i;v;yL
i;a;yL
i;dhi
: (3)
yL
i;e2f0;1gdenotes if xxxxxxxL
ihas Emotion e.yL
i;e¼1means xxxxxxxL
i
belongs to emotion category eand should be included in
the calculation of the conditional labels for e, and vice versa.
The conditional task labels of the dimensional emotions
for an unlabeled sample xxxxxxxU
jare calculated by
~yyyyyyyU
j;Tdis¼gð^yyyyyyyU
j;CÞ¼X
e2E^yU
j;e/C1hhhhhhhe:(4)
Note that the original features are not considered at
all in gt.
2.4 Inconsistency Calculation
With the estimated label ^yU
j;tfrom ftand the conditional task
label ~yU
j;tfrom gtof Task t2Tdis, the inconsistency djof an
unlabeled sample xxxxxxxU
jcan then be computed. We employ
two different inconsistency calculation functions for MDEE
and SECE to demonstrate their ﬂexibility.
In MDEE, the inconsistency djis computed as
dj¼X
t2Tdisdj;t¼X
t2Tdis^yU
j;t/C0~yU
j;t/C12/C12/C12/C12/C12/C12;j¼1;...;NU:(5)
Fig. 1. The ﬂowchart of IMCL in two-task dimensional emotion estimation. It ﬁrst trains regression models fAandfBfor Tasks AandB, separately,
using the labeled samples, and uses them to estimate labels of the unlabeled samples. Then, it uses the labeled samples to construct the conditional
label estimation function gAandgBbetween the label spaces of the two tasks. The conditional task labels of the unlabeled samples are obtained
through gAandgBfrom their estimated labels. With estimated labels and conditional task labels, the inconsistency of each unlabeled sample can be
computed. IMCL next selects the most inconsistent sample to query for its groundtruth label, and assigns pseudo-labels to some other samples with
low inconsistency . These selected samples with true labels are then added to the training set and combined with the pseudo-labeled samples to
update the regression models fAandfB. Conditional label estimation functions gAandgBare next updated using manually labeled samples.XU ET AL.: INCONSISTENCY-BASED MULTI-TASK COOPERATIVE LEARNING FOR EMOTION RECOGNITION 2019
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:24 UTC from IEEE Xplore.  Restrictions apply. In SECE, the inconsistency djis computed as
dj¼^yyyyyyyU
j;Tdis/C0~yyyyyyyU
j;Tdis/C13/C13/C13/C13/C13/C13
2;j¼1;...;NU: (6)
2.5 Inconsistency-Based Multi-Task AL (IMAL)
The inconsistency indicates the label difference of t2Tdis
between an unlabeled sample and the current labeled data-
setXL, i.e., the diversity a sample could bring to XL. Thus,
it can be used as an informativeness indicator to guide AL.
We propose inconsistency-based multi-task active learn-
ing (IMAL), which selects the unlabeled sample xxxxxxxU
qwith the
maximum inconsistency and queries for its groundtruth
label in all tasks, i.e.,
q¼arg max
j¼1;...NUdj:(7)
The pseudo-code of IMAL is shown in Algorithm 1. Both
fftgt2Tandfgtgt2Tdisare updated iteratively during training
and used to select samples for AL. In the test stage, only ftis
used.
Algorithm 1. Inconsistency-Based Multi-Task Active
Learning (IMAL)
Input : Labeled training data XL¼fxxxxxxxL
i;yyyyyyyL
i;TgNL
i¼1;
Unlabeled training data XU¼fxxxxxxxU
jgNU
j¼1;
K, number of samples to be queried.
Output :jTjemotion recognition models fftgt2T.
fort2T do
UsefxxxxxxxL
i;yL
i;tgNL
i¼1to train ft;
end
fork¼1:Kdo
Estimatef^yyyyyyyU
j;TgNU
j¼1ofXUusing (1);
fort2Tdisdo
Construct the conditional label estimation function gt
usingfyyyyyyyL
i;Trel
t;yL
i;tgNL
i¼1;
Obtain the conditional task labels f~yU
j;tgNU
j¼1ofXUusing (2)
for MDEE or (4) for SECE;
end
Compute the inconsistency fdjgNU
j¼1using (5) for MDEE or
(6) for SECE;
Select the most inconsistent sample xxxxxxxU
qusing (7);
Query for yyyyyyyU
q;T, groundtruth labels of xxxxxxxU
qin all the tasks;
XU XUnxxxxxxxU
q,NU NU/C01;
XL XL[ðxxxxxxxU
q;yyyyyyyU
q;TÞ,NL NLþ1;
fort2T do
UsefxxxxxxxL
i;yL
i;tgNL
i¼1to update ft;
end
end
2.6 Inconsistency-Based SSL
We adopt self-training SSL to exploit the unlabeled samples
with high consistency between the estimated labels and the
conditional task labels. These samples are selected and
assigned pseudo-labels. Speciﬁcally, a subset of samples
selected under some user-speciﬁed rules, e.g., samples with
top-p% minimum inconsistency, are automatically assigned
pseudo-labels, and then added to the sample set XP
tand
incorporated into the training set to train the corresponding
task model ft.The pseudo-label of xxxxxxxU
jin Task t2Tdisis calculated by
/C22yj;t¼a/C2^yU
j;tþð1/C0aÞ/C2~yU
j;t; (8)
where a2½0;1/C138is the weight of the estimated labels in the
pseudo-labels, which is a hyper-parameter to be speciﬁed
by the user. For a large a, the estimated labels from ftwould
dominate the pseudo-labels. On the contrary, a small aena-
bles the conditional task labels from gtto dominate the
pseudo-labels.
2.7 IMCL
IMCL integrates IMAL and SSL to utilize both manually
and pseudo- labeled samples. Its pseudo-code is given in
Algorithm 2. We attempted to adopt different inconsistency
measures and self-training sample selection rules in MDEE
and SECE to verify the ﬂexibility of IMCL. More implemen-
tation details of IMCL in MDEE and SECE can be found in
Section 3.3.
Algorithm 2. Inconsistency-Based Multi-Task Coopera-
tive Learning (IMCL)
Input : Labeled training data XL¼fxxxxxxxL
i;yyyyyyyL
i;TgNL
i¼1;
Unlabeled training data XU¼fxxxxxxxU
jgNU
j¼1;
a, weight of the estimated label in (8);
K, number of samples to be queried;
Sample selection rules in SSL.
Output :jTjemotion recognition models fftgt2T.
fort2T do
UsefxxxxxxxL
i;yL
i;tgNL
i¼1to train ft;
end
fork¼1:Kdo
Estimatef^yyyyyyyU
j;TgNU
j¼1ofXUusing (1);
Initialize XP
tto;,t2Tdis;
fort2Tdisdo
Construct the conditional label estimation function gt
usingfyyyyyyyL
i;Trel
t;yL
i;tgNL
i¼1;
Obtain the conditional task labels f~yU
j;tgNU
j¼1ofXUusing (2)
for MDEE or (4) for SECE;
end
forj¼1:NUdo
fort2Tdisdo
Add sample xxxxxxxU
jand its corresponding pseudo-label /C22yj;t
computed by (8) to XP
t, if it satisﬁes sample selection
rules in SSL;
end
end
Compute the inconsistency fdjgNU
j¼1using (5) for MDEE
or (6) for SECE;
Select the most inconsistent sample xxxxxxxU
qusing (7);
Query for yyyyyyyU
q;T, groundtruth labels of xxxxxxxU
qin all the tasks;
XU XUnxxxxxxxU
q,NU NU/C01;
XL XL[ðxxxxxxxU
q;yyyyyyyU
q;TÞ,NL NLþ1;
fort2T do
ift2Tdisdo
UsefxxxxxxxL
i;yL
i;tgNL
i¼1[XP
tto update ft;
else
UsefxxxxxxxL
i;yL
i;tgNL
i¼1to update ft;
end
end
end2020 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:24 UTC from IEEE Xplore.  Restrictions apply. Bothfftgt2Tandfgtgt2Tdisare updated iteratively during
training and used to select samples for AL and SSL. In the
test stage, only ftis used.
3D ATASETS AND EXPERIMENTAL SETUP
This section presents experimental results to demonstrate
the effectiveness of the proposed IMAL and IMCL.
3.1 Datasets and Feature Extraction
We veriﬁed the performance of our proposed IMAL and
IMCL in MDEE on three public affective computing data-
sets, VAM, IAPS and IEMOCAP, with Valence-Arousal-
Dominance dimensional emotion annotations. Their charac-
teristics are shown in Table 1. Experiments on SECE were
conducted on the IEMOCAP dataset, which has both cate-
gorical and dimensional emotion annotations.
The VAM corpus [22] contains 947 emotional utterances
collected from 47 guests (11m/36f) in a German TV talk-
show Vera am Mittag (Vera at Noon in English). The
weighted average values of emotion primitives from several
evaluators were used as the ground-truth labels of each sen-
tence. We used 46 acoustic features, including nine pitch
features, ﬁve duration features, six energy features, and 26
Mel Frequency Cepstral Coefﬁcient (MFCC) features, as
in [4], [10], [23]. Each feature was normalized to mean 0 and
standard deviation 1.
The IAPS (International Affective Picture System) dataset
[24] used documentary-style natural color images that can
evoke strong emotions as stimuli. It includes 1,182 images,
but we removed four duplicate ones. The mean valence,
arousal and dominance values collected from multiple
annotators were used as ground-truth labels. ResNet50 [25]
pre-trained on ImageNet [26] was used as the encoder to
extract features from the images. Principal component anal-
ysis was used to reduce the dimensionality to 30. Each fea-
ture was then z-normalized.
IEMOCAP (Interactive Emotional Dyadic Motion Cap-
ture Database) [27] is a multi-modal dataset annotated with
both categorical and dimensional emotion labels. We used
only the audio modality. Because IEMOCAP has both cate-
gorical and dimensional annotations, it was used in experi-
ments of both MDEE and SECE. Only ﬁve emotion
categories (‘angry’, ‘happy’, ‘excited’, ‘sad’ and ‘frustrated’
with 289, 284, 663, 608 and 971 samples, respectively) were
selected. 35 features were used [19], including two signal
amplitude features (mean and standard deviation), twoenergy features, one pause feature, one harmonics feature,
two pitch features, one zero-crossing rate feature, and 26
MFCC features. Each feature was also z-normalized. The
datasets used in MDEE and SECE were identical, except
that MDEE only used the dimensional labels, whereas SECE
further used the categorical emotion label.
3.2 Experimental Setup
We compared the performances of the following seven sam-
ple selection strategies:
1) Baseline-all (BL-all), which assumes all samples in
the data pool are labeled, and uses them to construct
the models for each task. This represents the perfor-
mance upper bound that an algorithm can achieve.
2) Random sampling (Rand), which selects all Ksam-
ples randomly to annotate.
3) Single-task AL for Task t(ST-t). In MDEE, iGS [16]
was used for each single emotion regression task.
iGS uses greedy sampling in both feature and label
spaces. In SECE, only single-task AL for classiﬁca-
tion (denoted as ST) was considered. A classical
uncertainty-based AL algorithm that selects the sam-
ple with the least maximum classiﬁcation conﬁdence
for annotation was used.
4) Multi-task iGS (MT-iGS) [10] that extends the single-
task iGS to multi-task learning, by considering fea-
ture diversity and label diversity in the three affect
primitives simultaneously.
5) Rank combination (RankComb) [19], which com-
putes the weighted average of each sample’s AL
ranks in different tasks and then selects the sample
with the smallest overall rank to annotate. This
approach was only used as a baseline in SECE. The
AL approach in emotion classiﬁcation was identical
to that in ST, and in each emotion estimation task
was iGS [16]. The parameter settings, i.e., the rank
weights of the tasks, were identical to those in [19].
6) IMAL, which was introduced in Algorithm 1 that
uses inconsistency to select the samples for human
annotation.
7) IMCL, which was introduced in Algorithm 2. The
weight ain calculating the pseudo-labels in (8) was
set to 0.5.
3.3 Implementation Details
In MDEE, we used Ridge Regression (RR) as the base model
in each dimensional emotion estimation task. The weight of
the regularization term in RR was set to 10=NL, as in [10].
MDEE used thresholding to identify the samples with
high consistency and assigned them pseudo-labels. For each
Task t, the samples with inconsistency dj;tcomputed from
(5) no larger than a user-speciﬁed threshold ttwas assigned
pseudo-labels using (8). We set the threshold ttin Task tto
et=2, where etwas the root mean squared error (RMSE) of ft
onXL.
In SECE, we used a Logistic Regression base model for
emotion classiﬁcation, in addition to the three RR base mod-
els for Valence-Arousal-Dominance estimation. The weight
of the regularization term in all models was set to 10=NL.TABLE 1
Characteristics of the Three Affective Computing Datasets
Dataset Size dValence Arousal Dominance
(mean/C6std) (mean/C6std) (mean/C6std)
VAM 947 46-0.2282 0.0280 0.0924
/C60.1991/C60.3425/C60.3025
IAPS 1,178 305.0314 4.8159 5.1580
/C61.7708/C61.1509/C61.0811
IEMOCAP 2,815 352.8272 3.1829 3.2481
/C61.0576/C60.7606/C60.8077
‘Size’ denotes the number of samples, and dis the feature dimensionality.XU ET AL.: INCONSISTENCY-BASED MULTI-TASK COOPERATIVE LEARNING FOR EMOTION RECOGNITION 2021
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:24 UTC from IEEE Xplore.  Restrictions apply. SECE used a different sample selection strategy: top- p%
(pwas set to 5 in the experiments) unlabeled samples with
the minimum inconsistency calculated from (6) were
assigned pseudo-labels in all three dimensional emotions.
Here we set both pin SECE and ttin MDEE empirically to a
median value. We did not assign pseudo-labels of categori-
cal emotion to the unlabeled samples.
Note that different conditional label estimation func-
tions in MDEE and SECE were designed according to the
characteristics of the task scenario, and different choices
of aggregation functions and self-training sample selec-
tion rules were used to verify the ﬂexibility of IMAL and
IMCL.
3.4 Performance Evaluation
For the two speech datasets, we consider the cross-speaker
scenario, i.e., the speakers in the training set do not overlap
at all with those in the test set.
In VAM, each speaker has different numbers of samples,
ranging from 4 to 46. We ﬁrst randomly shufﬂed the indices
of the speakers, and then added all samples from each indi-
vidual speaker according to the shufﬂed order one by one,
until the total number of selected samples reached 30% of
the original dataset size (947). Finally, we used these
30%/C1947 samples as the training set and the remaining
70%/C1947 samples for test to validate the performances of dif-
ferent algorithms. This process was repeated 100 times, and
the average results were reported.
IEMOCAP has ﬁve sessions, each including emotional
samples from two-speaker interactions. Each time we used
two sessions as the training set and the remaining three as
the test set, and then switched the training and test set,
resulting in 10 different training-test partitions in total.
Experiments on each dataset partition were repeated ten
times with different initial labeled samples.
For IAPS, the image dataset, we randomly selected 30%
of the samples as the training set, and the remaining 70% as
the test set, and repeated this process 100 times.
For all three datasets, the initial dþ1labeled samples
were selected randomly, where dis the feature dimensional-
ity of each dataset, as in [9]. Then, we iteratively selected
samples from the unlabeled data pool by different algo-
rithms to annotate, and updated the emotion recognition
models accordingly.
We used RMSE and correlation coefﬁcient (CC) to evalu-
ate the performance of the regression models, where RMSE
directly indicates the prediction error and was our primary
performance measure. In emotion classiﬁcation, weighted
accuracy (the average of the per-class accuracies) was used
to measure the performance of the classiﬁcation models,
eliminating the inﬂuence of class imbalance. The average
RMSEs and CCs in emotion estimation, and weighted accu-
racies in emotion classiﬁcation, are reported. 200 iterations
of sample selection were performed on VAM and IAPS, and
300 on IEMOCAP due to its larger size.
4R ESULTS AND DISCUSSIONS
The section presents our experimental results in MDEE
and SECE, parameter sensitivity analysis, and additional
discussions.4.1 Experimental Results in MDEE
The performances of different sample selection approaches
in MDEE are shown in Fig. 2. To emphasize the perfor-
mance differences among different algorithms at the initial
sampling stage, the horizontal axis used logarithm scale.
Fig. 2 shows that:
1) As the number of labeled samples increased, the per-
formance of all models improved and gradually con-
verged to BL-all. This is intuitive, since more labeled
samples result in more reliable regression models.
2) All three single-task AL algorithms performed much
better than random sampling on all three tasks, no
matter which task it focused on. For a particular
Task t, ST- talways outperformed ST- t0(t06¼t). The
results veriﬁed the effectiveness of single-task iGS
even in multi-task scenarios, due to the intrinsic rela-
tionship among the tasks. We can also observe that
among the three single-task AL algorithms, whereas
usually ST- tranked ﬁrst in Task t(the task it focused
on), it ranked poorly in the other two tasks. This is
due to the fact that ST- temphasized too much on a
single task, and hence may sacriﬁce its performances
on other tasks, i.e., single-task AL cannot achieve
good compromise among multiple tasks.
3) MT-iGS takes all three tasks into consideration
simultaneously, instead of emphasizing only one of
them. For a particular Task t, MT-iGS either achieved
comparable performance with ST- t, or performed
only slightly worse than ST- tbut much better than
ST-t0(t06¼t). Its overall superior performance on all
three tasks demonstrated the advantage of multi-
task AL over single-task AL: it can achieve a better
trade-off among multiple tasks.
4) Our proposed IMAL achieved comparable perfor-
mance with MT-iGS on all three datasets, since the
sample selection criteria of these two approaches are
both based on sample diversity. This demonstrates
that the proposed inconsistency is a valid measure of
informativeness in AL. The average performance of
IMAL on all three tasks was generally better than
that of the single-task AL approaches.
5) IMCL always reduced the RMSE of IMAL, many times
also increased the CC. However, the increase of CC
was not guaranteed, because the training of the regres-
sion models explicitly minimized the RMSE, but no
objective was imposed on the CC directly. Surpris-
ingly, IMCL sometimes even outperformed the best-
performing single-task AL, suggesting the beneﬁts of
SSL.
Tables 2, 3, and 4 respectively show the average RMSEs
on all three emotion dimensions at the 5th, 10th, 20th and
50th iterations on the three datasets. The performance gaps
among different algorithms became smaller when the num-
ber of labeled samples increased, so the results after more
iterations are omitted. IMCL achieved the lowest RMSEs on
all three datasets.
To check if the performance improvement of IMCL over
other approaches were statistically signiﬁcant, paired t-tests
with Holm’s p-value adjustment [28] were performed. Those
with the adjusted p-values smaller than 0.05 are marked with2022 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:24 UTC from IEEE Xplore.  Restrictions apply. * in Tables 2, 3, and 4. Most results were statistically sig-
niﬁcant, especially at the initial sample selection stages,
suggesting that IMCL was efﬁcient in selecting valuable
unlabeled samples for annotation and utilizing unsuper-
vised information.
In summary, on average, IMCL performed the best
among all algorithms, demonstrating the effectiveness of
integrating AL and SSL.4.2 Experimental Results in SECE
The average performances in SECE on the IEMOCAP data-
set are shown in Fig. 3. Logarithm scale was also used on
the horizontal axis. The classiﬁcation performance of differ-
ent approaches did not vary much.
As the dimensional emotions are more ﬁne-grained and
contain richer information, we mainly focus on the perform-
ances of the dimensional regression tasks. The average RMSEsFig. 2. Average performance of different sample selection algorithms in MDEE on (a) VAM, (b) IAPS, (c) IEMOCAP . Generally our proposed IMCL
achieved the best performance, and IMAL the second best. Kis the number of samples selected by different strategies to be manually annotated, in
addition to the initial dþ1randomly selected labeled samples.XU ET AL.: INCONSISTENCY-BASED MULTI-TASK COOPERATIVE LEARNING FOR EMOTION RECOGNITION 2023
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:24 UTC from IEEE Xplore.  Restrictions apply. at the 5th, 10th, 20th and 50th iterations are shown in Table 5.
Paired t-tests with Holm’s p-value adjustment were also per-
formed to check if IMCL signiﬁcantly outperformed others.
Those with the adjusted p-values smaller than 0.05 are marked
with *.
The following observations can be made from Fig. 3 and
Table 5 in SECE, similar to those in MDEE:
1) Different sample selection approaches achieved sim-
ilar performance in emotion classiﬁcation. Although
single-task AL on emotion classiﬁcation may not
beneﬁt the classiﬁcation performance much, it still
helped improve the emotion estimation perfor-
mance, demonstrating that there exists some intrin-
sic relationship between categorical and dimensional
emotions.
2) MT-iGS outperformed both RankComb and ST, indi-
cating again that it is a very strong approach.
3) Our proposed IMAL outperformed MT-iGS slightly,
but other AL approaches by a large margin. IMCL
further improved IMAL by incorporating samples
with pseudo-labels through SSL, achieving the best
performance.
4.3 Parameter Sensitivity
There are two hyper-parameters in our proposed IMCL in
Algorithm 2: a, weight of the estimated label in (8), and the
sample selection rules in SSL, i.e., threshold vector tttttttin
MDEE and percentage pin SECE. Experiments in MDEE
were carried out to analyze their inﬂuence.4.3.1 Effect of Weight a
To analyze the sensitivity of IMCL to the weight ain the calcu-
lation of the pseudo-labels, we ﬁxed each tttoet=2and con-
ducted experiments for a¼f0:1;0:5;0:9g. The average
results on all three emotion dimensions of each dataset are
s h o w ni nF i g .4 .I M C La l m o s ta l w a y so u t p e r f o r m e dI M A Lo n
RMSE, regardless of a, though their CCs were similar. Again,
this may be due to the fact that only the RMSE was explicitly
considered in the training of the regression models fftgt2T.
For different a, the performance of IMCL was quite sta-
ble, especially for the RMSE. A closer look may reveal that
generally a smaller aresulted in slightly better performance,
especially when Kwas small. This is because when the size
of the labeled sample set is small, kNN can better ﬁt the
sparse conditional label distribution. As the number of
labeled sample increases, the label distribution becomes
more complex and overwhelms the ﬁtting capability of
kNN. Hence, the label estimator gtis more likely to help
improve the performance of ft, and the estimated condi-
tional label should be assigned a larger weight, correspond-
ing to a smaller a. These observations suggest that maybe
an adaptive ashould be used for better performance.
4.3.2 Effect of Threshold ttttttt
We also conducted experiments with ttttttt¼feeeeeee; eeeeeee=2;eeeeeee=5g
[where each et2eeeeeeewas the training RMSE of the RR model
in Task t] while ﬁxing a¼0:5. The average results on all
three emotion dimensions are shown in Fig. 5. Again, IMCL
almost always outperformed IMAL on RMSE, regardless of
a, though their CCs were similar.
For different ttttttt, the performance of IMCL on VAM and
IAPS was quite stable. A closer look may reveal that gener-
ally a larger tttttttresulted in slightly better performance, espe-
cially when Kwas small. This is because when Kwas small
(the number of labeled samples was small), it is more bene-
ﬁcial to make use of more pseudo-labeled samples (corre-
sponding to a larger ttttttt), though they may be noisy.
However, as Kincreased, gtwas not able to ﬁt the condi-
tional label distribution well. As a result, the beneﬁt of the
pseudo-labeled samples gradually vanished, and using too
many such samples may even hurt the performance, as
more clearly demonstrated on IEMOCAP. These observa-
tions suggest that maybe an adaptive tttttttcould be used for
better performance.TABLE 2
Average RMSEs of the Three Emotion Dimensions at the 5th,
10th, 20th and 50th Iterations, on VAM in MDEE
Iteration 5 10 20 50
Rand 0.3876* 0.3654* 0.3352* 0.2796*
ST-V 0.3643* 0.3302* 0.2973* 0.2586*
ST-A 0.3602* 0.3300* 0.2994* 0.2610*
ST-D 0.3594* 0.3341* 0.3012* 0.2605*
MT-iGS 0.3559* 0.3206* 0.2855 0.2555*
IMAL (ours) 0.3532* 0.3186* 0.2858 0.2483
IMCL (ours) 0.3298 0.3041 0.2785 0.2484
* means IMCL outperformed the corresponding approach signiﬁcantly in
paired t-test with Holm’s p-value adjustment ( a¼0:05).
TABLE 3
Average RMSEs of the Three Emotion Dimensions at the 5th,
10th, 20th and 50th Iterations, on IAPS in MDEE
Iteration 5 10 20 50
Rand 2.2146* 2.0333* 1.7496* 1.4337*
ST-V 1.9425* 1.6552* 1.4624* 1.3124*
ST-A 2.0154* 1.7179* 1.5082* 1.3229*
ST-D 1.9329* 1.6666* 1.4657* 1.3078*
MT-iGS 1.9038* 1.5956* 1.4071* 1.2928
IMAL (ours) 1.8558* 1.5917* 1.4182* 1.3040
IMCL (ours) 1.6159 1.4708 1.3760 1.2906
* means IMCL outperformed the corresponding approach signiﬁcantly in
paired t-test with Holm’s p-value adjustment ( a¼0:05).TABLE 4
Average RMSEs of the Three Emotion Dimensions at the 5th,
10th, 20th and 50th Iterations, on IEMOCAP in MDEE
Iteration 5 10 20 50
Rand 1.2232* 1.1975* 1.1302* 1.0083*
ST-V 1.1731* 1.0854* 1.0087* 0.9140*
ST-A 1.1610* 1.1031* 1.0307* 0.9287*
ST-D 1.1536* 1.0865* 1.0210* 0.9303*
MT-iGS 1.1579* 1.0512* 0.9664* 0.9072*
IMAL (ours) 1.1351* 1.0441* 0.9550* 0.8869
IMCL (ours) 1.0362 0.9470 0.8951 0.8860
* means IMCL outperformed the corresponding approach signiﬁcantly in
paired t-test with Holm’s p-value adjustment ( a¼0:05).2024 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:24 UTC from IEEE Xplore.  Restrictions apply. 4.4 Discussion
The proposed IMCL may be explained as utilizing the pre-
diction inconsistency from different models, ftand gt,t o
measure the informativeness of unlabeled samples and
selecting the most informative ones for annotation. From this
aspect, it resembles the model disagreement-based AL
approaches. Typically, the classical QBC [18] constructs a
committee of several base learners that trained on different
subsets of labeled samples and selects the samples with max-
imum disagreement among the base learners. In multi-view
learning, Muslea et al. [29] proposed to annotate samples
that have inconsistent predictions from models in multiple
views with different disagreement measure strategy.
However, IMCL has different motivation from these
approaches: gtrepresents the conditional label distribution
ofXL, whereas ftis the task learner; the inconsistency
between ftand gtmeasures the label diversity a sample
could bring to XL. Whereas in conventional committee-
based approaches, every model is a task learner, and the
disagreement among them is usually supposed to be a mea-
sure of the prediction uncertainty.
5C ONCLUSION AND FUTURE RESEARCH
Discrete categories and continuous dimensional primitives
are common emotion representations. To recognize categor-
ical and dimensional emotions, usually a large number of
labeled samples are required to train the classiﬁcation and
regression models. Manually labeling the samples in multi-task emotion recognition is costly and time-consuming, due
to the ambiguity and subjectiveness of emotions. Multi-task
AL can be used to save the labeling effort in MDEE and
SECE and it can be further integrated with SSL to improve
the performance of emotion recognition models with lim-
ited labeled samples.
This paper has proposed an inconsistency measure that
can indicate the difference between the labels estimated from
the feature space and label distribution of the labeled dataset.
It is then used in multi-task AL to select the most inconsistent
sample to label, and in self-training SSL to select the least
inconsistent samples to assign pseudo-labels. Experiments on
three popular affective computing datasets demonstrated the
effectiveness of our proposed IMCL, which integrates AL and
SSL. It generally outperformed a state-of-the-art single-task
AL approach and two multi-task AL approaches.
Our research can be improved or extended in the follow-
ing ways:
1) Our proposed inconsistency measure utilizes the
relationship among multiple tasks and is used as an
informativeness indicator in AL and SSL. There are
other important factors that could be considered in
AL for regression [9], e.g., feature diversity and rep-
resentativeness. They are complementary to label
diversity and hence can be considered simulta-
neously for better performance. Our future research
will integrate inconsistency with feature diversity
and representativeness.
2) In our current approach, the ﬁrst dþ1labeled samples
are randomly selected. This initialization can also be
optimized. For example, in [9] it has been shown that
the representativeness and feature diversity criteria
can be used for better initialization in AL for regres-
sion. Similar strategies will also be considered in our
future research.
3) In our experiments, we ﬁxed the hyper-parameters a
and set the sample selection rules in SSL heuristi-
cally. Though IMAL and IMCL are not very sensitive
to them, they do have some impact on the perfor-
mance. Our future research will consider adaptive a
and sample selection threshold/percentage in SSL
with K, as suggested in Section 4.3. Additionally, theFig. 3. Average performance of different sample selection algorithms on IEMOCAP in SECE (valence-arousal-dominance estimation and also emo-
tion classiﬁcation). Generally our proposed IMCL achieved the best performance, and IMAL the second best. Kis the number of samples selected
by different strategies to be manually annotated, in addition to the initial dþ1randomly selected labeled samples.
TABLE 5
Average RMSEs of the Three Emotion Dimensions at the 5th,
10th, 20th and 50th Iterations, on IEMOCAP in SECE
Iteration 5 10 20 50
Rand 1.1441* 1.1087* 1.0350* 0.9210*
ST 1.1430* 1.1043* 1.0403* 0.9247*
MT-iGS 1.0742* 0.9845* 0.8993* 0.8377*
RankComb 1.0852* 1.0265* 0.9481* 0.8595*
IMAL (ours) 1.0616* 0.9696* 0.8796* 0.8163
IMCL (ours) 0.9973 0.9325 0.8691 0.8174
* means IMCL outperformed the corresponding approach signiﬁcantly in
paired t-test with Holm’s p-value adjustment ( a¼0:05).XU ET AL.: INCONSISTENCY-BASED MULTI-TASK COOPERATIVE LEARNING FOR EMOTION RECOGNITION 2025
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:24 UTC from IEEE Xplore.  Restrictions apply. models used in ftandgtmay also change with Kto
better ﬁt the distribution of dataset.
4) This paper only considered the inconsistency in
MDEE and SECE. There also exist other multi-task sce-
narios in affective computing, e.g., determining both
emotions and paralinguistics [30] in speech simulta-
neously. How to extend the inconsistency measure to
such scenarios is another interesting problem.
REFERENCES
[1] J. Tao and T. Tan, “Affective computing: A review,” in Proc. Int.
Conf. Affect. Comput. Intell. Interaction , 2005, pp. 981–995.
[2] M. Pantic and L. Rothkrantz, “Automatic analysis of facial expres-
sions: The state of the art,” IEEE Trans. Pattern Anal. Mach. Intell. ,
vol. 22, no. 12, pp. 1424–1445, Dec. 2000.
[3] R. E. Kaliouby and P. Robinson, “Real-time inference of complex
mental states from facial expressions and head gestures,” in Proc.
Int. Conf. Comput. Vis. Pattern Recognit. , 2004, Art. no. 154.
[4] D. Wu, T. D. Parsons, E. Mower, and S. Narayanan, “Speech emo-
tion estimation in 3D space,” in Proc. IEEE Int. Conf. Multimedia
Expo , 2010, pp. 737–742.
[5] D. Wu et al., “Optimal arousal identiﬁcation and classiﬁcation for
affective computing: Virtual Reality Stroop Task,” IEEE Trans.
Affect. Comput. , vol. 1, no. 2, pp. 109–118, Jul.–Dec. 2010.
[6] P. Ekman et al., “Universals and cultural differences in the judg-
ments of facial expressions of emotion,” J. Pers. Soc. Psychol. ,
vol. 53, no. 4, 1987, Art. no. 712.[7] J. A. Russell, “A circumplex model of affect,” J. Pers. Soc. Psychol. ,
vol. 39, no. 6, 1980, Art. no. 1161.
[8] A. Mehrabian, Basic Dimensions for a General Psychological The-
ory: Implications for Personality, Social, Environmental, and Devel-
opmental Studies . Cambridge, MA, USA: Oelgeschlager, Gunn
& Hain, 1980.
[9] D. Wu, “Pool-based sequential active learning for regression,”
IEEE Trans. Neural Netw. Learn. Syst. , vol. 30, no. 5, pp. 1348–1359,
May 2019.
[10] D. Wu and J. Huang, “Affect estimation in 3D space using multi-
task active learning for regression,” IEEE Trans. Affect. Comput. ,
vol. 13, no. 41, pp. 16–27, Jan–March. 2022.
[11] D. Wu, “Active semi-supervised transfer learning (ASTL) for ofﬂine
BCI calibration,” in Proc. IEEE Int. Conf. Syst., Man Cybern. , 2017,
pp. 246–251.
[12] G. Muhammad and M. F. Alhamid, “User emotion recognition
from a larger pool of social network data using active learning,”
Multimedia Tools Appl. , vol. 76, no. 8, pp. 10 881–10 892, 2017.
[13] Y. Zhang, E. Coutinho, Z. Zhang, C. Quan, and B. Schuller,
“Dynamic active learning based on agreement and applied to
emotion recognition in spoken interactions,” in Proc. ACM Int.
Conf. Multimodal Interaction , 2015, pp. 275–278.
[14] W. Han, H. Li, H. Ruan, L. Ma, J. Sun, and B. W. Schuller, “Active
learning for dimensional speech emotion recognition,” in Proc.
Annu. Conf. Int. Speech Commun. Assoc. , 2013, pp. 2841–2845.
[15] M. Abdelwahab and C. Busso, “Active learning for speech emo-
tion recognition using deep neural network,” in Proc. Int. Conf.
Affect. Comput. Intell. Interaction , 2019, pp. 1–7.
[16] D. Wu, C.-T. Lin, and J. Huang, “Active learning for regression
using greedy sampling,” Inf. Sci. , vol. 474, pp. 90–105, 2019.Fig. 5. Average RMSEs and CCs of IMCL on the three emotion dimensions in MDEE using different t.Kis the number of samples selected by differ-
ent strategies to be manually annotated, in addition to the initial dþ1randomly selected labeled samples.Fig. 4. Average RMSEs and CCs of IMCL on the three emotion dimensions in MDEE using different a.Kis the number of samples selected by differ-
ent strategies to be manually annotated, in addition to the initial dþ1randomly selected labeled samples.2026 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:24 UTC from IEEE Xplore.  Restrictions apply. [17] W. Cai, Y. Zhang, and J. Zhou, “Maximizing expected model
change for active learning in regression,” in Proc. Int. Conf. Data
Mining , 2013, pp. 51–60.
[18] T. RayChaudhuri and L. G. Hamey, “Minimisation of data collec-
tion by active learning,” in Proc. IEEE Int. Conf. Neural Netw. , 1995,
pp. 1338–1341.
[19] X. Jiang, L. Meng, and D. Wu, “Multi-task active learning for
simultaneous emotion classiﬁcation and regression,” in Proc. IEEE
Int. Conf. Syst., Man, Cybern. , 2021, pp. 1947–1952.
[20] R. Reichart, K. Tomanek, U. Hahn, and A. Rappoport, “Multi-task
active learning for linguistic annotations,” in Proc. Annu. Meeting
Assoc. Comput. Linguistics , 2008, pp. 861–869.
[21] Z. Zhang, E. Coutinho, J. Deng, and B. Schuller, “Cooperative
learning and its application to emotion recognition from speech,”
IEEE/ACM Trans. Audio, Speech, Lang. Process. , vol. 23, no. 1,
pp. 115–126, Jan. 2015.
[22] M. Grimm, K. Kroschel, and S. Narayanan, “The Vera am Mittag
German audio-visual emotional speech database,” in Proc. IEEE
Int. Conf. Multimedia Expo , 2008, pp. 865–868.
[23] D. Wu, T. D. Parsons, and S. S. Narayanan, “Acoustic feature anal-
ysis in speech emotion primitives estimation,” in Proc. Annu. Conf.
Int. Speech Commun. Assoc. , 2010.
[24] P. J. Lang, M. M. Bradley, and B. N. Cuthbert, “International affec-
tive picture system (IAPS): Technical manual and affective ratings,”
NIMH Center Study Emotion Attention , vol. 1, pp. 39–58, 1997.
[25] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for
image recognition,” in Proc. IEEE Conf. Comput. Vis. Pattern Recog-
nit., 2016, pp. 770–778.
[26] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Image
Net: A large-scale hierarchical image database,” in Proc. IEEE
Conf. Comput. Vis. Pattern Recognit. , 2009, pp. 248–255.
[27] C. Busso et al., “IEMOCAP: Interactive emotional dyadic motion cap-
ture database,” Lang. Resour. Eval. , vol. 42, no. 4, pp. 335–359, 2008.
[28] S. Holm, “A simple sequentially rejective multiple test procedure,”
Scand. J. Statist. , vol. 6, no. 2, pp. 65–70, 1979.
[29] I. Muslea, S. Minton, and C. A. Knoblock, “Active learning with
multiple views,” J. Artif. Intell. Res. , vol. 27, pp. 203–233, 2006.
[30] B. Schuller et al., “Paralinguistics in speech and language – State-
of-the-art and the challenge,” Comput. Speech Lang. , vol. 27, no. 1,
pp. 4–39, 2013.
Yifan Xu received the BE degree in automation
from the Huazhong University of Science and
Technology, Wuhan, China, in 2018, where she is
currently working toward the PhD degree in artiﬁ-
cial intelligence with the School of Artiﬁcial Intelli-
gence and Automation. Her research interests
include affective computing, brain–computer inter-
faces, and machine learning.
Yuqi Cui received the BE degree in electronic
information engineering, and the PhD degree in
control science and engineering both from the
Huazhong University of Science and Technology,
Wuhan, China, in 2017 and 2022, respectively .
His research interests include machine learning,
fuzzy systems, and brain-computer interfaces.
Xue Jiang received the BE degree in communi-
cation engineering from Southwest University,
Chongqing, China, in 2019. She is currently work-
ing toward the PhD degree in control science and
engineering with the School of Artiﬁcial Intelli-
gence and Automation, Huazhong University of
Science and Technology, Wuhan, China. Her
research interests include brain-computer interfa-
ces and machine learning.
Yingjie Yin received the PhD degree in pattern
recognition and artiﬁcial intelligence from the Insti-
tute of Automation, Chinese Academy of Sciences,
Beijing, China, in 2016. He was an assistant profes-
sor with the Research Center of Precision Sensing
and Control, Institute of Automation, Chinese
Academy of Sciences, from 2016 to 2018, and an
associate professor from 2018 to 2020. Between
2017 and 2019, he was also a Hong Kong Scholar
with the Department of Computing, Hong Kong
Polytechnic University , Hong Kong, China. He
joined Ant Group, in 2020 and is now a staff engineer of the Biometrics
Technology Research Group. His research interests include computer
vision and pattern recognition.
Jingting Ding received the PhD degree from
Zhejiang university , China, in 2012. He joined Ant
Group, in 2015, and is now a staff engineer . His
main research interest is deep learning for com-
puter vision, focusing particularly on biometrics
and security , including face liveness, adversarial
examples and Deepfake.
Liang Li received the PhD degree in pattern recog-
nition and artiﬁcial intelligence from the Institute of
Automation, Chinese Academy of Sciences, in
2007. He is a senior staff engineer with IOT busi-
ness unit of Ant Group. Between 2007 and 2014,
he was a senior researcher with Sony China
Research Lab, and led multiple projects on image
recognition for Sony’s digital camera/TV/Playsta-
tion product line. He joined Ant Group, in 2014 and
is now director of the biometrics technology
research group, in charge of identity authentication
for various ﬁnancial applications. His research interests include biometrics,
computer vision, and pattern recognition.
Dongrui Wu (Senior Member, IEEE) received the
BE degree in automatic control from the University
of Science and Technology of China, in 2003, the
ME degree in electrical engineering from the
National University of Singapore, in 2005, and the
PhD degree in electrical engineering from the Uni-
versity of Southern California, in 2009. He is now a
professor with the School of Artiﬁcial Intelligence
and Automation, Huazhong University of Science
and Technology, Wuhan, China, and deputy direc-
tor of the Key Laboratory of Image Processing and
Intelligent Control, Ministry of Education. His research interests include
affective computing, brain-computer interface, computational intelligence,
and machine learning. He has more than 190 publications, including more
than 60 IEEE Transactions papers. He received the IEEE Computational
Intelligence Society Outstanding PhD Dissertation Award, in 2012, the
IEEE Transactions on Fuzzy Systems Outstanding Paper Award, in 2014,
the NAFIPS Early Career Award, in 2014, the IEEE Systems, Man and
Cybernetics (SMC) Society Early Career Award, in 2017, the USERN
Prize in Formal Sciences, in 2020, the IEEE Transactions on Neural Sys-
tems and Rehabilitation Engineering Best Paper Award, in 2021, and the
Chinese Association of Automation Early Career Award, in 2021. His
team won the First Prize in China BCI Competition in three successive
years (2019-2021). He is a BoG member and associate vice president for
Human-Machine Systems of the IEEE SMC Society, and editor-in-chief of
itseNewsLetter . He will be the editor-in-chief of the IEEE Transactions on
Fuzzy Systems , in 2023.
"For more information on this or any other computing topic,
please visit our Digital Library at www.computer .org/csdl.XU ET AL.: INCONSISTENCY-BASED MULTI-TASK COOPERATIVE LEARNING FOR EMOTION RECOGNITION 2027
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:24 UTC from IEEE Xplore.  Restrictions apply. A Multimodal Approach for Mania Level
Prediction in Bipolar Disorder
Pınar Baki, Heysem Kaya ,Member, IEEE , Elvan C ¸ iftc¸i,
H€useyin G €ulec¸, and Albert Ali Salah ,Senior Member, IEEE
Abstract— Bipolar disorder is a mental health disorder that causes mood swings that range from depression to mania. Clinical diagnosis
of bipolar disorder is based on patient interviews and reports obtained from the relatives of the patients. Subsequently , the diagnosis
depends on the experience of the expert, and there is co-morbidity with other mental disorders. Automated processing in the diagnosis
of bipolar disorder can help providing quantitative indicators, and allow easier observations of the patients for longer periods. In this paper,
we create a multimodal decision system for three level mania classiﬁcation based on recordings of the patients in acoustic, linguistic, and
visual modalities. The system is evaluated on the Turkish Bipolar Disorder corpus we have recently introduced to the scientiﬁc community .
Comprehensive analysis of unimodal and multimodal systems, as well as fusion techniques, are performed. Using acoustic, linguistic, and
visual features in a multimodal fusion system, we achieved a 64.8% unweighted average recall score, which advances the state-of-the-art
performance on this dataset.
Index Terms— Affective disorders, bipolar disorder , multimodal fusion, mania level prediction
Ç
1I NTRODUCTION
ASSESSMENT of mental health disorders from behavioral
data using machine learning methods is a recently grow-
ing research area, with focused work including depression [1],
anxiety disorders [2] and bipolar disorder [3]. Unobtrusive
affective assessment makes it possible to observe multimodal
responses during structured or semi-structured observation
sessions, to derive indicators and deviations from behavior,
or to observe subtle changes over time [3], [4]. While, fully
automated diagnosis requires the integration of a comprehen-
sive set of indicators and detailed patient history, automatic
analysis of behavior can provide clinicians with useful quanti-
tative measurement and monitoring tools [5].
Bipolar disorder (BD) is a mental health condition that
causes extreme mood swings from elevated (mania, hypo-
mania) to diminished state (depression), as well as mixed
episodes, where depression and manic symptoms occur
together. Its diagnosis is performed through a set of medicalexaminations administered by the psychiatrist, but may
require lengthy observations of the patient as there is no com-
prehensive test [6]. There is a lot of co-morbidity with other
mental disorders including, but not limited to, any anxiety
disorder, conduct disorder, and substance use disorder [6].
The disease affects 2% of the population, sub-threshold forms
(recurrent hypomania episodes without major depressive
episodes) affect an additional 2%, and together, the lifetime
prevalence estimates are 4.4% [7]. It is ranked as one of the
top ten diseases of disability-adjusted life year indicator
among young adults [8], and as the 17th leading source of dis-
ability among all diseases worldwide [9].
Diagnosis of mental health disorders rely on medical
examinations administered by psychiatrists and reports
from patients and their relatives or friends. But there is a
need for more systematic and objective diagnosis methods,
for remote treatment and diagnosis approaches assisted
using automated methods. It is possible to collect behavioral
data from people during their everyday lives [10], which
creates an opportunity to create tools to monitor the symp-
toms of the patients for longer periods, screen patients
before they see the psychiatrists, assist clinicians in the diag-
nosis, and capture patient behaviors in situations where
they cannot act or hide the symptoms.
Different types of bipolar disorder are characterized by
changes in the patient’s mood, energy, and activity levels.
The patient experiences periods of intense emotion and
uncharacteristic behaviors, called mood episodes , which can
be manic (high arousal and valence) or depressive (low
arousal and valence). Manic episodes , the focus of this paper,
include elated, erratic, charged behaviors. While a loss of
appetite or decreased need of sleep is difﬁcult to judge auto-
matically from multimedia recordings, traces of elation and
irritability, fast and incoherent thought, feelings of grandeur
and recklessness can be gleaned from affective language/C15Pınar Baki is with the Department of Computer Engineering, Bo /C21gazic¸i
University, Istanbul 34342, Turkey. E-mail: pinarbaki95@gmail.com.
/C15Heysem Kaya is with the Department of Information and Computer Scien-
ces, Utrecht University, 3584 Utrecht, Netherlands. E-mail: h.kaya@uu.nl.
/C15Elvan C ¸ iftc¸i is with the NP Brain Hospital and €Usk€udar University, Istan-
bul 34768, Turkey. E-mail: elvanlciftci@gmail.com.
/C15H€useyin G €ulec¸ is with the Psychiatric and Neurological Diseases Training
and Research Hospital, Health Science University, Istanbul 34668, Turkey.
E-mail: huseyingulec@yahoo.com.
/C15Albert Ali Salah is with the Department of Computer Engineering,
Bo/C21gazic¸i University, Istanbul 34342, Turkey, and also with the Department
of Information and Computer Sciences, Utrecht University, 3584 Utrecht,
Netherlands. E-mail: a.a.salah@uu.nl.
Manuscript received 3 May 2021; revised 13 June 2022; accepted 6 July 2022.
Date of publication 21 July 2022; date of current version 15 November 2022.
(Corresponding author: Heysem Kaya.)
Recommended for acceptance by A. Dhall.
Digital Object Identiﬁer no. 10.1109/TAFFC.2022.3193054IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022 2119
1949-3045 ©2022 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See ht _tps://www.ieee.org/publications/rights/index.html for more information.
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:26 UTC from IEEE Xplore.  Restrictions apply. and behavioral cues. Hypomania is a less severe form of
mania, and remission is the period when the behavior is
returning to normal. Patients admitted to the hospital with
manic episodes are medicated, closely followed, and dis-
charged only after entering the remission stage. In this
paper, we work with data collected from such patients
encompassing manic, hypomanic, and remission stages.
A gold standard tool used to rate the severity of the
manic episodes of a patient is the Young Mania Rating Scale
(YMRS) [11] (see Section. 4.4.2). During the interviews, psy-
chiatrists observe and rate the patient’s symptoms via
eleven indicators. Using a structured interview, it is possible
to observe some of these from speech patterns, body or
facial movements, and from the content of what was spoken
during the interview.
In this work, we propose a multimodal machine learning
system that uses information from acoustic, linguistic, and
visual modalities to classify the bipolar patients into remis-
sion, mania and hypomania classes. Our aim is to investi-
gate to what extent automatic analysis approaches can
provide the psychiatrists with quantitative indicators to
help in their diagnosis. Despite recently increasing inter-
est [12], [13], there are very limited publicly available
resources in this area. We evaluate our proposed multi-
modal approach using the Turkish Audio-Visual Bipolar
Disorder corpus that we have recently collected and made
available to the research community [3], [14], and push the
state-of-the-art performance achieved on the corpus so far.
We discuss our results extensively in the light of our quanti-
tative ﬁndings, provide insights and point out to challenges
in this problem.
The rest of the paper is organized as follows. Section 2
discusses the previous work on bipolar disorder and related
mental conditions, including Section 2.2 on the Turkish
Audio-Visual Bipolar Disorder corpus used in our study.
Section 3 explains the features used for each modality, the
preprocessing methods, classiﬁcation algorithms, and the
modality fusion approach used in our study. Section 4
presents the results for uni- and multimodal experiments.
We discuss our ﬁndings in Section 5 and provide some ﬁnal
remarks.
2R ELATED WORK
In this section, we ﬁrst brieﬂy summarize the main ﬁndings
in the related area of multimodal depression analysis. Then
we describe our dataset, before moving to a more technical
exposition of speciﬁc works on BD estimation.
2.1 Depression Analysis
Research on depression analysis has shown that multimodal
fusion of features in various levels increase the performance
of single modalities [4], [15]. Fusion of textual, acoustic and
visual features extracted from the clinical patient interviews
outperforms unimodal models [15], [16], [17]. Recently,
using a feature selection framework, F0, HNR, formants, and
MFCC for the speech, and left-right eye movement, gaze
direction and yaw head movement for visual modality are
shown to be the most distinctive features for depression anal-
ysis [1]. These are in line with the former research showing
that stillness of eyes [18] and low acoustic variability areimportant indicators of depression [4]. Additionally, lexical
content of what people say during interviews is also useful
in the detection of depression [19], [20]. Using only audio
and textual information in a multimodal system is useful
when there is no visual data, such as during phone call con-
versations [15], [21].
While low energy and acoustic/visual variability are
indicators of a higher level of depression, BD patients show
an inverse pattern during mania episodes. Higher bodily
and acoustic energy, higher variability and lack of focus in
the spoken content are correlated with mania levels [6].
2.2 The Turkish Audio-Visual Bipolar Disorder (BD)
Corpus
In this paper, we use the Turkish Audio-Visual Bipolar Dis-
order Corpus [14]1to report experimental results. Before
discussing the related work performed on this corpus, we
provide some details about the data. In our experiments, we
have adhered to the 2018 AVEC Bipolar Disorder and
Cross-cultural Affect Recognition Competition [3] protocol
to ensure comparability of results with the literature. The
aim of the AVEC competition series is developing and com-
paring machine learning models using audio and visual
components on various affective computing problems. Par-
ticipants were encouraged to achieve the highest perfor-
mance, considering the baseline performance provided by
the organizers. The BD corpus was used in the 2018 AVEC
challenge for the ﬁrst time, and only a part of it was opened
for the challenge.
The original BD corpus contains video clips of 46 bipolar
disorder patients and 49 healthy controls collected at the
Istanbul Health Sciences University, Erenkoy Mental Health
Research and Training Hospital2. Mania level of the patients
is evaluated on 0th, 3rd, 7th and 28th days of the hospitaliza-
tion and after discharge on the 3rd month. On those days,
psychiatrists performed an interview with the patients, ask-
ing the same questions each time, and taking audiovisual
recordings of the sessions. Annotation was done based on
the Young Mania Rating Scale (YMRS) score [11], which is a
continuous clinical interview assessment scale used for rat-
ing the severity of manic episodes of a patient. Scores range
from 0 to 60, where higher scores represent severe mania. In
the BD corpus, bipolar patients are grouped into three ordi-
nal classes (remission, hypomania, and mania, respectively)
based on their session-wise YMRS score, as described in [14].
During recordings, patients were asked to perform
seven tasks, designed to arouse different emotions in the
patients. The ﬁrst three tasks can be considered as negative
emotion eliciting tasks, the subsequent two tasks are neu-
tral, and the two ﬁnal tasks are positive emotion eliciting
tasks. The performed tasks are 1) explaining the reason for
coming to the hospital, 2) describing van Gogh’s Depres-
sion painting, 3) describing a sad memory, 4) counting
from one to thirty, 5) counting from one to thirty faster, 6)
describing Dengel’s Home Sweet Home painting and 7)
describing a happy memory. The paintings used in the
study are shown in Fig. 1.
1.Corpus website: https://sites.google.com/view/tavbd/home
2.Please see [14] for patient sociodemographics, clinical characteris-
tics, and exclusion criteria.2120 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:26 UTC from IEEE Xplore.  Restrictions apply. Clips were recorded in a room where only the participant
and the psychiatrist were present. The participants were
recorded with a camera while performing tasks. They read
the descriptions of the tasks they were asked to perform
from the computer screen. After completing a task, the par-
ticipants pushed a button and a description of the next task
appeared on the screen, while a ‘knock’ sound was played
to mark the beginning of a new task. This sound helps to
split tasks if one wants to use the tasks separately for classi-
ﬁcation. Our preliminary experiments have shown that
task-based analysis results in too small data partitions for
training, and does not result in higher overall accuracy [22],
since some negative-emotion eliciting tasks are skipped by
a number of patients.
In the AVEC 2018 Challenge, only data from the bipolar
patients are used for a three-class (R: remission, H: hypoma-
nia, M: mania) classiﬁcation. The healthy controls have visual
properties that may help in their identiﬁcation (e.g., clothing
colors for doctors), and subsequently, they are not used in the
AVEC Challenge or in this paper. In the competition, there
were 104 (R: 25, H: 38, M:41), 60 (R: 18, H: 21, M:21), and 54 (18
each) clips in the training, development, and test sets, respec-
tively. As it is the case with other mental-healthcare datasets,
the number of session-wise annotated samples is small, which
may lead to overﬁtting, and here is a mild data imbalance that
can cause bias in favor of the majority class.
2.3 Multimodal Supervised Learning for Mania
Estimation
The ﬁrst comprehensive set of investigations into the exten-
sion of multimodal methods to the analysis of BD started with
the 2018 Audio/Visual Emotion Challenge (AVEC) [3], which
introduced the Turkish BD corpus described in the previous
section to the larger affective computing community in form
of a challenge. Several groups have worked on this corpuswithin the AVEC Challenge [23], [24], [25], [26], [27], [28], [31].
Table 1 summarizes the major works reporting results on the
BD corpus to date. In this section, we summarize the feature
extraction and machine learning approaches that were used
for the mania level estimation problem. We caution the reader
that the reported accuracies in these works (including the
present paper) are not clinical results, but a good indication of
the possibilities of automatic analysis approaches.
As the classiﬁcation of manic episodes is correlated with
increased arousal levels, audio-visual detection of arousal is
a good place to start. In [23] arousal-related features
extracted from speech and from visual upper body motion
of patients were fused. Another important source of infor-
mation is the dynamics of affective cues. Syed et al. [25] pro-
posed to use turbulence features that represent the sudden
changes in feature contours of both audio and visual modal-
ities. In the extraction of audio features, they used a Fisher
vector encoding with a feature set extracted via the open-
SMILE tool [32]. They have used a standard feature set
introduced for the Interspeech Computational Paralinguis-
tics Challenge (ComParE). Other groups (e.g., [24]) have
used the extended Geneva Minimalistic Acoustic Parameter
Set (eGeMAPS) for acoustic feature extraction [33].
In [25], the classiﬁcation is performed using the Greedy
Ensemble of Weighted ELMs model [34]. Because of the
small number of samples, deep learning is not suitable for
end-to-end classiﬁcation, but transfer learning can be
adopted for feature extraction. Using highly complex classi-
ﬁers results in poor generalization due to the limits of the
training set. In [24], Xing et al. used linguistic features in
addition to visual and audio based features, and created
5,395 dimensional features by the early fusion of these three
modalities. Using eGeMAPS features, Mel frequency ceps-
trum coefﬁcients (MFCC), facial action units, and gaze fea-
tures, they achieved the highest Unweighted Average
Recall (UAR) on the validation set among the AVEC Chal-
lenge participants. However, the great difference between
the UAR scores on the development and test sets (i.e., 86.7%
versus 57.4%) shows that the proposed model cannot gener-
alize well to the sequestered test set data. In [31], an Incep-
tion module was combined with an LSTM network, and L 1
regularization to deal with overlearning. 16-dimensional
MFCC features are extracted from the speech ﬁles. Using
only audio features, 65.1% UAR is achieved on the valida-
tion set. However, no score was reported for the test set.
In [26], LSTM and Bi-LSTM models were trained on the
challenge baseline features including MFCCs, eGeMAPS,
Fig. 1. Van Gogh’s Depression (left), Dengel’s Home Sweet Home
(right).
TABLE 1
Summary of the Works That Use BD Dataset
Paper Modalities Features Classiﬁer
Ringeval et al. [3] (baseline) A,V eGeMAPS+FAUs SVMs
Yang et al. [23] A,V Arousal and upper body posture features Multistream
Xing et al. [24] A,V,T eGeMAPS+MFCC+Timing+FAUs+Emotion+Eyesight+Body movement Hierarchical recall model
+features from various NLP tools including SiNLP
Syed et al. [25] A,V FAUs+gaze+pose GEWELMs
Ebrahim et al. [26] A,V MFCC+eGeMAPS+BoAW+DeepSpectrum+FAUs+BoVW Bi-LSTM
Amiriparian et al. [27] A Mel-Spectogram CapsNet
Ren et al. [28] A MFCC Multi-instance learning
AbaeiKoupaei, Al Osman [29] V Facial Features LSTM
AbaeiKoupaei, Al Osman [30] A,T MFCC+eGeMAPS+SiNLP+SEANCE Stacked Ensemble ModelBAKI ET AL.: MULTIMODAL APPROACH FOR MANIA LEVEL PREDICTION IN BIPOLAR DISORDER 2121
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:26 UTC from IEEE Xplore.  Restrictions apply. Bag-of-Acoustic-Words [35], DeepSpectrum3, Facial Action
Units (FAU) and Bag-of-Visual-Words (BoVW). Their best
result on the test set was achieved with the Bi-LSTM net-
work trained on the concatenation of all the features.
AbaeiKoupei and Al Osman reach the test set baseline by
only using visual features extracted from a pre-trained and
ﬁne-tuned deep neural network model [29], achieving 60.6%
and 57.4% UAR on development and test sets, respectively,
which shows that the model does not overﬁt. Capsule Neural
Networks (CapsNet) [36] were used in [27], on Mel-frequency
spectrograms extracted from small segments of raw audio
ﬁles. In [28], audio clips were segmented into chunks to
increase the dataset size. However, each clip has only one
label and after segmenting the clip, each chunk becomes
weakly labeled. This problem was solved using multi-
instance learning, where training was performed with a bag
of instances, instead of one single feature vector. Using ensem-
bles of DNNs, 61.6% UAR on the development, and 57.4%
UAR on the test set was achieved using the audio modality.
The works mentioned so far used audio and video fea-
tures, but the text transcriptions of the speech of the patients
during the tasks are also informative, particularly in a mul-
timodal context. For instance, when a patient describes a
sad memory, e.g. the death of a loved one, in a cheerful
voice, this presents a strong case for elevated mania levels.
Zhang et al. proposed ﬁxed length, session-level paragraph-
vector representations for the text modality [37]. They
showed that early fusion on audio-visual and textual repre-
sentation vectors was beneﬁcial.
The highest test set score achieved on the BD corpus so far
was 59.3% UAR [30], using eGeMAPS and MFCC acoustic
features, as well as linguistic features, such as the number of
words, number of types, letters per word, number of para-
graphs, number of sentences, and number of words per sen-
tence. Additionally, sentiment information was extracted
using the SEANCE tool [38]. In the next section, we present a
tri-modal system that advances the state of the art in this
problem.
3M ETHODOLOGY
Fig. 2 illustrates the proposed multimodal framework for
the classiﬁcation of the patients into one of remission, hypo-
mania, and mania classes, based on a short video interviewwhere the patient performs several tasks, as described in Sec-
tion 2.2. The components of the pipeline are summarized in
the following subsections. Besides leveraging multimodality,
our goal was to reach a minimal set of modalities and fea-
tures that provide the highest predictive performance. Thus,
we conducted comparative unimodal experiments to choose
the optimal sets of features in each modality.
3.1 Feature Extraction
3.1.1 Acoustic Feature Extraction
Acoustic cues are used for the diagnosis of bipolar disorder
by psychiatrists. Rapid, pressured speech and speaking too
much (amount of speech) are common indicators of manic
episodes. Subsequently, we perform acoustic analysis of the
patient interviews.
For acoustic feature extraction, we use the openSMILE fea-
ture extraction toolkit [32], which provides many built-in con-
ﬁguration ﬁles that extract the baseline audio features from
INTERSPEECH and AVEC challenges, and some parameter
sets proposed for voice research and affective computing
studies on audio. In our experiments, we use eGeMAPS [33]
(Extended Geneva Minimalistic Acoustic Parameters Set),
which is a parsimonious set of audio features, chosen for their
ability to represent affective physiological changes in voice
production. 23 eGeMAPS low level descriptors (LLD) are
summarized using the functionals from the original eGe-
MAPS conﬁguration [33], and this set is enriched by 10 func-
tionals we have added [14] (see Table 2 for the entire set).
Additionally, the baseline acoustic feature set from
INTERSPEECH 2010 Paralinguistic Challenge (IS10) [39] is
used, with 38 low-level descriptors and their temporal
derivatives, indicating paralinguistic activity.
3.1.2 Linguistic Feature Extraction
Clinicians assess the presence of risk of suicide, risk of vio-
lence to persons or property, risk-taking behavior, sexually
inappropriate behavior, substance abuse, patient’s ability to
care for himself/herself, etc., using the patient interview
contents [40].
We use the Google Automatic Speech Recognition (ASR)
tool4to convert the interviews to text, and obtain one text
segment per task for each interview.
Fig. 2. Pipeline of the multimodal system. For each unimodal system, different feature sets are used. The best performing feature sets are combined.
3.https://github.com/DeepSpectrum/DeepSpectrum 4.https://cloud.google.com/speech-to-text2122 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:26 UTC from IEEE Xplore.  Restrictions apply. Transformer language embeddings (GPT-2 [41], BERT
[42], GPT-3 [43]) are the state-of-the-art natural language
processing (NLP) models in representing language features.
However, these complex models show unreliable results on
small datasets. Consequently, we use three alternative fea-
ture sets for the linguistic experiments, which are linguistic
inquiry and word count (LIWC) [44], term frequency-inverse
document frequency (TF-IDF), and polarity features.
LIWC is a text analysis tool that calculates the linguistic
or psychological categories of words where the categories
indicate social, cognitive, and affective processes. It extracts
93 features for an input text ﬁle. Before using LIWC, the
Turkish transcripts extracted from the patient clips are
translated into English via Google Translation engine5.
TF-IDF is a statistical measure that shows how much a
word is important in a document. They are used commonly
in NLP, information retrieval, and text mining tasks. As a
preprocessing step, stop words are removed using the
NLTK library [45], and stemming is applied using the Porter
algorithm [46]. After these steps, TF-IDF features are com-
puted over the set of uni-grams and bi-grams.
Aspolarity f e a t u r e s ,w eu s et h eo u t p u t so ft h r e es e n t i m e n t
analysis tools together, whic h are Natural Language Toolkit
Valence Aware Dictionary fo r sEntiment Reasoning (NLTK
Vader) [47], TextBlob [48] and Flair [49] due to their comple-
mentary information and generalization performance on a
recent mood recognition challenge [50]. NLTK Vader uses a
sentiment lexicon together with grammatical rules for express-
ing polarity, but performs weakly on unseen words. Flair uses
a character-level LSTM networ k for sentiment analysis, with
good generalization to unseen words. The TextBlob library
returns a sentiment with polarit y and subjectivity scores, where
subjectivity represents the amount of personal and factual
information in the sentence, which is a good feature for the
valence dimension. However, it does not consider negation in
sentences for the polarity score, which can be misleading.
Sentiment and subjectivity features obtained from these
three libraries are combined into a feature vector. Then each
feature is summarized with mean, standard deviation, max,
min, and sum functions.3.1.3 Visual Feature Extraction
Clinicians gain signiﬁcant insight from visual cues and
some items of YMRS can be obtained from visual cues like
increased motor activity-energy, irritability, elevated mood,
appearance, and disruptive-aggressive behavior. Besides,
the speech rate and the amount can also be observed in the
facial actions.
For the visual experiments, we use facial action units
(FAUs), as well as geometric features extracted from each
face, provided as baseline features in the AVEC chal-
lenge [3]. The FAUs are based on the Facial Action Coding
System (FACS), which describes the movements of speciﬁc
facial muscles [51]. Emotional expressions typically corre-
spond to combinations of various action units. In [3], inten-
sities of 16 FAUs along with a conﬁdence score are
extracted using the OpenFace toolkit [52].
The set of 23 geometric features we use are based on our
early work for video-based emotion recognition in uncon-
trolled conditions. They are extracted from detected and
aligned faces of the BD corpus and represent different geo-
metric aspects like distance, angle, and aspect ratio based
on facial landmarks (see [53] for a full list).
3.2 Preprocessing
The feature vectors extracted for each clip contain represen-
tations of auditory, visual, and textual signals with different
ranges and scales. Subsequently, feature standardization or
normalization needs to be performed before model training.
The features we used for the classiﬁcation of the clips are
represented as two-dimensional matrices, where columns
are the functionals of the low level descriptors and each
row contains the feature vector of a clip. We experiment
with both row-level and column-level normalization. For
the column-level, standardization (z-normalization) brings
each feature to the same scale. For the row-level, we apply
L2normalization, which effectively transforms a linear ker-
nel into a cosine similarity kernel.
3.3 Feature Selection
We use high-dimensional feature sets in our experiments.
Considering the sample size of the BD dataset, we consid-
ered reducing the feature dimensionality with feature selec-
tion. We tested the tree feature selection method [54], which is
expected to be robust against overﬁtting. In this approach, a
random forest [55] is trained, and features are ranked based
on the information gain for each feature. We report experi-
mental results with this approach, even though ultimately,
it did not yield improved test set results.
3.4 Classiﬁcation Using Extreme Learning
Machines
Due to the difﬁculty of collecting data from bipolar patients,
the BD dataset has only 164 data points in total, and it is cru-
cial to pay attention to getting accurate predictions while
avoiding overﬁtting. The performance measure used here is
Unweighted Average Recall (UAR), which is the average of
recall performances across the classes (the same measure as
in the AVEC 2018 challenge [3]). This measure has a chance
level performance of 1=KforK-class classiﬁcation.TABLE 2
List of Statistical Functionals Applied to LLDs
Functional Description
Mean Arithmetic mean
Std Standard deviation
Curvature Leading coefﬁcient of the 2ndorder
polynomial ﬁt to LLD contour
Slope + offset Coeff. of the 1storder polynomial ﬁt to LLD
contour
Min Minimum value
Relative Min
LocationLocation index of min value divided by the
length of LLD contour
Max Maximum value
Relative Max
LocationLocation index of max value divided by the
length of LLD contour
ZCR Zero crossing rate of LLD contour
normalized to [-1,1]
5.https://cloud.google.com/translateBAKI ET AL.: MULTIMODAL APPROACH FOR MANIA LEVEL PREDICTION IN BIPOLAR DISORDER 2123
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:26 UTC from IEEE Xplore.  Restrictions apply. In our experiments, we employ a Kernel Extreme Learn-
ing Machine (Kernel ELM) classifer [56]. The initially intro-
duced Basic ELM is a simple and robust machine learning
method that contains a single hidden layer. Input weights
are randomly generated and they are not tuned. The
weights between the hidden layer and the output layer are
analytically calculated by a pseudo inverse operation. While
multi-layered deep learning approaches are used in many
problems, on such a small dataset they will easily overﬁt,
and simpler models should be preferred.
In a single hidden layer ELM, the hidden layer output
matrix is H2RN/C2h, the weight matrix between the hidden
layer and the output layer is b2Rh/C21and the output layer
matrix is T2RN/C21, where Nis the number of training sam-
ples and his the number of hidden layer nodes. The output
weight matrix bis calculated using least squares solution of
Hb¼Tasb¼H{T.H{represents the Moore-Penrose gen-
eralized inverse [57], which minimizes L2norms of both
kHb/C0Tkandkbk. For increased generalization and robust-
ness, the kernel trick and a regularization coefﬁcient Cis
used. The set of weights is calculated as:
b¼I
CþK/C18/C19/C01
T; (1)
where Iis an identity matrix, and Kis a kernel (i.e., similar-
ity matrix) obtained from the training dataset. We use a
radial basis function (RBF) kernel K, as suggested in [58].
In weighted ELM [34], which is a variant that is used
with class imbalanced problems, we deﬁne a N/C2Ndiago-
nal weight matrix W. Each diagonal element stores the mul-
tiplicative inverse of the number of training samples Niof
the corresponding class i. Integrating Winto the formula, b
is calculated as:
b¼I
CþWK/C18/C19/C01
WT: (2)
There is a trade-off between weighted and unweighted
models, where the former favors minority classes (better
UAR), while the latter favors majority classes (better accu-
racy). To reach the best performance, we investigate a
weighted decision level fusion approach:
Pfusion ¼aPunweighted þð1/C0aÞPweighted ; (3)
where Pis an N/C2tmatrix that contains the class probabili-
ties of each sample. ais a coefﬁcient in [0, 1] range. The best
ais chosen according to the UAR score of Pfusion on the
development set.
3.5 Modality Fusion
The expert assesses patient’s speech patterns (e.g., rate,
amount of speech), visual appearance, gestures, motor
activity, as well as expressed sentiments and ideas, and the
content of speech during the interviews. All of these indica-
tors are informative, and instrumental in deciding the
patient’s YMRS score and to diagnose BD episodes. We
investigate how an automatic system can best extract indica-
tors from each modality, and how they complement each
other by providing context or including more discrimina-
tive information.We ﬁrstly experiment with audio, speech, and text modali-
ties separately. Our experiments show that the audio modal-
ity gives a better score overall, while the hypomania class is
not classiﬁed well. The linguistic modality generally gives
lower UAR than audio modality, but all three levels of mania
are classiﬁed with similar performance. For the multimodal
systems, we use the best performing unimodal models and
features.
First, we consider two late fusion methods, namely,
majority voting and weighted sum. Majority voting outputs
the mostly seen label (mode) for a sample, and uses the
audio modality for tie breaking. Weighted sum combines
two or three modalities. When fusing two modalities, the
probability vectors from each model are given as input and
the ﬁnal probabilities are obtained as:
Pfusion ¼aPmodel 1þð1/C0aÞPmodel 2; (4)
where Pis anN/C2tmatrix that contains the class probabilities
of each sample, Nis the number of samples and tis the num-
b e ro fc l a s s e s . ais a coefﬁcient in the [0, 1] range, optimized
according to the UAR score obtained from Pfusion . For the
fusion of three modalities, we apply a variant of Equation (4).
Pfusion ¼XK
k¼1akPmodel k(5)
In Equation (5), the alpha values are the elements of the
vector drawn from a Dirichlet distribution. We sample 500
times randomly and ﬁnd the values that maximize the
UAR of the ﬁnal fusion model. A probability density
function of a Dirichlet distribution of order N52with
parameters a1; :::;an>0is
1
BðaÞYN
i¼1xðai/C01Þ
i; (6)
where BðaÞis a normalizing factor given in terms of multi-
variate beta function, and xi2ð0;1ÞandPN
i¼1xi¼1.
We also experiment with early (feature level) fusion meth-
ods. In our approach, the features from different modalities
are summarized via LLDs, concatenated and normalized
into a single feature vector before the classiﬁcation.
We evaluated only a small number of models on the test
set to prevent overﬁtting. While selecting the fusion models
to test, we considered the Multimodal 1 (MM1) metric [59],
which measures the improvement in the ﬁnal fusion model:
MM 1¼UAR fusion /C0maxðUAR 1; UAR 2; UAR 3Þ
maxðUAR 1; UAR 2; UAR 3Þ; (7)
where UAR fusion is the UAR score of the fusion model, UAR i
are the UAR scores of the unimodal models. While calculat-
ing the MM1 score, we use 4-fold cross-validation scores,
since it gives more robust results. After getting the test set
results for the selected fusion models, we calculate MM1
scores using test set UARs.
3.6 Shapley Additive Explanations
To further investigate the contribution of features for classi-
ﬁcation, we use the SHAP method [60], which aims to pro-
vide an explanation to a particular prediction by means of2124 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:26 UTC from IEEE Xplore.  Restrictions apply. additive feature attributions based on cooperative game the-
ory [61]. Given a model fand input feature vector xwith D
features, SHAP assigns each feature ian importance weight
fiðf; xÞmeasuring its marginal contribution to the model out-
put, where marginal contribution is calculated as a (weighted)
average over a range of ‘observed’ feature subsets:
fiðf; xÞ¼X
z0/C18x0jz0jðD/C0jz0j/C01Þ!
D!ðfxðz0Þ/C0fxðz0niÞÞ; (8)
where x02f0;1gDis the simpliﬁed feature representation, jz0j
is the number of non-zero entries in z0andz0/C18x0represents
allz0vectors where the non-zero entries are a subset of the
non-zero entries in x0[60]. Observing means that a subset of
features are seen by the model with their original values,
while others are missing (represented with a 0 in z0). Since not
all models are inherently capable of handling missing values,
the authors propose an approximation to missing values,
assuming model linearity and feature independence [60].
4E XPERIMENTS AND RESULTS
We report both unimodal and multimodal experimental
results in this section. In all tables, we denote tree-based fea-
ture selection with a star.
4.1 Unimodal Experiments
4.1.1 Audio Classiﬁcation
For the clip level audio classiﬁcation, we used eGeMAPS
and IS10 feature sets, which are extracted using the open-
SMILE feature extraction toolkit [32]. The LLD features are
summarized using 10 functionals proposed in [14]. We also
used original eGeMAPS feature set [33]. Z-normalization is
applied to each feature separately. After that, L 2normaliza-
tion is applied to the feature vector of each clip. The deci-
sion level fusion of weighted and unweighted RBF Kernel
ELMs is used for the classiﬁcation. In this setup, the best
result is achieved on eGeMAPS10 features with 63.7% UAR
on the development set (see Table 3).
4.1.2 Text Classiﬁcation
Text level BD classiﬁcation is performed with the same con-
ﬁguration used in the audio experiments. Since better fea-
ture extraction tools are available for the English language
compared Turkish [50], we use Google Translation engine
to process the text in English. We experiment with LIWC,
TF-IDF and polarity features (see Section 3.1.2) for the text
classiﬁcation.
Table 3 shows the results on the text features obtained
from the entire clip, with late fusion of weighted and
unweighted Kernel ELMs. LIWC features give the best
results for both development set and the cross-validation
experiment, with 53.7% and 57.7% UAR, respectively.
4.1.3 Video Classiﬁcation
For the visual experiments, FAUs, geometric features, and
appearance descriptors, obtained from the pre-classiﬁcation
layer of a convolutional neural network (VGG), are used
(see Section 3.1.3). Table 3 shows the best results achieved
on the visual modality. All feature sets are normalized usingZ- and L 2normalization, as explained in Section 3.2. The
4,096-dimensional VGG features are extracted from the pre-
trained network, then summarised with mean and standard
deviation functionals, which creates an 8,192-dimensional
feature vector for each clip. We then reduce the dimension-
ality using PCA (retaining 99% variance) to 49 dimensions,
and apply tree-based feature selection. The ExtraTreeClassi-
ﬁer method from the scikit-learn library [62] is used for fea-
ture selection. For the VGG feature set, using only PCA
gives the best result. The fourth row in the Visual part of
Table 3 shows the results obtained with a 49-dimensional
feature vector after applying PCA to the VGG feature vec-
tor. On the development set, the best result (57.1% UAR) is
achieved using geometric features summarized using the
mean functional. Using 4-fold cross-validation (4F-CV),
60.7% UAR is achieved on geometric features summarized
with mean and standard deviation.
4.2 Fusion of Modalities
After investigating unimod al performances, we perform
multimodal fusion experi ments using weighted sum,
majority voting, and feature fusion methods as explained
in Section 3.5. Mainly, we select the feature sets that per-
formed well in the unimodal experiments and use them
in multimodal fusion. For the acoustic modality, we
select eGeMAPS10 and eGeMAPS feature sets, since the
eGeMAPS feature set is created speciﬁcally for the
affective paralinguistic tasks, and may provide better
intelligibility compared to the IS10 feature set. For the
linguistic modality, LIWC features and for the visual
modality, FAU and geometric features are used for the
fusion experiments.
Many previous works on this dataset used a validation
set to optimize their models, but such optimization did not
correlate well with the results obtained on the test set, and
often, these models did not perform better than the baseline
test set performance. We use 4F-CV and MM1 scores to
select the fusion systems that will be used for the limited
test set probes.
Ranking the fusion system sw i t hr e s p e c tt o4 F - C V
UAR performance, the top systems are observed to use
majority voting, which shows the effectiveness of this
approach. Feature fusion method is not as successful as
majority voting in improving the unimodal performan-
ces, since after concatenating the feature sets, the newlyTABLE 3
UAR Scores for Single Modality Experiments
Modality Features Dimension Dev. 4F-CV
IS10 760 55.2% 56.8%
AudioeGeMAPS10 230 63.7% 53.1%
eGeMAPS10* 98 60.8% 52.2%
eGeMAPS 88 52.9% 53.8%
LIWC 93 53.7% 57.3 %
TextTF-IDF 500 Bigram 500 49.4% 57.3%
Polarity 35 48.9% 42.5%
GEO - Mean 23 57.1% 59.2%
VisualGEO - Mean, Std. 46 55.8% 60.7%
FAU - Mean, Std. 32 55.8% 56.0%
VGG - Mean, Std. 49 41.2% 52.2%BAKI ET AL.: MULTIMODAL APPROACH FOR MANIA LEVEL PREDICTION IN BIPOLAR DISORDER 2125
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:26 UTC from IEEE Xplore.  Restrictions apply. generated feature vector ha s a higher dimension, which
requires more data for a robust training [63]. Regarding
the visual modality, the FAU features are observed to
contribute more in the multimodal setup, but the geo-
metric features perform better in the unimodal system.
The best 4F-CV UAR score (65.8%) is achieved using eGe-
MAPS10 with tree feature selection, LIWC, and FAU fea-
tures fused with the majority voting method. The MM1
score shows that fusion of the modalities increases the maxi-
mum unimodal performance by 15%, which is the highest
MM1 score achieved on the 4F-CV results as well.
The ﬁnal test set experiments are done using the top per-
forming three multimodal fusion systems for a fair compari-
son with the literature. We also limit the test set probes to
10, including the unimodal constituents. We also obtain the
test set results of the constituent unimodal models (i.e., eGe-
MAPS10, eGeMAPS10 with tree feature selection, eGe-
MAPS, LIWC, and FAU) in order to report their MM1
scores on the test set.
Table 4 shows the test set results for the top three multi-
modal systems. The best test set result is achieved using the
eGeMAPS10, LIWC and FAU feature sets with the majority
voting method. With this setup, we achieve 64.8% UAR
score, which is 5.5% (absolute) higher than the best result
published on this challenging corpus (see Table 6).
We further analyzed the contributions of each modality/
feature set on the fusion performance of the top three sys-
tems in the 4F-CV setting with a randomization test. For
this, we randomly generated predictions for each modality
and combined them with the true predictions of from the
other modalities via majority voting. This was repeated a
hundred times and an average UAR was calculated. The
drops in UAR performance with respect to the original sys-
tems are shown in Fig. 3. We observe that within each sys-
tem, the contributions of the modalities are similar,
however the ranking changes per system. In two systems
(Systems 1 and 3), the contribution of the visual (FAU)
model is the highest and the linguistic modality ranks the
second. In System 2, the linguistic modality has the highest
contribution.4.3 Choice of Classiﬁer
In earlier sections, we have compared the unimodal per-
formances of the acoustic, linguistic and visual features
used in the proposed pipeline. In order to motivate the
choice of Kernel ELM in the proposed system, we compare
it with Ordinal Multi-Class SVM (OMSVM) [64] classiﬁer
with the same set of features and optimization procedure as
in the Kernel ELM. The choice of OMSVM is motivated by
the ordinal nature of the mania levels under investigation
and recent trends to use ordinal machine learning methods,
as opposed to classiﬁcation and regression in ordinal affec-
tive computing [65], [66]. We report the results with
OMSVM at the end of Table 6. Unlike Kernel ELM, OMSVM
does not model FAU and LIWC features well for mania
level prediction, and the development and test set UAR per-
formances remain below 40%. Subsequently, the majority
voting of the three modalities fall below the performance of
eGEMAPS features alone, which gives consistent perfor-
mance across the development and test sets, but with dra-
matically lower performance compared to the proposed
system.
Analyzing the confusion matrices of the best unimodal
and multimodal systems (see Fig. 4), we observe that the
multimodal system improves the recall performance of the
hypomania class on both 4F-CV setting and the test set. Fur-
thermore, the recall of the mania class is dramatically
improved with multimodality on the test set, while the
recall of the remission class is markedly higher (0.89) with
the acoustics-based unimodal model, compared to the tri-
modal fusion system (0.78).
Table 5 shows the test set results of the constituent unim-
odal systems obtained using the feature sets that perform
the best on the multimodal fusion experiments. The test set
results are obtained using the model trained on the combi-
nation of the training and the development sets, with the
parameters optimized on the 4F-CV experiments. eGe-
MAPS10 gives the highest unimodal UAR score (59.2%),TABLE 4
The Best Majority Voting Fusion Results
Acoustic Visual LinguisticUAR MM1 UAR MM1
4F-CV 4F-CV Test Test
eGeMAPS10* FAU LIWC 65.8% 0.15 61.1% 0.10
eGeMAPS FAU LIWC 65.1% 0.14 57.4% 0.0
eGeMAPS10 FAU LIWC 64.9% 0.13 64.8% 0.09
Fig. 3. Impact of each modality on the fusion UAR in the 4F-CV setting in
terms of performance drop due to randomization of the corresponding
predictions.
Fig. 4. Confusion matrices of the best unimodal system using eGE-
MAPS10 features (a-b) and the top multimodal fusion system (c-d).2126 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:26 UTC from IEEE Xplore.  Restrictions apply. which is on par with the state-of-the-art multimodal test set
performance reported on this dataset (see Table 6).
4.4 Interpretability Analysis
In order to provide insights into the decision making pro-
cess of the multimodal system, we carried out two sets of
experiments. The ﬁrst set of experiments are conducted
using a state-of-the-art explainable machine learning
method, Shapley Additive Explanations [60], to obtain fea-
ture importance attributions for each modality-speciﬁc
model that are fused via majority voting. The second set of
experiments are conducted to predict activity in each of the
eleven items of Young Mania Rating Scale (YMRS), which
served as ground truth annotations in the present study.
4.4.1 SHAP Based Interpretability Analysis
Since the top multimodal systems are based on decision
fusion rather than feature fusion, we analyse each modality-
speciﬁc model separately using Kernel SHAP [60], a model-
agnostic explanation method. The ELM models do not
provide a straightforward way to read the relative feature
importances. SHAP gives an ‘explanation’ for each decision,
and this amounts to the impact of each feature on the out-
put. For Dfeatures and t> 2classes, SHAP will output a
D/C2tmatrix of feature attributions for each prediction. A
common approach for explaining an instance is to visualize
the vector for the class having the highest posterior proba-
bility. To provide insight into our trained models, we aver-
aged the absolute SHAP feature attributions corresponding
to the predicted class over the test set. The resulting feature
importances are sorted and the top 20 features for each fea-
ture set are shown in Fig. 5.
Analyzing the plots for eGEMAPS related acoustic fea-
tures, we observe a large difference in the top features, due
to the additional functionals used in the proposed eGE-
MAPS10 set over the set of LLDs (see Table 2). However, in
both plots we observe the dominance of formant related
supra-segmental features among the most inﬂuential fea-
tures. In the affective computing literature, formats (reso-
nant frequencies of the vocal tract ﬁlter), speciﬁcally the
ﬁrst three, are known to carry affect related information [67],
[68]. Thus, this outcome is very intuitive. The F2descriptor
appears at the top of both rankings, albeit with different
functionals; the curvature and slope for eGEMAPS10, and
the mean for eGEMAPS.
Another observation in the acoustic models is about
prosody and voice quality related features. High level of
mania is associated with high vocal and kinesthetic energy,
thus it is not surprising to see functionals of loudnessamong the most inﬂuential features. In both plots, we
observe Harmonics-to-Noise Ratio (HNR), although sum-
marized with different functionals. Jitter and shimmer, two
other voice quality features, are also commonly associated
with mood disorders, mainly with depression [4], and we
observe them among the inﬂuential features.
When we look at the FAU features in Fig. 5, we see that
AU12 (lip corner puller) is the most prominent facial fea-
ture, which is an important feature for capturing mirth, as it
is activated in smiling and laughter. The second important
feature is AU17 (the chin raiser), which could indicate rapid
speech and excitation.
For the LIWC features, we have an interesting pattern,
with the “Religion” category showing a large feature impor-
tance. When we investigate the transcripts that show high
value for this category, we notice several manic patients
producing an incoherent discourse intermingled with heavy
religious terminology. In the literature, religious fervor is
listed as a possible indicator of the manic stage [69], and the
8th item of the YMRS measures hyper-religiosity explicitly.
Our patients indeed were exhibiting this symptom. Signiﬁ-
cantly, our experimental setting does not naturally lead to
the recounting of a religious experience; the two paintings
shown to the patients do not have religious overtones, and
the happy/sad memories also do not suggest anything in
this direction. Our conclusion is that LIWC-based automatic
analysis is powerful enough to spot these instances, and
such a cue is not detectable via face analysis, or voice para-
linguistics. We note that such symptoms are not seen in
each patient, and must be treated cautiously. By themselves,
religious fervor or discourse obviously do not immediately
suggest mania; it is by the combination of several symptoms
that a clinical diagnosis can be made.
4.4.2 YMRS Item Activity Analysis
YRMS scores are composed of eleven items that are
summed up and thresholded for mania level classiﬁcation
in the present study. These items assess the elevated mood,
increased motor activity-energy, sexual interest, sleep, irri-
tability, speech rate and amount, language-thought disor-
der, content, disruptive-aggressive behavior, appearance,
and insight.TABLE 5
Unimodal UAR (%) Performances of the Features Found
in the Best Performing Fusion Systems
Modality Feature Set 4F-CV Test
Acoustic eGeMAPS10* 52.2 55.5
Acoustic eGeMAPS 53.8 57.4
Acoustic eGeMAPS10 53.1 59.2
Linguistic LIWC 57.3 51.8
Visual FAU 56.0 51.8TABLE 6
UAR (%) Comparison of Works Using BD
with AVEC 2018 Challenge Protocol
Paper Validation Test
Ringeval et al. [3] (AVEC challenge baseline) 55.0 57.4
Yang et al. [23] 78.3 40.7
Xing et al. [24] 86.7 57.4
Syed, Sidorov, Marshall [25] 55.0 48.2
Ebrahim, Al-Ayyoub, Alsmirat [26] 59.2 44.4
Amiriparian et al. [27] 46.2 45.5
Ren et al. [28] 61.6 57.4
AbaeiKoupaei, Al Osman [29] 60.6 57.4
AbaeiKoupaei, Al Osman [30] 64.0 59.3
eGEMAPS with OMSVM 54.8 53.7
MV(eGEMAPS, FAU, LIWC) with OMSVM 45.5 51.9
Proposed system with KELM 64.0 64.8
MV: Majority voting of unimodal models.BAKI ET AL.: MULTIMODAL APPROACH FOR MANIA LEVEL PREDICTION IN BIPOLAR DISORDER 2127
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:26 UTC from IEEE Xplore.  Restrictions apply. A detailed description of the levels of each item can
be found online6. While some of these items are can be
predicted from acoustic, linguistic and visual features/
correlates, some involve evaluation of the clothing choice
(YMRS10-Appearance) or medical expert’s insight
(YMRS11-Insight), which we cannot model directly using
the modalities and features in question. Therefore, we set
up the following experiment in order to have an insight on
the extent a modality/feature set used in the proposed
pipeline can predict activity in these items. For the sake of
uniformity of treatment and ease of interpretation, we
binarized each YMRS item score yiat a threshold of 0 to
indicate the existence of activity aiin the respective item:
ai¼Iðyi>0Þ,w h e r e IðÞis the indicator function. We then
trained models to predict these activities using the constit-
uent feature sets in our system.The results of the YMRS item activity recognition experi-
ments are summarized in Table 7. All experiments are con-
ducted in the same manner as in the three level mania
classiﬁcation.
Looking at the 4F-CV UAR performances, on the overall
we observe the dominance of acoustic models’ recognition
of item activity (seven out of 11 items). A similar observa-
tion is that FAU and LIWC based models do not rank at the
top in any of the YMRS items, while their contribution is
observed in four YMRS items, where the multimodal fusion
performs the best. As observed before, FAU based visual
model contains cues for measuring the “Speech rate and
amount” as scored in YMRS6, and gives a higher UAR per-
formance than LIWC.
Observing the held-out test set performances, it is not sur-
prising to see that the best model setting on 4F-CV performs
worse than chance level on YMRS10 (Appearance) and
YMRS11 (Insight). We also see that YMRS5 (Irritability) activ-
ity is also a difﬁcult task to generalize, where the multimodal
Fig. 5. SHAP based feature importance plots for unimodal constituent models of the proposed mania level recognition system.
TABLE 7
4F-CV and Test Set (Last Row) UAR (%) Scores of YMRS Item Activity Prediction Models
Feature/System YMRS1 YMRS2 YMRS3 YMRS4 YMRS5 YMRS6 YMRS7 YMRS8 YMRS9 YMRS10 YMRS11
eGEMAPS10 66.3 65.4 63.4 56.9 65.6 76.4 72.5 63.5 67.4 68.2 63.6
eGEMAPS 74.1 66.2 69.5 55.0 68.4 76.4 69.5 67.9 61.6 62.3 64.9
FAU 68.6 67.4 56.8 56.1 64.8 73.5 60.6 61.6 58.0 69.4 56.6
LIWC 65.7 63.1 61.0 53.3 62.3 69.8 64.8 63.8 56.1 62.5 57.9
MV (eGEMAPS10) 68.5 69.2 61.9 56.2 68.3 78.1 70.2 65.8 64.4 73.8 61.8
MV (eGEMAPS) 70.1 69.2 65.2 55.0 70.7 77.8 64.6 66.6 60.6 66.9 59.8
Test Set Performance 64.1 68.6 74.3 62.6 51.7 69.0 67.0 72.0 57.4 46.5 46.5
MV: three modal majority voting.
6.https://dcf.psychiatry.uﬂ.edu/ﬁles/2011/05/Young-Mania-
Rating-Scale-Measure-with-background.pdf2128 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:26 UTC from IEEE Xplore.  Restrictions apply. system performs at chance level on the test set. From the
remaining eight items, seven of them have higher than 60%
UAR, of which four could reach higher than 68% UAR, moti-
vating a further study in this direction for interpretable
modeling based on symptoms. While the test set perfor-
mance of YMRS2 (Increased motor activity-energy, 68.6%
UAR) correlates with our expectations, observing the best
overall test set performance (74.3%) on YMRS3 (Sexual inter-
est) is positively surprising and motivating. We attribute this
to the acoustic features that capture arousal that also corre-
late with sexual arousal. We note that this aspect addresses a
long standing source of violence on psychiatry nurses [70].
Considering the potential for preventing violence on nurses
alone, this item deserves further study for accurate auto-
matic prediction.
When we use the ground truth binarized YMRS item
activity labels to predict mania levels, we obtain a 4F-CV
UAR performance of 83.5% and a corresponding test set
UAR performance of 72.2%. The test set performance is dra-
matically higher compared to the state-of-the-art reported
here and has room for improvement, since a simple discreti-
zation to two levels with thresholding at zero causes loss of
valuable intensity information. To the best of our knowl-
edge, this is the ﬁrst study to conduct item-wise YMRS
activity prediction. However, we are cautious not to over-
interpret the results, due to the low number of samples,
which limits the successful application of regression on
original YMRS item scores.
5D ISCUSSION AND CONCLUSIONS
In this paper, we investigated mania-level classiﬁcation
(mania, hypomania, remission) of bipolar disorder (BD)
patients using the Turkish Audio-Visual BD dataset, and
proposed a trimodal architecture. We have performed a
comprehensive analysis of fusion of modalities for predict-
ing mania levels. The results showed that multimodality
improves the classiﬁcation of bipolar disorder. The acoustic,
textual, and visual modalities complement each other and
using all three modalities gives the best performance. A
fusion model of just the linguistic and acoustic modalities
still performs well, while requiring less information. This
may be important, in case a camera is considered to be
intrusive in the assessment sessions.
The best performing system combines audio, video and
linguistic modalities using modality-speciﬁc weighted score
fusion of weighted and unweighted Kernel ELMs, decisions
of which are ﬁnally fused using majority voting. We achieve
64.8% test set UAR on this conﬁguration, which advances
the state-of-the-art on the BD dataset. The unimodal test
performance breakdown of the top multimodal systems
conﬁrm the robustness of acoustic eGeMAPS descriptors,
which deserves further research in depression studies.
The accuracy results we have obtained are not high
enough to use the proposed system in a real-world clinical
application as a decision support system for the clinician.
But this may partly be due to the small size of the training
corpus. There are 25, 38, and 41 clips in the training set for
the remission, hypomania, and mania classes, respectively,
which is not enough to generalize with a high certainty. On
the positive side, the dataset is collected in a real-lifescenario, and has a high level of ecological validity. It con-
tains background noise, and in some cases, the voice of the
clinician to explain points related to the questions. These
issues are expected to be present if a real-life application is
created, and the natural recording setup makes this data-
base valuable. Another difﬁculty stems from missing infor-
mation in some clips, where patients do not answer some of
the questions. In one of the test case clips, the patient does
not answer any questions at all. For the clinician, this may
inform the diagnosis, but for an automatic system, it is difﬁ-
cult to take such features into account, and for the standard
assessment methodology we use in this paper to ensure
comparability, these cases cause issues.
Experimental results have shown that the linguistic
modality contributes to the performance. We note two limi-
tations related to this modality, which can be tackled in
future studies. First, we use automatic transcription, which
is prone to errors, as Turkish is not a well-studied language
for automated speech recognition. Note that a fully auto-
mated recognition system was a requirement in the AVEC
Challenge. For a fair and direct comparison with the works
presented in the challenge, we have strictly adhered to the
challenge protocol in this work, and did not use manual
translation. We have, in a preliminary experiment, manu-
ally transcribed one task to assess the performance of the
automatic translation, and veriﬁed that it was producing
comparable results with the manual translation.
Our ﬁnal model contains information from three differ-
ent modalities, and each modality is represented using fea-
ture vectors with various sizes. It is especially important to
create explainable [71], and more preferably, interpret-
able [72] models in the medical domain, but model com-
plexity poses challenges from this perspective. In this study,
we opted for a compact set of interpretable features in each
modality and we analyzed the models to gain insights into
the inﬂuential features in the decision-making process. The
most important features in each modality correlate well
with the domain knowledge and have complementary
information. While the top ranking features in each modality
are not individually sufﬁcient for diagnosis, collectively they
contain information that correlates with observable symp-
toms and have a high potential to be used in a clinical deci-
sion support system. To provide further insights into the
capability of the used feature sets in symptomatic mania clas-
siﬁcation, we conducted item-wise YMRS activity modeling.
In line with our expectations, this analysis showed which
YMRS items cannot be accurately modeled with the used fea-
ture sets, while some items, such as ‘Sexual interest’, ‘Speech
rate and amount’ as well as ‘Content’, provided promising
results for future studies.
It is crucial to note that AI systems similar to the one we
have proposed in this paper use a very limited set of sources
in their assessment compared to the clinician, and are pri-
marily statistical (as opposed to causal) in their nature.
These limitations should be very clear in the reporting of
the results, and the support offered by automatic tools
should not be over-estimated. The Turkish Audio-Visual
BD dataset we have opened to the research community is
the ﬁrst dataset including audio, visual, and text modalities
in this area, and we hope it will foster the development of
richer analysis tools for helping clinicians.BAKI ET AL.: MULTIMODAL APPROACH FOR MANIA LEVEL PREDICTION IN BIPOLAR DISORDER 2129
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:26 UTC from IEEE Xplore.  Restrictions apply. REFERENCES
[1] S. M. Alghowinem, T. Gedeon, R. Goecke, J. Cohn, and G. Parker,
“Interpretation of depression detection models via feature selec-
tion methods,” IEEE Trans. Affect. Comput. , to be published,
doi:10.1109/TAFFC.2020.3035535 .
[2] M. Arif et al. , “Classiﬁcation of anxiety disorders using machine
learning methods: A literature review,” Insights Biomed. Res. ,
vol. 4, no. 1, pp. 95–110, 2020.
[3] F. Ringeval et al. , “AVEC 2018 workshop and challenge: Bipolar
disorder and cross-cultural affect recognition,” in Proc. Audio/Vis.
Emotion Challenge Workshop , 2018, pp. 3–13.
[4] N. Cummins, S. Scherer, J. Krajewski, S. Schnieder, J. Epps, and T.
F. Quatieri, “A review of depression and suicide risk assessment
using speech analysis,” Speech Commun. , vol. 71, pp. 10–49, 2015.
[5] A. A. Salah, “Designing computational tools for behavioral and
clinical science,” in Proc. Companion ACM SIGCHI Symp. Eng.
InterAct. Comput. Syst. , 2021, pp. 1–4.
[6] C. C. Nuckols and C. C. Nuckols, “Diagnostic and statistical man-
ual of mental disorders (DSM-V),,” Arlington: Amer. Psychiatr.
Assoc. , 2013, doi: 10.1176/appi.books.9780890425596 .
[7] K. R. Merikangas et al. , “Lifetime and 12-month prevalence of
bipolar spectrum disorder in the national comorbidity survey rep-
lication,” Arch. Gen. Psychiatry , vol. 64, no. 5, pp. 543–552, 2007.
[8] World Health Organization, The Global Burden of Disease: 2004
Update . Geneva, Switzerland: World Health Organization, 2008.
[9] A. F. Carvalho, J. Firth, and E. Vieta, “Bipolar disorder,” New Eng-
land J. Med. , vol. 383, no. 1, pp. 58–66, 2020.
[10] K. Van Til, M. G. McInnis, and A. Cochran, “A comparative study
of engagement in mobile and wearable health monitoring for
bipolar disorder,” Bipolar Disord. , vol. 22, no. 2, pp. 182–190, 2020.
[11] R. C. Young, J. T. Biggs, V. E. Ziegler, and D. A. Meyer, “A rating
scale for mania: Reliability, validity and sensitivity,” Brit. J. Psychi-
atry, vol. 133, no. 5, pp. 429–435, 1978.
[12] R. Voleti, S. Woolridge, J. M. Liss, M. Milanovic, C. R. Bowie, and
V. Berisha, “Objective assessment of social skills using automated
language analysis for identiﬁcation of schizophrenia and bipolar
disorder,” in Proc. Interspeech , 2019, pp. 1433–1437.
[13] K. Matton, M. G. McInnis, and E. M. Provost, “Into the wild: Tran-
sitioning from recognizing mood in clinical interactions to per-
sonal conversations for individuals with bipolar disorder,” in
Proc. Interspeech , 2019, pp. 1438–1442.
[14] E. C ¸ iftc¸i, H. Kaya, H. G €ulec¸, and A. A. Salah, “The Turkish audio-
visual bipolar disorder corpus,” in Proc. 1st Asian Conf. Affect.
Comput. Intell. Interaction , 2018, pp. 1–6.
[15] N. Aloshban, A. Esposito, and A. Vinciarelli, “What you say or
how you say it? Depression detection through joint modeling of
linguistic and acoustic aspects of speech,” in Cognitive Computa-
tion. Berlin, Germany: Springer, 2021, pp. 1–14.
[16] M. Nasir, A. Jati, P. G. Shivakumar, S. Nallan Chakravarthula, and
P. Georgiou, “Multimodal and multiresolution depression detec-
tion from speech and facial landmark features,” in Proc. Audio/Vis.
Emotion Challenge Workshop , 2016, pp. 43–50.
[17] S. Alghowinem et al. , “Multimodal depression detection: Fusion
analysis of paralinguistic, head pose and eye gaze behaviors,”
IEEE Trans. Affect. Comput. , vol. 9, no. 4, pp. 478–490, Apr. 2016.
[18] H. Kaya and A. A. Salah, “Eyes whisper depression: A CCA based
multimodal approach,” in Proc. Int. Conf. Multimodal Interfaces ,
2014, pp. 961–964.
[19] H. Kaya et al. , “Predicting depression and emotions in the cross-
roads of cultures, para-linguistics, and non-linguistics,” in Proc.
9th Int. Audio/Vis. Emotion Challenge Workshop , 2019, pp. 27–35.
[20] L. Yang, D. Jiang, and H. Sahli, “Integrating deep and shallow mod-
els for multi-modal depression analysis—hybrid architectures,”
IEEE Trans. Affect. Comput. , vol. 2, no. 1, pp. 239–253, Jan. 2021.
[21] T. Al Hanai, M. M. Ghassemi, and J. R. Glass, “Detecting depres-
sion with audio/text sequence modeling of interviews,” in Proc.
Interspeech , 2018, pp. 1716–1720.
[22] P. Baki, H. Kaya, E. C ¸i f t c¸ i ,H .G €ulec¸ ,a n dA .A .S a l a h ,“ S p e e c ha n a l y s i s
for automatic mania assessment in bipolar disorder,” in Proc.
IEEE 28th Signal Process. Commun. Appl. Conf. , 2020, pp. 2104–
2007.
[23] L. Yang, Y. Li, H. Chen, D. Jiang, M. C. Oveneke, and H. Sahli,
“Bipolar disorder recognition with histogram features of arousal
and body gestures,” in Proc. Audio/Vis. Emotion Challenge Work-
shop, 2018, pp. 15–21.[24] X. Xing, B. Cai, Y. Zhao, S. Li, Z. He, and W. Fan, “Multi-modality
hierarchical recall based on GBDTs for bipolar disorder classi-
ﬁcation,” in Proc. Audio/Vis. Emotion Challenge Workshop , 2018,
pp. 31–37.
[25] Z. S. Syed, K. Sidorov, and D. Marshall, “Automated screening for
bipolar disorder from audio/visual modalities,” in Proc. Audio/
Vis. Emotion Challenge Workshop , 2018, pp. 39–45.
[26] M. Ebrahim, M. Al-Ayyoub, and M. Alsmirat, “Determine bipolar
disorder level from patient interviews using Bi-LSTM and feature
fusion,” in Proc. 5th Int. Conf. Social Netw. Anal. Manage. Secur. ,
2018, pp. 182–189.
[27] S. Amiriparian et al., “Audio-based recognition of bipolar disorder
utilising capsule networks,” in Proc. Int. Joint Conf. Neural Netw. ,
2019, pp. 1–7.
[28] Z. Ren, J. Han, N. Cummins, Q. Kong, M. D. Plumbley, and B. W.
Schuller, “Multi-instance learning for bipolar disorder diagnosis
using weakly labelled speech data,” in Proc. 9th Int. Conf. Digit.
Public Health , 2019, pp. 79–83.
[29] N. AbaeiKoupaei and H. Al Osman, “A hybrid model for bipolar
disorder classiﬁcation from visual information,” in Proc. IEEE Int.
Conf. Acoust. Speech Signal Process. , 2020, pp. 4107–4111.
[30] N. AbaeiKoupaei and H. Al Osman, “A multi-modal stacked
ensemble model for bipolar disorder classiﬁcation,” IEEE Trans.
Affect. Comput. , to be published, doi: 10.1109/TAFFC.2020.3047582 .
[31] Z. Du, W. Li, D. Huang, and Y. Wang, “Bipolar disorder recogni-
tion via multi-scale discriminative audio temporal representation,”
inProc. Audio/Vis. Emotion Challenge Workshop , 2018, pp. 23–30.
[32] F. Eyben, M. W €ollmer, and B. Schuller, “Opensmile: The Munich
versatile and fast open-source audio feature extractor,” in Proc.
18th ACM Int. Conf. Multimedia , 2010, pp. 1459–1462.
[33] F. Eyben et al. , “The Geneva minimalistic acoustic parameter set
(GeMAPS) for voice research and affective computing,” IEEE
Trans. Affect. Comput. , vol. 7, no. 2, pp. 190–202, Feb. 2015.
[34] W. Zong, G.-B. Huang, and Y. Chen, “Weighted extreme learning
machine for imbalance learning,” Neurocomputing , vol. 101,
pp. 229–242, 2013.
[35] M. Schmitt and B. W. Schuller, “openXBOW – Introducing the
passau open-source crossmodal bag-of-words toolkit,” J. Mach.
Learn. Res. , vol. 18, pp. 1–5, 2017.
[36] S. Sabour, N. Frosst, and G. E. Hinton, “Dynamic routing between
capsules,” in Proc. 31st Int. Conf. Neural Inform. Process. Syst. , 2017,
pp. 3856–3866.
[37] Z. Zhang, W. Lin, M. Liu, and M. Mahmoud, “Multimodal deep
learning framework for mental disorder recognition,” in Proc. 15th
IEEE Int. Conf. Autom. Face Gesture Recognit. , 2020, pp. 344–350.
[38] S. A. Crossley, K. Kyle, and D. S. McNamara, “Sentiment analysis
and social cognition engine (seance): An automatic tool for senti-
ment, social cognition, and social-order analysis,” Behav. Res.
Methods , vol. 49, no. 3, pp. 803–821, 2017.
[39] B. Schuller et al. , “The INTERSPEECH 2010 paralinguistic
challenge,” in Proc. Interspeech , 2010, pp. 2794–2797, doi: 10.21437/
Interspeech.2010-739 .
[40] D. M. Hilty, K. T. Brady, and R. E. Hales, “A review of bipolar disor-
der among adults,” Psychiatr. Serv. , vol. 50, no. 2, pp. 201–213, 1999.
[41] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever,
“Language models are unsupervised multitask learners,” OpenAI
Blog, vol. 1, no. 8, 2019, Art. no. 9.
[42] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-
training of deep bidirectional transformers for language under-
standing,” 2018, arXiv:1810.04805 .
[43] T. B. Brown et al. , “Language models are few-shot learners,”
2020, arXiv:2005.14165 .
[44] J. W. Pennebaker, M. E. Francis, and R. J. Booth, “Linguistic
inquiry and word count: LIWC 2001,” Mahway: Lawrence Erlbaum
Associates , vol. 71, 2001, Art. no. 2001.
[45] S. Bird, E. Klein, and E. Loper, Natural Language Processing With
Python: Analyzing Text With the Natural Lang. Toolkit . Sebastopol,
CA, USA: O’Reilly Media, 2009.
[46] M. F. Porter et al. , “An algorithm for sufﬁx stripping,” Program ,
vol. 14, no. 3, pp. 130–137, 1980.
[47] C. Gilbert and E. Hutto, “VADER: A parsimonious rule-based
model for sentiment analysis of social media text,” in Proc. 8th Int.
Conf. Weblogs Social Media , 2014.
[48] S. Loria, “textblob documentation,” Release 0.15 , vol. 2, no. 8, 2018.
[Online]. Available: https://buildmedia.readthedocs.org/media/
pdf/textblob/latest/textblob.pdf2130 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:26 UTC from IEEE Xplore.  Restrictions apply. [49] A. Akbik, T. Bergmann, D. Blythe, K. Rasul, S. Schweter, and R.
Vollgraf, “FLAIR: An easy-to-use framework for state-of-the-art
NLP,” in Proc. Conf. North Amer. Chapter Assoc. Comput. Linguistics ,
2019, pp. 54–59.
[50] G. So /C21gancıo /C21gluet al., “Is everything ﬁne, grandma? Acoustic and
linguistic modeling for robust elderly speech emotion recog-
nition,” in Proc. Interspeech , 2020, pp. 2097–2101.
[51] P. Ekman and E. Rosenberg, What the Face Reveals: Basic and Applied
Studies of Spontaneous Expression Using the Facial Action Coding System
(FACS) . New York, NY, USA: Oxford Univ. Press, 1997.
[52] T. Baltru /C20saitis, P. Robinson, and L.-P. Morency, “Openface: An
open source facial behavior analysis toolkit,” in Proc. IEEE Winter
Conf. Appl. Comput. Vis. , 2016, pp. 1–10.
[53] H. Kaya, F. G €urpinar, S. Afshar, and A. A. Salah, “Contrasting and
combining least squares based learners for emotion recognition in
the wild,” in Proc. ACM Int. Conf. Multimodal Interaction , 2015,
pp. 459–466.
[54] P. Geurts, D. Ernst, and L. Wehenkel, “Extremely randomized
trees,” Mach. Learn. , vol. 63, no. 1, pp. 3–42, 2006.
[55] L. Breiman, “Random forests,” Mach. Learn. ,v o l .4 5 ,n o .1 ,p p .5 – 3 2 ,
2001.
[56] G.-B. Huang, H. Zhou, X. Ding, and R. Zhang, “Extreme learning
machine for regression and multiclass classiﬁcation,” IEEE Trans.
Syst., Man, Cybern. B, Cybern. , vol. 42, no. 2, pp. 513–529, Apr. 2011.
[57] C. R. Rao et al., “Generalized inverse of a matrix and its applications,”
inProc. 6th Berkeley Symp. Math. Statist. Probability , 1972, pp. 601–620.
[58] F. Gurpinar, H. Kaya, H. Dibeklioglu, and A. Salah, “Kernel ELM
and CNN based facial age estimation,” in Proc. IEEE Conf. Comput.
Vis. Pattern Recognit. Workshops , 2016, pp. 80–86.
[59] S. K. D’mello and J. Kory, “A review and meta-analysis of multi-
modal affect detection systems,” ACM Comput. Surv. , vol. 47,
no. 3, pp. 1–36, 2015.
[60] S. M. Lundberg and S.-I. Lee, “A uniﬁed approach to interpreting
model predictions,” in Proc. 31st Int. Conf. Neural Inform. Process.
Syst., 2017, pp. 4768–4777.
[61] L. S. Shapley, A Value for N-Person Games. Contributions to the The-
ory of Games , H. Kuhn and A. Tucker, Eds., Princeton, NJ, USA:
Princeton Univ. Press, 1953.
[62] F. Pedregosa et al. , “Scikit-learn: Machine learning in python,” J.
Mach. Learn. Res. , vol. 12, pp. 2825–2830, 2011.
[63] H. Gunes and M. Piccardi, “Automatic temporal segment detec-
tion and affect recognition from face and body display,” IEEE
Trans. Syst., Man, Cybern. B , vol. 39, no. 1, pp. 64–84, Jan. 2008.
[64] K.-J. Kim and H. Ahn, “A corporate credit rating model using
multi-class support vector machines with an ordinal pairwise par-
titioning approach,” Comput. Operations Res. , vol. 39, no. 8,
pp. 1800–1811, 2012.
[65] S. Jayawardena, J. Epps, and E. Ambikairajah, “Ordinal logistic
regression with partial proportional odds for depression pre-
diction,” IEEE Trans. Affect. Comput. , to be published, doi: 10.1109/
TAFFC.2020.3031300 .
[66] J. Wu, T. Dang, V. Sethu, and E. Ambikairajah, “Multimodal affect
models: An investigation of relative salience of audio and visual
cues for emotion prediction,” Front. Comput. Sci. , vol. 3, pp. 1–12,
2021, doi: 10.3389/fcomp.2021.767767 .
[67] M. El Ayadi, M. S. Kamel, and F. Karray, “Survey on speech emo-
tion recognition: Features, classiﬁcation schemes, and databases,”
Pattern Recognit. , vol. 44, no. 3, pp. 572–587, 2011.
[68] E. Lyakso, O. Frolova, and A. Grigorev, “A comparison of acoustic
features of speech of typically developing children and children
with autism spectrum disorders,” in Proc. Int. Conf. Speech Com-
put., 2016, pp. 43–50.
[69] T. D. Brewerton, “Hyperreligiosity in psychotic disorders,” J. Ner-
vous Ment. Dis. , vol. 182, pp. 302–304, 1994.
[70] H. Nijman, L. Bowers, N. Oud, and G. Jansen, “Psychiatric nurses’
experiences with inpatient aggression,” Aggressive Behav. , vol. 31,
no. 3, pp. 217–227, 2005.
[71] S. Tonekaboni, S. Joshi, M. D. McCradden, and A. Goldenberg,
“What clinicians want: Contextualizing explainable machine learn-
ing for clinical end use,” in Proc. Mach. Learn. Healthcare Conf. , 2019,
pp. 359–380.
[72] C. Rudin, “Stop explaining black box machine learning models for
high stakes decisions and use interpretable models instead,” Nat.
Mach. Intell. , vol. 1, no. 5, pp. 206–215, 2019.
Pınar Baki received the BSc and MSc degrees
from the Computer Engineering Department,
Bogazici University , in 2018, and 2020, respec-
tively . For her masters thesis, she worked on
detecting the mood states of the bipolar disorder
patients from multimodal data. She is also a
machine learning engineer at Arcelik Innovation
Center, studying age, gender, and emotion analy-
sis from speech data.
Heysem Kaya (Member, IEEE) received the PhD
degree from Computer Engineering Department,
Bogazici University , in 2015. He is an assistant
professor with the Department of Information and
Computing Sciences, Utrecht University . He has
coauthored more than 70 publications on
machine learning, computational paralinguistics
and affective computing. His works won six
Computational Paralinguistics Challenge (Com-
ParE) awards and four awards in multimodal
affective computing challenges, including three
ChaLearn competitions.
Elvan C ¸ iftc¸ihas a background in neuroscience
and clinical psychiatry . She is currently working as
a psychiatrist responsible from inpatient unit, a
specialty outpatient clinic in NP Brain Hospital
and an association with €Usk€udar University . Her
research interests include related with neurobiol-
ogy of schizophrenia and bipolar disorder, using
brain imaging techniques, electrophysiological
paradigms, clinical databases, and in vivo studies.
H€useyin G €ulec¸is an associate professor and chair
of the Psychiatric Department and Sleep Center,
Psychiatric and Neurological Diseases Training and
Research Hospital of Health Science University Tur-
key . He has coauthored more than 100 publications
in the ﬁelds of Consultation Liaison Psychiatry, Psy-
chometry, Schizophrenia, Sleep Medicine, General
Psychiatry, Personality, Violence and Cognition. He
serves as an Editorial and Advisory Board member
of several journals in his ﬁeld.
Albert Ali Salah (Senior Member , IEEE) is profes-
sor and chair of Social and Affective Computing,
Information and Computing Sciences Department,
Utrecht University and adjunct professor with the
Computer Engineering Department, Bogazici Uni-
versity . He works on pattern recognition, multi-
modal interaction, and computer analysis of human
behavior . He serves in the Steering Boards of ACM
ICMI and IEEE FG, and as an associate editor of
journals including IEEE Transactions on Affective
Computing ,Pattern Recognition ,a n d International
Journal on Human-Computer Studies .
"For more information on this or any other computing topic,
please visit our Digital Library at www.computer .org/csdl.BAKI ET AL.: MULTIMODAL APPROACH FOR MANIA LEVEL PREDICTION IN BIPOLAR DISORDER 2131
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:26 UTC from IEEE Xplore.  Restrictions apply. The Arousal Video Game AnnotatIoN (AGAIN)
Dataset
David Melhart , Antonios Liapis , and Georgios N. Yannakakis ,Senior Member, IEEE
Abstract— How can we model affect in a general fashion, across dissimilar tasks, and to which degree are such general
representations of affect even possible? To address such questions and enable research towards general affective computing, this
paper introduces The Arousal video Game AnnotatIoN (AGAIN) dataset. AGAIN is a large-scale affective corpus that features over
1,100 in-game videos (with corresponding gameplay data) from nine different games, which are annotated for arousal from 124
participants in a ﬁrst-person continuous fashion. Even though AGAIN is created for the purpose of investigating the generality of
affective computing across dissimilar tasks, affect modelling can be studied within each of its 9 speciﬁc interactive games. To the best
of our knowledge AGAIN is the largest—over 37 hours of annotated video and game logs—and most diverse publicly available affective
dataset based on games as interactive affect elicitors.
Index Terms— Emotional corpora, arousal, human-computer interaction, affective computing, games
Ç
1I NTRODUCTION
Acore challenge of affective computing (AC) is the inves-
tigation of generality in the ways emotions are elicited
and manifested, in the annotation protocols designed, and
ultimately in the affect models created. To examine the
degree to which general representations of affect are possi-
ble and meaningful, AC research requires access to corpora
containing affect responses and annotations across dissimi-
lar tasks, participants and annotators. Traditional large-
scale AC datasets feature affect annotation of static images,
videos, sounds and speech ﬁles within a narrow context
through which affect is elicited from a particular task. Even
when the various tasks under annotation may vary, those
are still limited to a very speciﬁc context—such as viewing a
set of social interactions under a theme or playing sessions
of the same game.
This paper identiﬁes games as a unique opportunity in
AC to observe emergent emotions in a well-deﬁned but
highly interactive environment. Interactivity is especially
important for the future of AC research as emotions perme-
ate our daily interactions—not just with each other, but
with our environment and computers as well. Affective
states arising from these interactions impact our behaviour
and decision making on a fundamental level [1], [2].Therefore, modelling emotions that emerge from interac-
tions is becoming paramount to AC research.
Motivated by the lack of corpora for the study of general
properties of affect across tasks and participants, in this
paper we introduce The Arousal video Game AnnotatIoN
(AGAIN) dataset, which contains data from over 120 partic-
ipants who played and annotated over 1,000 gameplay ses-
sions. AGAIN is accessible online1and features data
collected from nine games of three dissimilar genres, which
were developed speciﬁcally for the purposes of the dataset
(see Fig. 1). As shown in Table 1, along with game telemetry
and self-annotated arousal labels, the dataset also features a
video database of unique gameplay sessions with over 37
hours of in-game footage. The diverse nature of the AGAIN
affect elicitors (games) provides a testbed for general affect
detection in games [3], [4] and broadens the horizons for
research on general-purpose AI representations [5], [6] and
artiﬁcial general intelligence.
While AC datasets in general rely on collecting periph-
eral physiological signals in laboratory settings, the AGAIN
dataset moves data collection to an online setting. On the
one hand, this setup only allows us to collect behavioural
data in a reliable way. However, since the tools and pipe-
lines employed to collect the dataset emphasise a simple
crowdsourced setup, the AGAIN database is much more
ﬂexible, extensible and scalable. The design and creation of
AGAIN was indeed guided by the following factors: a)
accessibility , which is achieved through an online crowd-
sourcing framework; b) scalability : AGAIN is utilising the
PAGAN online annotation framework [7] and, hence, one
can easily populate the AGAIN database with more partici-
pants and annotators; c) extensibility : more affect dimensions
and categories can be considered and integrated to the exist-
ing dataset through the customisable PAGAN annotation
tool; d) generality : any additional online game or interactive/C15The authors are with the Institute of Digital Games, University of Malta,
2080 Msida, Malta. E-mail: {david.melhart, antonios.liapis, georgios.
yannakakis}@um.edu.mt.
Manuscript received 17 January 2022; revised 21 June 2022; accepted 29 June
2022. Date of publication 6 July 2022; date of current version 15 November
2022.
The works of Antonios Liapis and Georgios N. Yannakakis were supported by
the European Union’s H2020 programme under Grant 951911.
This work involved human subjects or animals in its research. The author(s)
conﬁrm(s) that all human/animal subject research procedures and protocols
are exempt from review board approval.
(Corresponding author: David Melhart.)
Recommended for acceptance by J. Broekens.
Digital Object Identiﬁer no. 10.1109/TAFFC.2022.3188851 1.http://again.institutedigitalgames.com/IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022 2171
1949-3045 ©2022 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See ht _tps://www.ieee.org/publications/rights/index.html for more information.
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:58 UTC from IEEE Xplore.  Restrictions apply. session can be easily integrated to the experimental protocol
of AGAIN. While at the time of writing the database hosts 9
games annotated for arousal, AGAIN is designed with all
aforementioned factors in mind so that it can host data from
more games and user modalities, considering alternative
affective labels.
The AGAIN dataset is unique in a number of ways. First,
it is the largest and most diverse publicly available affective
dataset based on games as interactive elicitors. Given the
breadth of elicitors offered, the dataset can be used for test-
ing speciﬁc affect models on one particular task (i.e., a par-
ticular game) all the way to general models of affect across
tasks (game genres and games in general). Second, the data-
set is annotated with the core affective dimension of arousal,
linking dominant annotation practices in affective comput-
ing with player modelling and game user research. Finally,
it employs a novel annotation framework [8] which captures
subjective annotations in a continuous and unbounded
manner that can be further processed as labels for regres-
sion, classiﬁcation or ordinal learning affect modelling tasks
[9], [10].
The remainder of the paper is structured as follows. Sec-
tion 2 contextualises the dataset within the ﬁelds of affective
computing and affect modelling in games while Section 3
offers a systematic review of existing audiovisual datasets.
The games used as the affect elicitors of AGAIN are
described in Section 4. Section 5 details the AGAIN dataset
by describing the protocol followed, the characteristics of
the participants, the data types collected, and the annotation
framework used. Section 6 offers a detailed yet preliminary
data analysis of the dataset. Limitations and extensions of
AGAIN are discussed in Section 7 and the paper concludes
with Section 8.
2B ACKGROUND
AGAIN is an accessible dataset offered for research in affec-
tive computing at large and player modelling in particular.
This background section discusses the importance of
arousal within the ﬁeld of affect representation (Section 2.1)and reviews studies for modelling the affect of game users
(i.e., players) in Section 2.2.
2.1 Arousal as Affect Representation
While there are different approaches to affect representation
including categorical [11], [12], dimensional [13], and mixed
[14] frameworks, the AGAIN dataset uses a dimensional
representation based on the Pleasure-Arousal-Dominance
(PAD) model of affect [15] and the Circumplex Model of
Emotions [13]. In contrast to categorical frameworks, which
assume a clear division between emotional responses, these
models propose a more ambiguous and general representa-
tion. Instead of complex emotions, the PAD model focuses
on basic affective states represented across three dimen-
sions. Pleasure is associated with the valence of the emotion;
psychological arousal describes the intensity of the emotion;
and ﬁnally dominance describes the agency or level of auton-
omy during the emotional episode. One can place different
emotions within this 3D continuous space without explicitly
categorising them, reducing the chance of misrepresenting
how a subject feels. This type of evaluation lends itself bet-
ter for continuous and subjective annotation [9], [10].
While the Circumplex model and the PAD model repre-
sent affect across two and three dimensions, respectively, in
the AGAIN dataset we focus currently on soliciting annota-
tions based on the dimension of arousal . Selecting and inves-
tigating arousal ﬁrst—instead of other affect dimensions—
is relevant for games, the core domain of AGAIN. Arousal
is present and dominant as an emotional manifestation in
game affect interactions and has been associated with chal-
lenge [16], cognitive and affective engagement [17], tension
[18], fun [19], frustration [20] and ﬂow [21], as well as posi-
tive post-game outcomes, such as increased creativity [22]
and working memory [23] performance. Focusing on one
affect dimension reduces the cognitive load of the annota-
tion task [7], which in turn increases the reliability of our
data; however, it limits the expressive range of affect anno-
tation in the dataset. Moreover, the focus on arousal assists
the research community to build, extend upon and advance
research that already has benchmarked the study of arousal
in games [4], [5], [8].
2.2 Affect Modelling in Games
Player modelling is the study of video game play both in
terms of behavioural and affective patterns [24]. It relies
heavily on artiﬁcial intelligence methods for building
Fig. 1. All games featured in the AGAIN dataset currently . The dataset
includes 3 racing games (top row), 3 shooter games (middle row), and
three platformers (bottom row).TABLE 1
Core Properties of the AGAIN Dataset
Properties Raw dataset Clean dataset
Number of Participants 124 122
Number of Gameplay Videos 1116 995
Number of Game-telemetry Logs 1116 995
Video database size 37+ hours 33+ hours
Number of Elicitors 9 games (3 genres)
Gameplay/Video duration 2 min
Annotation Perspective First-person
Annotation Type Continuous unbounded
Affective Labels Arousal2172 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:58 UTC from IEEE Xplore.  Restrictions apply. predictive models of player behaviour [25], [26], playtime
[27], churn [28], [29], or player experience [5], [9], [30]. It is
naturally characterised by dynamic representations and
modelling of data, thereby providing even moment-to-
moment predictions of a game’s elicited experience [31]. A
key limitation of player modelling, as with any other data-
driven approach, is that it is data hungry. In particular,
studies that focus on affective aspects of player experience
require ground-truth affect labels which are often costly to
collect [32], [33].
To address the above challenge, an increasing number of
studies focus on approaches that could realise aspects of
general player modelling [3]. General player modelling fea-
tures methods that are able to predict a player’s affective
state on unseen games. While early studies such as that of
Martinez et al. [34] investigated game-independent features
of the playing experience, such as heart rate and skin con-
ductance, later studies put an emphasis on ﬁnding general
gameplay features either manually [35] or through algorith-
mic feature mapping [36]. More recently, Camilleri et al.
investigated general gameplay features and generalised
metrics of player experience across three dissimilar games
[4]. Their study used high-level features such as goal-oriented
and goal-opposed gameplay events and relative metrics of
arousal to moderate success, showing the difﬁculty of creat-
ing general player models. Similarly, Bonometti et al. used
high-level general features to characterise the gameplay
context (such as activity count and activity diversity) to
model engagement across six games published by Square
Enix Ltd. [37].
3A UDIOVISUAL AFFECTIVE DATASETS
The availability of large-scale corpora comprising affect
manifestations that are elicited through appropriate stimuli
is a necessity for affect modelling. Creating datasets that are
annotated with reliable affect information is, therefore,
instrumental to the ﬁeld of AC at large. In this section we
review representative affective corpora that rely on audiovi-
sual elicitors and discuss the contribution of AGAIN to the
current list of datasets that are enriched with affect labels.
Table 2 presents the outcome of our survey.2We follow a
systematic approach for reviewing the state of the art in
affect corpora and examine the following factors that distin-
guish the surveyed datasets: the interactivity and the typeof
the provided elicitors, the number of possible elicitor items ,
and the overall size of the available video database (see sec-
ond to ﬁfth column of Table 2), the number of participants
and their recorded modalities (see columns six and seven of
Table 2), the annotation protocol in terms of perspective and
type of annotation (see columns eight and nine of Table 2),
the affect labels (see column ten of Table 2), and ﬁnally the
number of annotators (if different from the number of partic-
ipants and/or not self-reported) and number of tasks each
annotator had to complete (see the last two columns of
Table 2).It is apparent from Table 2 that affective datasets have
gradually—over the last decade or so—drifted away from
traditional induced elicitation and posed expressions, and
instead turned towards soliciting spontaneous emotion
manifestations. New datasets have been focusing on elicita-
tion through naturalistic expressions. While some of these
datasets use their own elicitors, many rely on popular
media, using video clips and still images from music videos
and movies [39], [41], [42]. This method has proved to be
reliable, cost-effective, and easy to set up, which subse-
quently led to a widespread adoption in the ﬁeld. Com-
pared to staged videos and images, interactive elicitors
provide more organic stimuli. While reactions to non-inter-
active media can produce spontaneous expressions, interac-
tive elicitation can increase the participants’ involvement
with the elicitor and reveal emotional reactions that might
be hard to elicit with pre-recorded videos and images alone.
Subsequently, there has been a growing body of research
dedicated to enrich the set of affective corpora with interac-
tive elicitors. These datasets use a wide-range of methods
including dyadic tasks [43], [44], group tasks[45], board
games[46], and video games [48], [49]. These interactive
tasks provide a more complex and multifaceted affective
stimulus, while organically structuring the participants’
experience.
Most traditional affective computing databases surveyed
capture affective dimensions such as arousal and valence ,
with some datasets offering labels for additional dimen-
sions—such as dominance —and categorical labels (see the
annotation/labels column in Table 2). However, datasets
using interactive elicitors tend to have a wider focus. While
some of these datasets collect affective and emotional labels,
their primary focus is task-related emotional outcomes. In
datasets focusing on gameplay, this generally includes
gameplay experience [46] and other game-related out-
comes–such as frustration, perceived challenge [47], [48],
engagement [48], and fun [47], [49]. Studies that use the
same affective labels are easier to compare and their lessons
easier to transfer to new data than studies using a more
diverse set of labels (i.e., fun, engagement, challenge, etc.).
On one hand, the mapping between labels such as “fun”
and “challenge” can be uncertain; and on the other hand, it
can be difﬁcult to reliably translate outcomes such as
“engagement” to more traditional affective computing con-
cepts such as arousal or valence.
The affective datasets we survey appear to be rather split
in terms of annotation type used. While some (e.g., DEAP
[39], MANHOB-HCI [38]) opt for self-reporting (ﬁrst-person
annotation), many databases (e.g., RELOCA [44], SEWA
[43]) use only a few expert annotators in a third-person
manner. There is a clear trade-off between these
approaches. First-person annotations (i.e., self-reported
labels) are ideal for capturing the subjective appraisal of
emotional content, while third-person annotations are better
at labelling emotion manifestation through inter-rater agree-
ment [50]. Interestingly, most datasets using an interactive
elicitor also opt for ﬁrst-person annotation through self-
reports. A possible explanation is the higher degree of par-
ticipant involvement, which makes the experience more
unique to the participant. Such a highly subjective experi-
ence is better captured via ﬁrst-person reporting. There is a2.N/A indicates where the category is “not-applicable” (e.g., there
are no participants when third-party videos are used) and UNK indi-
cates if an attribute is “unknown”.MELHART ET AL.: AROUSAL VIDEO GAME ANNOTATION (AGAIN) DATASET 2173
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:58 UTC from IEEE Xplore.  Restrictions apply. deﬁnite trade-off, however, between interactive and non-
interactive elicitation. Using multimedia clips for elicitation
offers a cost-effective solution, which leads to large and var-
ied datasets. On the other hand, interactive elicitors can
stimulate emergent emotions more naturally.
Datasets using non-interactive elicitors are generally
larger, while the cost associated with using interactive elici-
tation limits these datasets. Table 2 shows that interactive
datasets often focus on fewer elicitor items. However, this
lack of variety is often offset by the different ways partici-
pants can interact with these elicitors, producing more
diverse data. Despite this diversity, there are not many data-
sets that feature multiple different interactive elicitors. Thehandful of examples either combine dissimilar datasets
[4]—which comes with its own challenges in reconciling dif-
ferent data formats—or use a small set of very similar elici-
tors—e.g., the FUNii dataset [49] features two similar games
from the same franchise. There are some exceptions: for
instance, the MUMBAI dataset [46] features six board
games, although the data here is not self-reported but
labelled by third-person annotators.
AGAIN addresses the aforementioned limitations by
offering a large-scale corpus that is based on a set of dissimi-
lar interactive affect elicitors that are annotated through a
ﬁrst-person protocol. While the dataset at the time of writ-
ing is limited to 9 games and their annotated arousal, theTABLE 2
A Survey of Affective Datasets of Audiovisual Content
Elicitation Participants Annotation
Database Interactive Type Items VideoNumber Modalities Perspective Type Labels Annotators Tasks
MAHNOB-
HCI [38]No Video 20
videos20
hours30 EEG, ECG, EDA,
temp., resp., face
and body video,
gaze, audioFirst-
personDiscrete
(9-step)Arousal, valence,
dominance, emotional
keywords,
predictabilityself-report 20
DEAP [39] No Video 40
videos40
mins32 EEG, BVP, EDA,
EMG, temp.,
resp., face videoFirst-
personDiscrete
(5-step)Arousal, valence,
dominance, liking,
familiarityself-report 40
LIRIS-
ACCEDE
[40]No Video 9,800
videos27
hoursN/A N/A First-
personPairwise Arousal, valence 1517
(arousal)
2442
(valence)UNK
Aff-Wild
[41]No Video 298
videos30
hours200 N/A Third-
personContinuous
boundedArousal, valence 6-8 298
AffectNet
[42]No Image 450,000
imagesN/A N/A N/A Third-
personContinuous
bounded,
categoricalArousal, valence, 8
emotion categories12 137,500
Sonancia
[18]No Audio 1280
soundsN/A N/A N/A First-
personPairwise Arousal, valence,
tensionUNK 10
SEWA DB
[43]Yes Video 4
videos
1 task27
hours
17
hours398 Facial
landmarks, FAU,
hand and head
gesturesThird-
personContinuous
boundedArousal, valence (dis)
liking intensity,
agreement, mimicry59 0
RELOCA
[44]Yes Video 1 task 4
hours46 ECG, EDA, face
video, audioThird-
personContinuous
boundedArousal, valence 6 23
GAME-ON
[45]Yes Social
game1 game 11.5
hours51 Video, audio,
and motion
capture dataFirst-
personDiscrete
(5–9-step)Emotions, cohesion,
warmth, competence,
competitivity,
leadership, and
motivationself-report 5
MUMBAI
[46]Yes Board-
game6
games46
hours58 Gameplay, facial
video, and facial
action unitsFirst-
person and
Third-
personDiscrete
labelsValence, attention,
gameplay experience,
personality56 (Third-
person) 58
(First-
person)6
MazeBall
[47]Yes Videogame 1 game N/A 36 BVP(HRV), EDA,
game telemetryFirst-
personPairwise Fun, challenge,
frustration, anxiety,
boredom, excitement,
relaxationself-report 1
PED [48] Yes Videogame 1 game 6
hours58 Gaze, head
position, game
telemetryFirst-
personDiscrete
(5-step),
pairwiseEngagement,
frustration, challengeself-report 1
FUNii [49] Yes Videogame 2
gamesN/A 190 ECG, EDA, gaze
and head
position,
controller inputFirst-
personContinuous,
discreteFun (cont.), fun,
difﬁculty, workload,
immersion, UXself-report 2
AGAIN Yes Videogame 9
games37
hours124 Game video,
game telemetryFirst-
personContinuous
unboundedArousal self-report 9
A table entry is indicated with ‘N/A’ and ‘UNK’ if it is not available and unknown, respectively.2174 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:58 UTC from IEEE Xplore.  Restrictions apply. dataset is planned to be augmented through more affective
dimensions and enriched through more games. The result-
ing dataset leverages the strength of active emotion elicita-
tion while producing data in volumes comparable to
databases featuring non-interactive affect stimuli. Moreover
AGAIN provides a diverse database for general player
affect modelling research that is not possible within any of
the existing corpora.
We position AGAIN at the intersection of traditional
affective computing corpora and datasets with a focus on
interactive emotion elicitation. By focusing on a core affect
dimension (i.e., arousal) instead of task-related complex
emotional outcomes, we aim to make the dataset more rele-
vant to traditional AC research. We argue that the use of
video games as interactive elicitors combined with tradi-
tional affective labels can also help bridge the gap between
AC and games user research. As games are highly interac-
tive media, the captured data and annotations encode not
merely player affect but also behaviour and game context.
We focus on self-reported labels to better capture the subjec-
tive intricacies of gameplay. Finally, we choose to record
continuous unbounded traces of arousal using RankTrace [8]
via the PAGAN online annotation framework [7]. Such
traces can be processed and machine learned in a number of
ways including regression, classiﬁcation and relational
learning [9].4G AMES
Nine games, across three different genres, were designed
and developed as affect elicitors speciﬁcally for the AGAIN
dataset. We put careful consideration to create software
which is aesthetically pleasing, representative of popular
sub-genres of games, can be understood immediately with a
basic level of game literacy [51], and produces a coherent
and consistent dataset without the need of heavy pre-proc-
essing. The game genres were selected (racing, shooters,
platformers) because they represent a good cross-section of
the game genres [52] and are among the most popular
among gamers [24], [53], but also because they have simple
enough controls and clear mechanics so that players can
pick them up quickly. Opposed to other genres, such as role
playing or strategy games, that require longer time invest-
ment and players to learn the speciﬁc mechanics, strategies
and synergies, the games in the dataset relied on fast-paced
genres and popular tropes to communicate the game rules
as fast as possible. Speciﬁc games were designed under
each genre are representative of the genre.
4.1 Racing
The racing genre is characterised by fast-paced driving
against a number of opponents in a given track. The dyn-
amics of the experience is partly dictated by the limited
Fig. 2. Start screens of the nine games included in the AGAIN dataset, showing the game’s rules and players’ controls.MELHART ET AL.: AROUSAL VIDEO GAME ANNOTATION (AGAIN) DATASET 2175
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:58 UTC from IEEE Xplore.  Restrictions apply. interaction with opponent vehicles (e.g., pushing into each
other) but mainly deﬁned by the track itself. In the AGAIN
database, three games are representing distinct sub-genres
of racing games. TinyCars is an arcade-style racer with an
isometric view (see Fig. 2a). Its controls are the hardest to
master due to the drifting of the player’s car. Solid is a more
traditional rally game, with more realistic handling (see
Fig. 2b). As the player sees the track from the driver’s seat,
adapting to the turns of the track is more challenging. Apex-
Speed is a speed-racer type game, with minimalist controls
(see Fig. 2c). While the player only has to change lanes (the
vehicle accelerates and follows the track automatically), the
game has a faster pace than other racing games and addi-
tional elements are complicating the track (i.e., speed boost
platforms and obstacles).
4.2 Shooter
The shooter genre focuses on eliminating opponents using
projectile weapons. The gameplay dynamic of these games
builds on hand-eye coordination and it is characterised by
periods of suspense and periods of engagement with the in-
game opponents. Shooter games in the AGAIN dataset pro-
vide examples of different shooter sub-genres. Heist! is a
typical ﬁrst-person shooter game with similar mechanics to
modern shooters (see Fig. 2d). Because the player has to
wait for their health to regenerate, the play experience is
broken up into smaller engagements. In contrast, TopDown
has a top-down view, an automatic weapon, and health
pickups (see Fig. 2e). This provides a more action-packed
environment as the player is not encouraged to stop if they
are low on health. These two games also feature less linear
maps compared to other games in the dataset. Shootout on
the other hand does not feature traversal at all. In this game
the player can only aim and shoot as the screen is ﬁlled with
more and more enemies (see Fig. 2f). This dynamic of even-
increasing intensity is typical of arcade-style games (includ-
ing shooters).
4.3 Platformer
The platformer genre focuses on traversal and often
requires precision and dexterity. The platformer games fea-
tured in the AGAIN dataset are the most diverse set of
games. Endless is an endless-runner, a popular mobile-game
genre. In these games, the player moves forward automati-
cally at an ever-increasing pace while they have to attack or
dodge incoming obstacles (see Fig. 2g). Subsequently, End-
less is one of the most frantic games in the dataset. Pirates!
is a classical platformer, akin to Super Mario Bros (Nin-
tendo, 1985) (see Fig. 2h). This game has a more relaxed
pace as the gameplay is focused on light platform puzzles
and simple traversal. Finally, Run’N’Gun is a shoot-em up
game, which has the characteristics of both a platformer
and shooter game (see Fig. 2i). Thanks to the shooting
mechanics, number of enemies, and enemy projectiles, this
game has a more intense gameplay loop compared to the
other platformer games.
5 AGAIN D ATASET
Games in the AGAIN dataset were built for the WebGL
platform and are played in a web-browser. The games wereintegrated into the PAGAN annotation platform [7], which
allowed the large-scale crowd-sourcing of both the game
playing and annotation tasks.
5.1 Protocol
The collection procedure took anywhere between 45 to 55
minutes and followed by a stimulated recall protocol [54].
Participants were invited through Amazon’s Mechanical
Turk service3and were compensated with 10 USD for their
time. The only criterion for participation was prior purchase
of video games, in order to ﬁlter out potential subjects who
might not have the game literacy required to play the
games. Participants were greeted with an introduction
screen (see Fig. 3), which informed them about the overall
task and explained arousal as a feeling of tension, excitement,
exhilaration or readiness and the opposite of boredom, calmness
or relaxation . The experiment consisted of 9 rounds, each
round consisting of 2 minutes of game-play followed by 2
minutes of annotation. Due to the high cognitive load of
video game play, the annotations could not be collected at
the same time as the game play telemetry. To mitigate this
issue, a stimulated recall technique was used. Participants’
gameplay was captured and played back to them during
the annotation process. The collection procedure was set up
in an iterative manner with participants playing for 2
minutes, then annotating their gameplay video for 2
minutes. The order of the games was randomised and this
procedure was repeated until all games were played and
annotated. After the experiment, participants ﬁlled in a sim-
ple exit-survey recording their biographical data and gam-
ing habits.
5.2 Participants
Through the procedure presented in Section 5.1, we col-
lected data from 124 participants4which include 1,116
Fig. 3. Introduction screen of the experiment.
3.https://requester.mturk.com/
4.While 169 participants completed the data collection process, 45
participants were omitted as their experiments were incomplete (i.e.,
no video or annotation data) due to software or hardware error.2176 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:58 UTC from IEEE Xplore.  Restrictions apply. gameplay sessions (124 sessions per game) with detailed
telemetry and over 37 hours of gameplay videos. Out of the
124 participants, one identiﬁed as non-binary, 43 as female,
and 80 as male. Participants’ age varied between 19 and 55
years old (average of 33). Most participants were from the
USA (82%); the remaining 22 participants came from Brazil
(10 participants), Italy (3), Canada (2), India (2), Czech
Republic (1), Germany (1), and Romania (1). Most partici-
pants identiﬁed as casual gamers (57%) or hard-core gamers
(36%). Reﬂectively, the majority of participants (87%) were
playing daily or weekly. All participants had either a PC or
a gaming console or both, with the most popular platform
being PC. Participants played very diverse games in their
free time across different genres: from casual games
through platformers, sports simulators, shooters, to role-
playing games. The anonymised demographic data is
included in the dataset.
5.3 Game Footage Videos
For realising ﬁrst person annotation, the gameplay footage
of players had to be recorded and annotated by the players
themselves. As a result the raw AGAIN dataset features
1,116 videos of around 2 mins each (i.e., over 37 hours of
game footage). The video database contains more than 3/C2
106frames of video, which are recorded at 24 FPS and have
a resolution of 960/C2600pixels. Such data can enable future
research that employs computer vision and deep-learning
to directly map pixels to emotions [5]. Previous studies
have shown promise in using general-purpose representa-
tions such as pixel data from game footage [55], and utilis-
ing audiovisual data for learning through privileged
information [56].
5.4 Game Context Features
In addition to the raw video game footage, AGAIN features
a number of hand-crafted attributes for each game. Inspired
by advances in machine learning with privileged informa-
tion [56], [57] we view telemetry data as privileged informa-
tion and we include such ad-hoc features in the dataset.
Privileged information here means information that per-
tains to an experience but not readily available to an
observer. This kind of information generally encodes
domain speciﬁc or hard-to-attain data. The associated cost
of learning the information makes the data valuable in
building expert systems, but poses some limitation to data-
hungry machine learning approaches. When it comes to
video games, privileged information can include player
physiology or game telemetry based on expert heuristics
[56]. Fusing gameplay features with other user modalities
has also been a dominant practice in game-based affective
computing [58], [59]. The game context features described
in this section are considered in the preliminary data analy-
sis of the dataset in Section 5.
All AGAIN games implement the same data-logging
strategy and use a similar method for recording telemetry.
Games within the same genre share the same feature labels.
Not all features, however, have a qualitative meaning for all
games within a genre—for instance, players move in Heist!
but are immobile in Shootout . To ease the data collection and
aggregation process, when features are absent from a gamethey are given values with zero-variance (zeroes or ones,
depending on the feature). For example, a looping racetrack
is only present in the Solid game (see Fig. 2b), therefore the
visible_loop_count feature is always zero in the other
racing games.
Table 3 shows the number of features we have extracted
per game with the zero-variance features removed.
Recorded game telemetry encodes control events initiated by
the player (e.g., player_steering ),player status (e.g.,
player_health ),gameplay events outside of the player’s
control (e.g., bot_aim_at_player ),bot status (e.g.,
bot_offroad ), and the proximal and general game context
(e.g bot_player_distance and pickups_visible ).
Gameplay is recorded at approximately 4Hz (every 250ms).
Due to limitations of the Unity engine and the WebGL for-
mat, the logging rate is not consistent. To mitigate this issue,
the logging script aggregates multiple ticks of the engine’s
update loop and provides an average value. Due to this
processing technique almost all events are represented by
continuous values. For example, pickups_visible can
take ﬂoat values under 1 when a pickup just became visible
at the end of the given 250ms window. The only features
which are represented by integer values are player_-
death and objects_destroyed because of their
sparsity.
In addition to the features enumerated in Table 3, the
dataset includes 14 general gameplay features . These general
features are ad-hoc designed and derived from the game-
speciﬁc events and are based on contemporary studies of
general player modelling [4], [37]. Events which require
expert evaluation of the game such as the goal-oriented and
goal-opposed events of Camilleri et al. [4] are omitted from
these general features of AGAIN, but may be considered as
additional features. Table 4 lists these features alongside
their explanation.
5.5 Annotation
The annotation task was administered through the PAGAN
platform [7], using the RankTrace annotation method [8].
PAGAN is an online annotation platform developed to be
an easy-to-use software for crowdsourcing annotation tasks
with a focus on one-dimensional time-continuous annotation
using three different methods. RankTrace [8], an ordinal
annotation framework, GTrace [60], a bounded annotation
scale which gathers continuous data that can be converted
to a Likert-like format, and BTrace , which is a binaryTABLE 3
Number of Features Extracted Per Game
Genre Sub-Genre Title # Features
Racing Arcade-Racing TinyCars 33
Rally Solid 34
Speed-Racer ApexSpeed 34
Shooter First-Person Shooter Heist! 37
Top-down Shooter TopDown 38
Arcade-Shooter Shootout 23
Platform Endless Runner Endless 33
Mario-Clone Pirates! 39
Shoot’Em’Up Run’N’Gun 47MELHART ET AL.: AROUSAL VIDEO GAME ANNOTATION (AGAIN) DATASET 2177
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:58 UTC from IEEE Xplore.  Restrictions apply. annotation tool for both time-continuous and discrete anno-
tation, inspired by AffectRank [61]. We have chosen Rank-
Trace as our annotation framework for this dataset.
RankTrace allowed us to collect data in an unbounded
fashion (see Fig. 4). Due this collection method, the value
range of the annotation is not bounded between 0 and 1,
which can make it signiﬁcantly harder to use the data for
certain tasks—for example applying regression. However,
this type of data is best interpreted as subjective, ordinal
labels as it preserves the relative relationships between
datapoints [9], and therefore is well suited for preference
learning tasks. The unbounded trace means that users can
always adjust their annotations higher or lower than previ-
ous values, which alleviates much of the guesswork com-
pared to when users annotate on an absolute and objective
scale [59]. The ordinal nature of the annotation follows the
cognitive process of human evaluation, as it provides a trace
which factors in habituation [62], anchoring bias [2], [63]
and recency-effects [64]. To preserve this subjectivity
encoded in the annotation, we apply data transformation
(i.e., normalisation and pairwise transformation) based on
individual sessions.
5.6 Data Cleaning
To ease any subsequent analysis and future studies based
on the dataset, in this section we propose a preprocessing
pipeline which removes 10.8% of the dataset as outliers.
AGAIN contains both the raw and the cleaned data that
result from the process outlined here.
Since PAGAN only records annotations when there is a
change in the signal and the Unity engine loop is affected
by hardware performance, as a ﬁrst step we resample the
whole dataset at 4Hz to get a consistent signal. We remove
duplicate values from the dataset, as well as sessions which
are either too short (less than 1 minute) or too long (more
than 3 minutes) due to software or technical errors during
crowdsourcing. We also prune sessions which have less
than 10 annotation points, assuming that the participant
was unresponsive. This initial cleanup phase removes 24
sessions ( 2:1%of the data).Inspired by Makantasis et al. [55], we apply Dynamic
Time Warping (DTW) to clean the dataset of irregular anno-
tations. DTW is used extensively in time-series analysis as a
distance measure [65], [66], [67]. DTW is an elastic measure
of distance between two signals, which can be of different
length and sampled at different rates [65], [68]. The DTW
distance is calculated based on the similarity matrix between
two time-series, where every point of the two sequences is
matched to each other—with one-to-many mapping where
necessary [68]. While in signal processing DTW is often
applied to synchronise different signals, the cumulative dis-
tance—calculated when ﬁnding the warping path between
signals—provides a reliable similarity measure between the
dynamics of the time-series in question [65], [66]. We apply
the cumulative DTW distance as a similarity measure
between arousal traces, in order to remove irregular annota-
tion patterns; we do not transform any of the signals. It
should be noted that the signals have been synchronised
and resampled, and thus the length and frequency of all
annotation traces is the same.
As a ﬁrst step in the cleanup process, we calculate the
cumulative DTW distance to an artiﬁcial ﬂat baseline
(arousal annotations at 0 in all time windows). The resulting
score provides us with a similarity measure to an artiﬁcial
session where the participant performed no annotation; this
allows us to remove unresponsive outliers. We remove all
sessions which fall more than two standard deviations
closer to zero from the average cumulative distance (the left
tail of the distribution). This step removes 28 additional ses-
sions from the dataset ( 2:5%). Since games in the dataset are
quite short and players encounter similar situations, we
assume that their experience would be somewhat similar.
Therefore, we remove sessions where the annotation traces
are too far from other traces in the dataset. To this end, we
apply the cumulative DTW distance metric between each
datapoint and sum up the resulting distances. This metric
shows us the relative similarity of a session to every other
session. We remove all sessions which fall more than two
standard deviations away from the average summedTABLE 4
The General Gameplay Features of AGAIN
feature description
time_passed time counted from the start of the
recording
score player score
input_intensity number of keypresses
input_diversity number of unique keypresses
idle_time percentage of time spent without
input
activity inverse of idle_time
movement distance travelled + reticle moved (in
shooters)
bot_count number of bots visible
bot_movement bot distance travelled
bot_diversity number of unique bots visible
object_intensity number of objects of interest
object_diversity number of unique objects
event_intensity number of events
event_diversity number of unique events
 Fig. 4. The PAGAN RankTrace annotation interface. The gameplay
video is played in the window above and the participant controls the
annotation cursor (blue circle) below, drawing a visible annotation trace.2178 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:58 UTC from IEEE Xplore.  Restrictions apply. cumulative distance (see Fig. 5). This step removes an addi-
tional 69 sessions ( 6:2%). This last step removes annotations
which are too dissimilar from the general trends of partic-
ipants’ annotations; we presume that either the annotation
was improper or that this session’s elicitor was somehow
not in line with how other players played the same game.
Observing outliers empirically afﬁrms that most partici-
pants whose annotation traces were atypical encountered
issues with game controls, experienced slow-down and
other glitches, or in most cases annotated positive and nega-
tiveevents instead of high and low arousal.
At the end of the cleaning process, 121 sessions—includ-
ing all data from 2 participants—are removed ( 10:8%).
Around 40% of the outliers are removed due to inactivity or
incompleteness, while the rest is held out due to unusual
annotation patterns. The cleaning process proposed in this
section is conservative due to the limitations of the online
collection process, where there is less control over the qual-
ity of annotation. However, the raw dataset is also made
available, which provides opportunities for different proc-
essing methods. The clean dataset consists of 122 partici-
pants and 995 sessions; details on the clean dataset are
provided in Section 6.
6 AGAIN A NALYSIS
Following the cleanup process presented in Section 5.6, this
Section performs a preliminary analysis of the clean version
of the AGAIN dataset, focusing on patterns in the arousal
annotations and the AGAIN game context features (see Sec-
tion 6.1). Section 6.2 describes an initial set of affect model-
ling experiments with this dataset, serving as baseline for
future studies. While some games receive more aggressive
data cleaning than others ( TinyCars, Solid, and Shootout ),overall there is an even distribution of data and sessions
across genres as shown in Table 5.
6.1 Trends in the Data
Fig. 6 shows the average annotation trace as calculated by
averaging values in time windows of 250 ms of all sessions’
traces. The gameplay sessions have been normalised to
show the relative trend in the data. It is evident that arousal
annotation tends to have an upwards tendency. This is not
surprising, as most games considered are action-oriented
with an ever-increasing challenge; for instance, Endless
keeps increasing the speed of the game which evidently
makes it both harder and more arousing as time passes.
Racing games (top row of Fig. 6), on the other hand, tend to
have arousal converging to a maximum mean value after
the ﬁrst 30 seconds. This is likely because the player is ini-
tially rushing to overtake the opponents’ cars (players
always start last); after this initial excitement the race
becomes repetitive, with players trying to either maintain
the lead or slowly catch up to the leader.
Observing the twelve general gameplay features shared
across all nine games, one can detect some notable differen-
ces between games. In terms of the player’s input (control),
games with more complex interaction schemes appear to
have higher input diversity and input intensity (see Table 4
for details on these features). Even accounting for the
games’ different control schemes (i.e., the number of con-
trols the player has available), ApexSpeed, Shootout , and End-
lesshave the lowest intensity (number of keypresses) and
diversity (number of unique keypresses) while Pirates! and
TinyCars have the highest diversity. This discrepancy could
point to an easier control scheme for the former games, but
it could also point to a more frantic and engaging interac-
tion in the latter games. The idle time and activity features
corroborate this observation, as racing games have less idle
time without keypresses (since in two of the games the
player needs to constantly press a button to move forward).Fig. 5. Distribution of summed cumulative DTW distance values of each
session compared to every other session. The solid line shows the aver-
age score, while the dotted lines show the ﬁrst and second standard
deviation. Values in the grey ﬁeld (right tail) are removed during data
cleaning.TABLE 5
Preliminary Analysis of the Clean AGAIN Dataset
Arousal (3 s interval)
Game Sessions Data ð/C1103Þ" # —
TinyCars 109 52.75 543 461 3386
Solid 109 53.42 613 492 3346
ApexSpeed 114 56.10 607 462 3581
Racing 332 162.27 1763 1415 10313
Heist! 110 53.91 580 424 3479
TopDown 115 56.90 650 463 3614
Shootout 106 51.77 471 341 3496
Shooter 331 162.57 1701 1228 10589
Endless 112 55.11 559 438 3595
Pirates! 110 52.26 625 534 3186
Run’N’Gun 110 54.97 618 431 3521
Platformer 332 162.34 1802 1403 10302
Total 995 487.18 5266 4046 31204
The table lists the number of game sessions and their corresponding data points
on a frame-by-frame basis (250 ms). The table also lists the number of 3s time
windows within which the arousal value increases ( "), decreases ( #) or stays
stable within a 10% threshold bound (—).MELHART ET AL.: AROUSAL VIDEO GAME ANNOTATION (AGAIN) DATASET 2179
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:58 UTC from IEEE Xplore.  Restrictions apply. In contrast, games where participants mainly reacted to
stimuli (e.g., in Shootout players react to opponents popping
up and in Endless players move only when a gap or obstacle
is near) featured much higher idle times. In terms of other
features, the number of bots (opponents) visible on the
screen varied wildly between games, with Tiny Cars and
Shootout having the highest number of visible enemies on
average. Perhaps due to the many enemies present, Shootout
had the highest number of events (event intensity in
Table 4), while Solid had the fewest events per time window.
6.2 Preliminary Arousal Models
In this section we provide an initial modelling approach for
the AGAIN dataset, serving as a baseline study for future
research with this dataset. As a preliminary step, we process
the clean AGAIN dataset to predict arousal. To this end, we
split the annotation traces into 3-second time windows—
computing the mean of the window—and introduce a 1-sec-
ond lag to the annotation trace. Our choice of time windows
and lag is motivated by best practices established by a long
line of prior research [4], [8], [61], [69], [70], [71], [72], as
well as empirical results of studies into AC research design.
It has been shown that a 3-second window size is well-
suited to capture valence and arousal changes [73]. In their
experiment on the DEAP dataset [39], Ayata et al. have
shown that affective data processed at this granularity leads
to a higher model performance [73]. Mariooryad and Busso
have shown that while an optimal input lag value can be
found algorithmically, an ad-hoc value between 1 to 3 sec-
onds gives a good approximation of human input lag in AC
annotation tasks [74]. Here we chose a 1-second lag to con-
form to the aforementioned body of research. All features
(including arousal values) are normalised on a per-session
basis to a [0,1] range. This means that feature values of 0
and 1 are indicating the minimum and maximum intensityof a given feature only within a session. This processing
method gives weight to the relative dynamics of features
instead of focusing on the absolute values.
While in the published dataset both clean and raw data is
available for the application of different machine learning
techniques, we treat arousal modelling in AGAIN as a pref-
erence learning task [9], [10], [75] and focus on predicting
arousal change from a 3-second time window to the next.
We apply preference learning through a pairwise transforma-
tion. During this transformation we observe consecutive
datapoints within sessions in pairs and create a new repre-
sentation of the dataset. By describing the difference
between arousal values of time windows, this new repre-
sentation reformulates the preference learning problem as
binary classiﬁcation (arousal increasing or decreasing). For
every ðxi;xjÞ2Xpair of game data we observe the relation-
ship of their affect output ðyi;yjÞ2Y.I fyiis preferred to yj,
we can label the distance between the corresponding data
points ( xi/C0xj) and 1. Conversely, we can label the reverse
of this observation ( xj/C0xi) as 0. While either one of these
observations is sufﬁcient to describe the relationship
between xiand xj, by keeping both observations ( /C21xi/C0xj¼1
and /C21xj/C0xi¼0), we can maintain a 50% baseline accuracy in
the post-transformation dataset independently of the trends
in the dataset before the transformation. While this method
creates redundancies in the training data, it mitigates some
of the issues that arise from the strong temporal patterns
discussed in Section 6.1, as the algorithm is trained on both
increasing and decreasing examples. To reduce experimen-
tal noise from trivial changes within the arousal trace, we
omit all consecutive time windows between which the
arousal change is less than 10% of the total amplitude of the
session’s arousal value. While this 10% threshold is based
on prior experiments in similar problems [18], [76], a more
extensive analysis could explore the impact of the threshold
value on prediction accuracy and the volume of data lost.
As mentioned above, applying this pairwise transforma-
tion to consecutive time windows reformulates the prefer-
ence learning paradigm as binary classiﬁcation. To
construct accessible and simple models of arousal, this ini-
tial study employs a Random Forest Classiﬁer . A Random For-
est (RF) is an ensemble learning method, which operates by
constructing a number of randomly initialised decision trees
and uses the mode of their independent predictions as its
output. Decision trees are simple learning algorithms,
which operate through an acyclical network of nodes that
split the decision process along smaller feature sets and
model the prediction as a tree of decisions [77]. In this paper
we are using the RF implementation in the Scikit-learn
Python library [78]. We initialise RFs with their default
parameters. For controlling overﬁtting we set the number of
estimators in the RF to 100 and the maximum depth of each
tree to 10. This experimental setup is meant to provide a
simple baseline prediction performance for the dataset, and
thus we are not tuning the hyperparameters of the
algorithm.
To examine the validity of the general features discussed
in Section 5.4, models are constructed for each game based
on three different feature sets: 1) game- speciﬁc features
excluding general features 2) general features across games
shown on Table 4 and 3) allfeatures combined. Due to theFig. 6. Average annotation traces (normalised per session) showing an
increasing tendency . The coloured area around the mean depicts the
95% conﬁdence interval of the mean.2180 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:58 UTC from IEEE Xplore.  Restrictions apply. pairwise transformation discussed above, the baseline accu-
racy of all experiments is 50%. Because RFs are stochastic
algorithms, we run each experiment 5 times and we report
the 10-fold cross validation accuracy. Note that each fold
contains the data of 10 to 12 participants and no two folds
contain data from the same participant. The reported statis-
tical signiﬁcance is measured with two-tailed Student’s
t-tests with a¼0:05, adjusted with the Bonferroni correc-
tion where applicable.
Fig. 7 shows the performance of the RF models. Predic-
tion accuracy varies between 58:06% and 82:50% across
games. The results reveal that arousal appears to be easier
to predict in some games (e.g., ApexSpeed, TopDown , and
Endless ) than others (e.g., TinyCars, Shootout , and Run-
’N’Gun ). In the racing and platformer genres, games with
fewer input options and an automatic progression system
(ApexSpeed and Endless respectively) are tied to higher
model performance. An explanation could be that games
with more internal structure (due to the sparsity of actions
the player can take and automatic progression through the
game with minimal input) present a simpler problem. An
exception to this observation is Shootout , in which the con-
trols are limited (only looking around and shooting) and
enemies appearing in an ever-increasing speed, but despite
these similarities with ApexSpeed andEndless ,Shootout mod-
els are struggling to reach 60% accuracy (the lowest perfor-
mance across all games).
Looking at individual games across different feature
sets, we observe that the general features manage to perform
comparably to the speciﬁc features independently of the
game tested. Game-speciﬁc features yield signiﬁcantly
higher performances than general features only in 4 games
(TinyCars, Solid, Endless , and Pirates! ). Moreover, the combi-
nation of both speciﬁc and general features yields signiﬁ-
cantly more accurate arousal models than either the game-
speciﬁc or general features (or both) in 5 games: Solid,
Heist!, TopDown, Endless , and Pirates! . These results demon-
strate the robustness of the general features presented in
Section 5.4 and show that there is little to no trade-off in
representing the presented games in a more abstract and
general manner.The arousal model performances presented in this sec-
tion highlight a number of challenges for future research.
First, the differences in performances between games show
that the complexity of the affect modelling task is depen-
dent on the characteristics of the elicitor and the game con-
text. Finding new processing methods, data treatment,
algorithms, and model architectures which perform equally
well across different games is an open problem. Second, the
robustness demonstrated by the general features proposed
in this paper point towards the possibility of general player
affect modelling across games. While research has already
been investigating general player modelling in video games
[4], early results showed only moderate success. The dataset
and baselines presented in this paper provide a large open
source database of games with robust enough general fea-
tures to continue the exploration of general player
modelling.
7D ISCUSSION
This paper presented the AGAIN dataset, a database for
affect modelling in video games. The dataset contains data
from 124 players and includes game telemetry, gameplay
videos, and arousal annotations of 1,116 gameplay sessions.
The paper also presented the dataset, discussed the under-
lying trends in the data, and showcased some preliminary
preference learning models. In this section, we discuss some
of the limitations and propose avenues for future work,
before concluding the paper.
7.1 Limitations
While the crowd-sourcing protocol for data collection
enabled a larger dataset with high extensibility potential,
most of the limitations of AGAIN stem from the same
crowd-sourced protocol. Collected data lacks modalities tra-
ditionally associated with AC datasets. Neither physiologi-
cal signals nor facial expression data is collected, as the
online procedure focused on behavioural telemetry instead.
While AGAIN features no peripheral signals, the dataset
also contains over 37 hours of gameplay video footage,
which can support a number of computer vision-based
applications [5], [55].
Whereas many affective datasets are composed of multi-
ple affective labels—with arousal and valence being the most
common—the AGAIN dataset focuses only on arousal .A s
the game-playing task is already a lengthy and involved
process, annotating multiple affective dimensions was
infeasible during data collection. The choice of arousal was
motivated by this affective dimension’s strong connection
to the dynamics of gameplay. This is especially important
for ﬁrst-person annotations, as games already encode posi-
tive and negative events (in the form of helpful and detri-
mental effects to the goal of the game) which can be
assessed by third-person annotators later. However, the
subjectively perceived dynamics of the game might differ
from an observer’s impressions.
While each game elicits similar playstyles across differ-
ent participants, the database features unique videos with
self-annotated arousal traces. AGAIN puts an emphasis on
self-reported labels as it is expected to yield ground truths
of affect that are closer to the experience [9], [69], [79]. TheFig. 7. Performance of random forest models of arousal for each game
with game-speciﬁc, general, and all available features. The dotted line
depict the performance baseline and the error bars represent 95% conﬁ-
dence intervals.MELHART ET AL.: AROUSAL VIDEO GAME ANNOTATION (AGAIN) DATASET 2181
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:58 UTC from IEEE Xplore.  Restrictions apply. existing in-game footage of AGAIN, however, can be used
directly for third-person annotation in future studies.
Regardless of the annotation scheme used (ﬁrst versus third
person) AGAIN annotations are captured in an unbounded
fashion which eliminates high degrees of reporting bias [9],
[10].
A necessary but limiting factor is that collection of affect
labels is not concurrent with the collection of video and
telemetry data. Collecting reliable ﬁrst-person affect labels
simultaneously to video game play is impossible due to the
high cognitive demand of the task; however, the stimulated
recall technique applied here does pose some limitations to
the annotation process as certain temporal biases can arise.
Finally, many game sessions in the dataset have an over-
all increasing intensity, which is reﬂected in the correspond-
ing affective labels as well. While this is a limitation of the
dataset, it is also a limitation of the domain. Although the
same increasing intensity is true to even high-budget con-
sole and PC games on the macro level, this dynamic is espe-
cially true to short casual and mobile games played over
short periods.
We note that the presented machine learning models are
quite preliminary, aiming to showcase a use-case for the
dataset along with a proposed cleaning and modelling pipe-
line. Future studies should look into training and tuning
more complex models on AGAIN. However, as the pub-
lished dataset contains both the clean and raw data, future
work can propose different processing methods. While the
presented models show some level of robustness, they do
not use all of the data the dataset has to offer, such as the
captured videos.
7.2 Future Work
The AGAIN dataset was created to facilitate games user
modelling through the lens of affective computing. The data-
set allows for the adoption of machine learning techniques
that use game telemetry and video data to model player
arousal. While a more traditional approach was presented
here, future studies should utilise the available video data-
base and apply deep leaning methods to create more com-
plex models. As the dataset contains a large set of games,
AGAIN is especially useful for research into general affect
models. Future studies should focus on the transferability of
models across different games and genres in the dataset [71].
The current dataset only encodes one affective dimension,
arousal , across videos from nine games; AGAIN, however is
easily scalable to more affective dimensions and more game-
based affect stimuli. Future work will focus on expanding
the labels with expert annotations of valence and dominance
to match the format of other affective computing databases
[39], [41], [43], [44]. Its accessibility and its unobtrusive data
collection via crowdsourcing make AGAIN easily extend-
able to more affect labels, affect elicitors and participants.
8C ONCLUSION
This paper introduced a new database for affect modelling,
the AGAIN dataset. AGAIN is the largest and most diverse
publicly available dataset coupling gameplay context,
gameplay videos, and annotated affect to date. It includes avariety of interactive elicitors, in the form of nine games
from three popular yet dissimilar game genres. In particu-
lar, the dataset consists of 37 hours of video footage accom-
panied by telemetry and self-annotated arousal labels from
1,116 gameplay sessions played by 124 participants. The
motivation behind the construction of this dataset is to facil-
itate and further advance research on general player model-
ling through a clean, large-scale, diverse (elicitor-wise) and
accessible database.
Inspired by recent work on the importance of gameplay
context as a predictor of affect [5], the user modalities of
AGAIN are currently limited to in-game video footage and
behavioural telemetry data. In addition, the protocol of
AGAIN limits the user modalities available so that crowd-
sourcing of self-reported affect annotations is both feasible
and efﬁcient. While AGAIN puts an emphasis on accessibil-
ity—soliciting game context and behavioural data from
users as its modalities—the AGAIN games can be used for
small-scale, lab-based affect studies that incorporate more
user modalities including visual and auditory player cues
(e.g., [48], [55]).
Given the characteristics of a unique set of diverse elici-
tors, a large participant count, ﬁrst-person annotations and
a large-scale video and game telemetry database, AGAIN
couples important aspects of affective computing with core
aspects of game user modelling—thereby enabling research
in the area of general player modelling, in games and
beyond.
REFERENCES
[1] J. J. Prinz, Gut Reactions: A Perceptual Theory of Emotion . London,
U.K.: Oxford Univ. Press, 2004.
[2] A. R. Damasio, Descartes’ Error: Emotion, Rationality and the Human
Brain . New York, NY, USA: Putnam, 1994.
[3] J. Togelius and G. N. Yannakakis, “General general game AI,” in
Proc. IEEE Conf. Comput. Intell. Games , 2016, pp. 1–8.
[4] E. Camilleri, G. N. Yannakakis, and A. Liapis, “Towards general
models of player affect,” in Proc. Conf. Affect. Comput. Intell. Inter-
act., 2017, pp. 333–339.
[5] K. Makantasis, A. Liapis, and G. N. Yannakakis, “From pixels to
affect: A study on games and player experience,” in Proc. Conf.
Affect. Comput. Intell. Interact. , 2019, pp. 1–7.
[6] V. Mnih et al., “Human-level control through deep reinforcement
learning,” Nature , vol. 518, no. 7540, pp. 529–533, 2015.
[7] D. Melhart, A. Liapis, and G. N. Yannakakis, “PAGAN: Video
affect annotation made easy,” in Proc. Conf. Affect. Comput. Intell.
Interact. , 2019, pp. 130–136.
[8] P. Lopes, G. N. Yannakakis, and A. Liapis, “RankTrace: Relative
and unbounded affect annotation,” in Proc. Conf. Affect. Comput.
Intell. Interact. , 2017, pp. 158–163.
[9] G. N. Yannakakis, R. Cowie, and C. Busso, “The ordinal nature of
emotions: An emerging approach,” IEEE Trans. Affect. Comput. ,
vol. 12, no. 1, pp. 16–35, Jan.–Mar. 2021.
[10] G. N. Yannakakis, R. Cowie, and C. Busso, “The ordinal nature of
emotions,” in Proc. Conf. Affect. Comput. Intell. Interact. , 2017,
pp. 248–255.
[11] J. Diehl-Schmid, C. Pohl, C. Ruprecht, S. Wagenpfeil, H. Foerstl,
and A. Kurz, “The Ekman 60 faces test as a diagnostic instrument
in frontotemporal dementia,” Arch. Clin. Neuropsychol. , vol. 22,
no. 4, pp. 459–464, 2007.
[12] C. Westbury, J. Keith, B. B. Briesemeister, M. J. Hofmann, and
A. M. Jacobs, “Avoid violence, rioting, and outrage; approach
celebration, delight, and strength: Using large text corpora to com-
pute valence, arousal, and the basic emotions,” Quart. J. Exp. Psy-
chol., vol. 68, no. 8, pp. 1599–1622, 2015.
[13] J. A. Russell, “A circumplex model of affect,” J. Pers. Social Psy-
chol., vol. 39, no. 6, 1980, Art. no. 1161.2182 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:58 UTC from IEEE Xplore.  Restrictions apply. [14] E. Cambria, A. Livingstone, and A. Hussain, “The hourglass of
emotions,” in Cognitive Behavioural Systems . Berlin, Germany:
Springer, 2012, pp. 144–157.
[15] A. Mehrabian, Basic Dimensions for a General Psychological Theory
Implications for Personality, Social, Environmental, and Developmental
Studies . Cambridge, MA, USA: Oelschlager, Gunn & Hain, 1980.
[16] M. Klarkowski, D. Johnson, P. Wyeth, C. Phillips, and S. Smith,
“Psychophysiology of challenge in play: EDA and self-reported
arousal,” in Proc. CHI Conf. Extended Abstr. Human Factors Comput.
Syst., 2016, pp. 1930–1936.
[17] A. Z. Abbasi, D. H. Ting, H. Hlavacs, L. V. Costa, and A. I. Veloso,
“An empirical validation of consumer video game engagement: A
playful-consumption experience approach,” Entertainment Com-
put., vol. 29, pp. 43–55, 2019.
[18] P. Lopes, A. Liapis, and G. N. Yannakakis, “Modelling affect for
horror soundscapes,” IEEE Trans. Affect. Comput. , vol. 10, no. 2,
pp. 209–222, Apr.–Jun. 2019.
[19] A. Clerico et al. , “Biometrics and classiﬁer fusion to predict the
fun-factor in video gaming,” in Proc. IEEE Conf. Comput. Intell.
Games , 2016, pp. 1–8.
[20] D. Melhart, “Towards a comprehensive model of mediating frus-
tration in videogames,” Game Stud. , vol. 18, no. 1, pp. 1–17, 2018.
[21] J. Seger and R. Potts, “Personality correlates of psychological
ﬂow states in videogame play,” Curr. Psychol. , vol. 31, no. 2,
pp. 103–121, 2012.
[22] C. S.-H. Yeh, “Exploring the effects of videogame play on creativ-
ity performance and emotional responses,” Comput. Hum. Behav. ,
vol. 53, pp. 396–407, 2015.
[23] D. Gabana, L. Tokarchuk, E. Hannon, and H. Gunes, “Effects of
valence and arousal on working memory performance in virtual
reality gaming,” in Proc. Conf. Affect. Comput. Intell. Interact. , 2017,
pp. 36–41.
[24] G. N. Yannakakis and J. Togelius, Artiﬁcial Intelligence and Games .
Berlin, Germany: Springer, 2018.
[25] S. C. Bakkes, P. H. Spronck, and G. van Lankveld, “Player behav-
ioural modelling for video games,” Entertainment Comput. , vol. 3,
no. 3, pp. 71–79, 2012.
[26] J. Pfau, J. D. Smeddinck, and R. Malaka, “Deep player behavior
models: Evaluating a novel take on dynamic difﬁculty adjust-
ment,” in Proc. CHI Conf. Extended Abstracts Hum. Factors Comput.
Syst., 2019, Art. no. LBW0171.
[27] T. Mahlmann, A. Drachen, J. Togelius, A. Canossa, and G. N.
Yannakakis, “Predicting player behavior in Tomb Raider:
Underworld,” in Proc. IEEE Symp. Comput. Intell. Games , 2010,
pp. 178–185.
[28] /C19A. Peri /C19a~nez, A. Saas, A. Guitart, and C. Magne, “Churn prediction
in mobile social games: Towards a complete assessment using sur-
vival ensembles,” in Proc. Conf. Data Sci. Adv. Analytics , 2016,
pp. 564–573.
[29] M. Viljanen, A. Airola, J. Heikkonen, and T. Pahikkala, “Playtime
measurement with survival analysis,” IEEE Trans. Games , vol. 10,
no. 2, pp. 128–138, Jun. 2018.
[30] D. Melhart, A. Azadvar, A. Canossa, A. Liapis, and G. N. Yanna-
kakis, “Your gameplay says it all: Modelling motivation in Tom
Clancy’s the Division,” in Proc. IEEE Conf. Games , 2019, pp. 1–8.
[31] D. Melhart, D. Gravina, and G. N. Yannakakis, “Moment-to-
moment engagement prediction through the eyes of the observer:
PUBG streaming on twitch,” in Proc. Conf. Found. Digit. Games ,
2020, Art. no. 60.
[32] C.-H. Wu, Y.-M. Huang, and J.-P. Hwang, “Review of affective
computing in education/learning: Trends and challenges,” Brit.
J. Educ. Technol. , vol. 47, no. 6, pp. 1304–1323, 2016.
[33] S. D’Mello, A. Kappas, and J. Gratch, “The affective computing
approach to affect measurement,” Emotion Rev. , vol. 10, no. 2,
pp. 174–183, 2018.
[34] H. P. Mart /C19ınez, M. Garbarino, and G. N. Yannakakis, “Generic
physiological features as predictors of player experience,” in Proc.
Conf. Affect. Comput. Intell. Interact. , 2011, pp. 267–276.
[35] N. Shaker, M. Shaker, and M. Abou-Zleikha, “Towards generic
models of player experience,” in Proc. Conf. Artif. Intell. Interactive
Digit. Entertainment , 2015, pp. 191–197.
[36] N. Shaker and M. Abou-Zleikha, “Transfer learning for cross-
game prediction of player experience,” in Proc. IEEE Conf. Comput.
Intell. Games , 2016, pp. 1–8.
[37] V. Bonometti, C. Ringer, M. Ruiz, A. Wade, and A. Drachen,
“From theory to behaviour: Towards a general model of
engagement,” 2020, arXiv:2004.12644 .[38] M. Soleymani, J. Lichtenauer, T. Pun, and M. Pantic, “A multi-
modal database for affect recognition and implicit tagging,” IEEE
Trans. Affect. Comput. , vol. 3, no. 1, pp. 42–55, Jan.–Mar. 2012.
[39] S. Koelstra et al. , “DEAP: A database for emotion analysis using
physiological signals,” IEEE Trans. Affect. Comput. , vol. 3, no. 1,
pp. 18–31, Jan.–Mar. 2012.
[40] Y. Baveye, E. Dellandrea, C. Chamaret, and L. Chen, “LIRIS-
ACCEDE: A video database for affective content analysis,”
IEEE Trans. Affect. Comput. , vol. 6, no. 1, pp. 43–55, Jan.–Mar.
2015.
[41] S. Zafeiriou, D. Kollias, M. A. Nicolaou, A. Papaioannou, G. Zhao,
and I. Kotsia, “Aff-Wild: Valence and arousal ‘in-the-wild’
challenge,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. Work-
shops , 2017, pp. 1980–1987.
[42] A. Mollahosseini, B. Hasani, and M. H. Mahoor, “AffectNet: A
database for facial expression, valence, and arousal computing in
the wild,” IEEE Trans. Affect. Comput. , vol. 10, no. 1, pp. 18–31,
Jan.–Mar. 2019.
[43] J. Kossaiﬁ et al., “SEWA DB: A rich database for audio-visual emo-
tion and sentiment research in the wild,” IEEE Trans. Pattern Anal.
Mach. Intell. , vol. 43, no. 3, pp. 1022–1040, Mar. 2021.
[44] F. Ringeval, A. Sonderegger, J. Sauer, and D. Lalanne,
“Introducing the RECOLA multimodal corpus of remote collabo-
rative and affective interactions,” in Proc. IEEE Conf. Workshops
Autom. Face Gesture Recognit. , 2013, pp. 1–8.
[45] L. Maman et al., “GAME-ON: A multimodal dataset for cohesion
and group analysis,” IEEE Access , vol. 8, pp. 124 185–124 203, 2020.
[46] M. Doyran et al. , “MUMBAI: Multi-person, multimodal board
game affect and interaction analysis dataset,” J. Multimodal User
Interfaces , vol. 15, pp. 373–391, 2021.
[47] G. N. Yannakakis, H. P. Mart /C19ınez, and A. Jhala, “Towards affec-
tive camera control in games,” User Model. User-Adapted Interact. ,
vol. 20, no. 4, pp. 313–340, 2010.
[48] K. Karpouzis, G. N. Yannakakis, N. Shaker, and S. Asteriadis,
“The platformer experience dataset,” in Proc. Conf. Affect. Comput.
Intell. Interact. , 2015, pp. 712–718.
[49] N. Beaudoin-Gagnon et al., “The FUNii database: A physiological,
behavioral, demographic and subjective video game database for
affective gaming and player experience research,” in Proc. Conf.
Affect. Comput. Intell. Interact. , 2019, pp. 1–7.
[50] S. Afzal and P. Robinson, “Natural affect data: Collection and
annotation,” in New Perspectives on Affect and Learning Technologies .
Berlin, Germany: Springer, 2011, pp. 55–70.
[51] D. Buckingham and A. Burn, “Game literacy in theory
and practice,” J. Educ. Multimedia Hypermedia , vol. 16, no. 3,
pp. 323–349, 2007.
[52] J. J. Vargas-Iglesias, “Making sense of genre: The logic of
video game genre organization,” Games Culture , vol. 15, no. 2,
pp. 158–178, 2020.
[53] R. Sevin and W. DeCamp, “Video game genres and advancing
quantitative video game research with the genre diversity score,”
Comput. Games J. , vol. 9, pp. 401–420, 2020.
[54] P. Lankoski and S. Bj €ork, Game Research Methods . Pittsburgh, PA,
USA: ETC Press, 2015.
[55] K. Makantasis, A. Liapis, and G. N. Yannakakis, “The pixels and
sounds of emotion: General-purpose representations of arousal in
games,” IEEE Trans. Affect. Comput. , to be published, Feb. 2021,
doi:10.1109/TAFFC.2021.3060877 .
[56] K. Makantasis, D. Melhart, A. Liapis, and G. N. Yannakakis,
“Privileged information for modeling affect in the wild,” in Proc.
Conf. Affect. Comput. Intell. Interact. , 2021, pp. 1–8.
[57] V. Vapnik and A. Vashist, “A new learning paradigm: Learning
using privileged information,” Neural Netw. , vol. 22, no. 5–6,
pp. 544–557, 2009.
[58] H. P. Mart /C19ınez and G. N. Yannakakis, “Deep multimodal fusion:
Combining discrete events and continuous signals,” in Proc. Conf.
Multimodal Interact. , 2014, pp. 34–41.
[59] H. P. Martinez, G. N. Yannakakis, and J. Hallam, “Don’t classify
ratings of affect; rank them!,” IEEE Trans. Affect. Comput. , vol. 5,
no. 3, pp. 314–326, Jul.–Sep. 2014.
[60] R. Cowie, M. Sawey, C. Doherty, J. Jaimovich, C. Fyans, and
P. Stapleton, “Gtrace: General trace program compatible with
EmotionML,” in Proc. Conf. Affect. Comput. Intell. Interact. , 2013,
pp. 709–710.
[61] G. N. Yannakakis and H. P. Martinez, “Grounding truth via ordi-
nal annotation,” in Proc. Conf. Affect. Comput. Intell. Interact. , 2015,
pp. 574–580.MELHART ET AL.: AROUSAL VIDEO GAME ANNOTATION (AGAIN) DATASET 2183
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:58 UTC from IEEE Xplore.  Restrictions apply. [62] R. L. Solomon and J. D. Corbit, “An opponent-process theory of
motivation: I. Temporal dynamics of affect,” Psychol. Rev. , vol. 81,
no. 2, 1974, Art. no. 119.
[63] B. Seymour and S. M. McClure, “Anchors, scales and the relative
coding of value in the brain,” Curr. Opin. Neurobiol. , vol. 18, no. 2,
pp. 173–178, 2008.
[64] S. Erk, M. Kiefer, J. Grothe, A. P. Wunderlich, M. Spitzer, and H.
Walter, “Emotional context modulates subsequent memory
effect,” Neuroimage , vol. 18, no. 2, pp. 439–447, 2003.
[65] X. Wang, A. Mueen, H. Ding, G. Trajcevski, P. Scheuermann, and
E. Keogh, “Experimental comparison of representation methods
and distance measures for time series data,” Data Mining Knowl.
Discov. , vol. 26, no. 2, pp. 275–309, 2013.
[66] F. Petitjean, G. Forestier, G. I. Webb, A. E. Nicholson, Y. Chen, and
E. Keogh, “Faster and more accurate classiﬁcation of time series
by exploiting a novel dynamic time warping averaging algo-
rithm,” Knowl. Inf. Syst. , vol. 47, no. 1, pp. 1–26, 2016.
[67] H. Li, J. Liu, Z. Yang, R. W. Liu, K. Wu, and Y. Wan, “Adaptively
constrained dynamic time warping for time series classiﬁcation
and clustering,” Inf. Sci. , vol. 534, pp. 97–116, 2020.
[68] D. J. Berndt and J. Clifford, “Using dynamic time warping to ﬁnd
patterns in time series,” in Proc. AAA1 Workshop Knowl. Discov.
Databases , 1994, pp. 359–370.
[69] A. Metallinou and S. Narayanan, “Annotation and processing of
continuous emotional attributes: Challenges and opportunities,”
inProc. IEEE Conf. Workshops Autom. Face Gesture Recognit. , 2013,
pp. 1–8.
[70] D. Melhart, A. Liapis, and G. N. Yannakakis, “I feel I feel you: A
theory of mindexperiment in games,” K€unstliche Intelligenz ,
vol. 34, pp. 45–55, 2019.
[71] D. Melhart, A. Liapis, and G. N. Yannakakis, “Towards general
models of player experience: A study within genres,” in Proc.
IEEE Conf. Games , 2021, pp. 1–8.
[72] C. Pacheco, D. Melhart, A. Liapis, G. N. Yannakakis, and D. Perez-
Liebana, “Trace it like you believe it: Time-continuous believabil-
ity prediction,” in Proc. Conf. Affect. Comput. Intell. Interact. , 2021,
pp. 1–8.
[73] D. Ayata, Y. Yaslan, and M. Kama ¸sak, “Emotion recognition via
random forest and galvanic skin response: Comparison of time
based feature sets, window sizes and wavelet approaches,” in
Proc. IEEE Congr. Med. Technol. Nat. , 2016, pp. 1–4.
[74] S. Mariooryad and C. Busso, “Analysis and compensation of the
reaction lag of evaluators in continuous emotional annotations,”
inProc. Conf. Affect. Comput. Intell. Interact. , 2013, pp. 85–90.
[75] J. F €urnkranz and E. H €ullermeier, “Preference learning,” in Ency-
clopedia of Machine Learning . Berlin, Germany: Springer, 2011,
pp. 789–795.
[76] D. Melhart, K. Sﬁkas, G. Giannakakis, G. N. Yannakakis, and
A. Liapis, “A study on affect model validity: Nominal vs ordinal
labels,” in Proc. IJCAI Workshop AI Affect. Comput. , 2020, pp. 26–33.
[77] R. J. Lewis, “An introduction to classiﬁcation and regression tree
(CART) analysis,” in Proc. Soc. Academic Emerg. Med. Annu. Meet-
ing, 2000, pp. 1–14.
[78] F. Pedregosa et al. , “Scikit-learn: Machine learning in Python,”
J. Mach. Learn. Res. , vol. 12, pp. 2825–2830, 2011.
[79] S. Afzal and P. Robinson, “Emotion data collection and its impli-
cations for affective computing,” in The Oxford Handbook of
Affect. Computing . London, U.K.: Oxford Univ. Press, 2014,
pp. 359–369.
David Melhart received the MA degree in cogni-
tion and communication from the University of
Copenhagen, in 2016, and the PhD degree in
game research from the University of Malta, in
2021. He is a postdoctoral researcher with the
Institute of Digital Games, University of Malta. His
research focuses on machine learning, affective
computing, and games user modelling. He was
the communication chair of FDG 2020, has been
a recurring organiser and publicity chair of the
Summer School series on Artiﬁcial Intelligence
and Games (2018-2022), and currently serves as an editorial assistant
to the IEEE Transactions on Games .
Antonios Liapis received the PhD degree in infor-
mation technology from the IT University of Copen-
hagen, in 2014. He is a senior lecturer with the
Institute of Digital Games, University of Malta,
where he bridges the gap between game technol-
ogy and game design in courses focusing on
human-computer creativity , digital prototyping, and
game development. His research focuses on artiﬁ-
cial intelligence in games, human-computer interac-
tion, computational creativity , and user modelling.
He has published more than 120 papers in the
aforementioned ﬁelds, and has received several awards for his research
contributions and reviewing effort. He serves as associate editor of the
IEEE Transactions on Games , and has served as general chair in four inter-
national conferences, as guest editor in four special issues in international
journals, and has co-organised 12 workshops.
Georgios N. Yannakakis (Senior Member, IEEE)
received the PhD degree in informatics from the
University of Edinburgh, in 2006. He is a professor
and director of the Institute of Digital Games, Uni-
versity of Malta (UM). Prior to joining the Institute
of Digital Games, UM, in 2012, he was an associ-
ate professor with the Center for Computer Games
Research, IT University of Copenhagen. He does
research at the crossroads of artiﬁcial intelligence,
computational creativity, affective computing,
advanced game technology, and human-computer
interaction. He has published more than 300 papers in the aforementioned
ﬁelds and his work has been cited broadly . His research has been sup-
ported by numerous national and European grants (including a Marie
Sk »odowska-Curie Fellowship) and has appeared in Science Magazine
andNew Scientist among other venues. He is currently the editor-in-chief
of the IEEE Transactions on Games and an associate editor of the IEEE
Transactions on Evolutionary Computation , and used to be associate edi-
tor of the IEEE Transactions on Affective Computing and the IEEE Trans-
actions on Computational Intelligence and AI in Games journals. He has
been the general chair of key conferences in the area of game artiﬁcial
intelligence (IEEE CIG 2010) and games research (FDG 2013, 2020).
Among the several rewards, he has received for journal and conference
publications. He is the recipient of the IEEE Transactions on Affective
Computing Most Inﬂuential Paper Award and the IEEE Transactions on
Games Outstanding Paper Award.
"For more information on this or any other computing topic,
please visit our Digital Library at www.computer .org/csdl.2184 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:58 UTC from IEEE Xplore.  Restrictions apply. Training Socially Engaging Robots: Modeling
Backchannel Behaviors with Batch
Reinforcement Learning
Nusrah Hussain , Engin Erzin ,Senior Member, IEEE , T . Metin Sezgin, and Y €ucel Yemez
Abstract— A key aspect of social human-robot interaction is natural non-verbal communication. In this work, we train an agent with
batch reinforcement learning to generate nods and smiles as backchannels in order to increase the naturalness of the interaction and
to engage humans. We introduce the Sequential Random Deep Q-Network (SRDQN) method to learn a policy for backchannel
generation, that explicitly maximizes user engagement. The proposed SRDQN method outperforms the existing vanilla Q-learning
methods when evaluated using off-policy policy evaluation techniques. Furthermore, to verify the effectiveness of SRDQN, a human-
robot experiment has been designed and conducted with an expressive 3d robot head. The experiment is based on a story-shaping
game designed to create an interactive social activity with the robot. The engagement of the participants during the interaction is
computed from user’s social signals like backchannels, mutual gaze and adjacency pair . The subjective feedback from participants and
the engagement values strongly indicate that our framework is a step forward towards the autonomous learning of a socially acceptable
backchanneling behavior .
Index Terms— Human-robot interaction, user engagement, backchannels, batch reinforcement learning
Ç
1I NTRODUCTION
SOCIAL robots are becoming more integrated into our daily
lives, with an increasing prevalence in human environ-
ments such as schools, museums and hospitals. Contrary to
industrial or service robots, these robots work in close prox-
imity to humans and are expected to display social behav-
iors that encourage their acceptance in the human company
[1]. From a broad perspective, the design of a social robot
aims a human-robot interaction (HRI) that is perceived sim-
ilar to a human-human interaction. Several design charac-
teristics contribute to achieving this objective such as
advanced conversation skills, emotion recognition and dis-
play capabilities, user response based behavior adaptation,
ability to develop a social relationship and the potential of
displaying varying social roles [2].
An important characteristic desired from a social robot is
its ability to sustain user engagement [3], [4]. Many have set
engagement as a common goal of human-robot interaction
and a metric to gauge its success [5], [6]. HRI with objectives
such as teaching a new skill, guiding in a public place, and
aiding in physical therapy, are some examples whereengagement is a key design feature. An attentive and engaged
user is more likely to beneﬁt from the service compared to dis-
engaged one. However, engagement is a complex concept,
with numerous proposed deﬁnitions [7]. It is an interdisci-
plinary ﬁeld between social sciences and robotics [8]. While
social scientists try to understand behaviors among humans
that enhance engagement, robotic engineers aim to replicate
these behaviors in a robot.
A key strategy that humans use to engage their interlocu-
tors during interaction is through verbal and non-verbal
cues. While the generation of such cues by the speaker is of
great importance [9], [10], [11], the generation of similar
feedback signals known as backchannels by the listener is
equally important [12], [13], [14], [15], [16]. Backchannels
are non-intrusive social signals generated during the listen-
ing turn [17]. They include facial expressions, body ges-
tures, display of emotions, and verbal expressions such as
’yeah’ and ’hmms’. Several empirical works, in a human-
robot interaction setup, have found that users are more
engaged when interacting with a backchanneling robot [18],
[19]. The common approach for automation of backchannel
behavior in robots is using rule-based methods. Supervised
methods can also be applied if a dataset is created with opti-
mum behavior demonstrations. However, these approaches
do not bring a sense of purpose explicitly into the robot (i.e.,
to engage user). Moreover, the behavior learned from super-
vised learning cannot perform better than the reference
behavior of the dataset. Recent works have demonstrated
the use of online reinforcement learning (RL) for engage-
ment maximization. These works incorporate user’s social
signals to measure engagement and exploit it as the reward
of the RL algorithm [20], [21]. A major issue faced by the
online methods is the necessity of direct interaction/C15The authors are with the KUIS AI Lab., College of Engineering, Koc ¸ Uni-
versity, 34450 Istanbul, Turkey. E-mail: {nhussain15, eerzin, mtsezgin,
yyemez}@ku.edu.tr.
Manuscript received 16 November 2021; revised 24 May 2022; accepted 29
June 2022. Date of publication 13 July 2022; date of current version 15
November 2022.
This work was supported in part by the Scientiﬁc and Technological Research
Council of Turkey (TUBITAK) under Grant 217E040. The work of Nusrah
Hussain was supported by Higher Education Commission (HEC) Pakistan.
(Corresponding author: Nusrah Hussain.)
Recommended for acceptance by J. Broekens.
Digital Object Identiﬁer no. 10.1109/TAFFC.2022.31902331840 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022
1949-3045 ©2022 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See ht _tps://www.ieee.org/publications/rights/index.html for more information.
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:22 UTC from IEEE Xplore.  Restrictions apply. with users during training. The training may be highly time-
consuming and early interactions with an untrained robot
can be frustrating.
We suggest the use of batch reinforcement learning
(batch-RL) to train a robot for engaging behaviors in an off-
line manner. Batch-RL determines optimum solution for
sequential decision-making problems using a batch of tra-
jectories collected by other behaviors, such as an expert
human [22], [23]. The available human-human interaction
datasets represent behavior trajectories when a human
interacts with another. With batch-RL techniques, these
datasets may be exploited to create decision-making
engines. In this work, we aim to train an agent for non-ver-
bal behaviors (smiles and nods) as backchannels which
maximizes user engagement. The human-human interac-
tion dataset is processed for batch-RL, where the rewards
come from user engagement. The nods and smiles (includ-
ing visual and audible laughter in the dataset) are annotated
as actions. The preliminary version of this approach was
presented in our earlier works [24], [25]. Although our pre-
vious work on ofﬂine RL has shown promising results in
terms of learning an engaging backchannel policy, off-pol-
icy methods used for ofﬂine RL are known to suffer from
distributional shift [72]. While the function approximator is
trained under one distribution, it might face a different dis-
tribution when interacting with the environment. This prob-
lem is particularly apparent in the case of backchannel
learning since backchannel events are usually very sparse
in human-human communication and datasets available are
rather biased towards rewarded laughs with almost no
demonstration of negative laughs and of what would hap-
pen for example when backchannels were used in excess.
Hence off-policy RL methods are prone to learning policies
that generate backchannels more frequently than a human
would do.
This paper is an extension of our preliminary work with
the following contributions.
/C15We propose the Sequential Random Deep Q-Network
(SRDQN) as a batch-RL algorithm to train an agent
for non-verbal gestures. The performance of the pro-
posed method is evaluated with ofﬂine evaluation
methods and compared to the existing batch-RL
methods such as neural ﬁtted Q-iterations (NFQ)
[26] and deep Q-network (DQN) [27].
/C15We address the distributional shift problem by
constraining the frequency of backchannels gener-
ated by the RL policy to remain close to the fre-
quency demonstrated in the dataset. We achieve
this by introducing a reward factor which penal-
izes the rewards that result in excessive number of
smiles/nods.
/C15We train our RL agent for two types of backchannels:
nods and smiles. Both backchannels are trained inde-
pendently with the SRDQN algorithm and their per-
formances are compared. Note that, our previous
work considered only smile generation.
/C15We conduct human-robot interaction experiments with
the trained RL-agent. The social robot Furhat is used to
interact with human subjects in a story-shaping game.
During the interaction, the robot generates nods andsmiles as backchannels following its RL-policy. The
results of the user engagement analyses and the feed-
back questionnaire favour the RL trained system over a
baseline rule-based policy.
/C15An important inference from our user study is the
lower acceptability by the users of untimely smiles
compared to nods. This indicates that while nod pol-
icy can be more ﬂexible, the smile policy needs to be
learned closer to the optimal policy.
2R ELATED WORK
Social robots can already be seen as tutors [28], guides in
public places like airports and museums [29], assistants in
work environments [30], healthcare robots for the elderly
[31], [32], and facilitators for children with autism spectrum
disorders [33]. For the central goal of user engagement, sev-
eral approaches have been used in the design of backchannel
behavior. The simplest one deﬁnes rule-based behaviors,
where the decision to trigger a backchannel is conditioned
on the user’s reply. In a study with educational robots [34], a
language learning experiment is conducted. The authors
show that supportive expressions such as ”don’t worry” and
encouraging behaviors like nod/smile at correct answers
improve the learning performance of the students. Similarly,
in another study with a robot, various backchannel strategies
are used during a mathematics test conducted on a tablet
device [35]. The authors show the effectiveness of non-verbal
feedback (nod/shake) after each answer, gaze shift from stu-
dent to tablet and the use of supportive phrases in improving
engagement and test outcomes of the students. Besides tutor-
ing, healthcare social robots for assistance and companion-
ship of the elderly, are becoming increasingly popular [31].
One of the challenges with implementing and designing
healthcare robot is its acceptance by the elderly. In a study
with an assistive activity, the interaction experience of cogni-
tively impaired seniors was investigated with a robot capa-
ble of dynamic facial expressions and gestures [19]. The
results showed that a human-like robot having an expressive
face and arm gestures signiﬁcantly increases levels of
engagement, positive affect, and perceived social intelli-
gence during the interaction.
However, only certain behaviors can be generalized with
a simple rule-based policy; the ones that are observed as
typical among humans. Some behaviors differ signiﬁcantly
from person to person, and hence require more intelligent
behavioral strategies [36]. The authors in [37] use sequential
probabilistic models (e.g., Hidden Markov Models or Con-
ditional Random Fields) with supervised learning to predict
listener backchannels using the speaker multimodal output
feature. In [38], a backchannel is triggered with some proba-
bility either when the user nods, or when a variation is
observed in the pitch of the user’s voice. The authors show
that robots that generate multi-modal backchannels under a
certain probabilistic rule, are perceived as sensitive listeners
and are more competent in sustaining engagement and the
conversational dialog. In another work, the prosody and
pause behaviors of the user are used to construct a rule-
based backchannel policy [39]. Engagement has also been
shown to improve when the robot mirrors the laughs of
user [40]. In [41], listener’s head nods are generated basedHUSSAIN ET AL.: TRAINING SOCIALLY ENGAGING ROBOTS: MODELING BACKCHANNEL BEHAVIORS WITH BATCH REINFORCEMENT ... 1841
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:22 UTC from IEEE Xplore.  Restrictions apply. on a speaker-adaptive prediction model using a corpus of
dyadic interactions, while the work in [42] investigates how
gaze, in addition to prosody, can cue backchannels. There
are also other works in the literature, which aim to learn
backchannel behaviour from recorded human-human inter-
action datasets via supervised learning [43], [44], [45].
Recent works have been exploring reinforcement learn-
ing for the design of more intelligent behavior strategies.
The strength of RL framework lies in the concept of
rewards, which allows the goal of the interaction to be inte-
grated into the formulation. In HRI, the robot behavior has
been optimized for a wide range of objectives. Existing RL-
based works focus on learning empathetic supportive strat-
egies [46], affective behavior [21], human-like greeting
approach [47] and natural interaction distance and gaze
control [48], [49]. While RL is a popular learning framework
in HRI, research works that incorporate engagement and
related social signals as rewards are still scarce. In [20],
online reinforcement learning is used to adapt the personal-
ity of a robot by varying extroversion through linguistic
styles, with the goal of keeping the user engaged in a story-
telling scenario . In this study, engagement is estimated
using head tilt and openness of the body. In the work by
Weber et al. [50], the robot’s sense of humor is adapted to
the user’s preference as an approach to engage and bond
with the user, where the reward signal comes from user’s
smiles and laughs as indicators of engagement. In a lan-
guage tutoring setup [21], facial expressions is exploited to
measure a child’s engagement and use it to adapt robot’s
behaviors.
The success of online reinforcement learning is however
limited in HRI due to the tediousness of interaction with
human. Researchers are now looking for solutions with
batch reinforcement (or ofﬂine reinforcement) learning for
social robots. The potential of batch-RL in learning language
and dialog has been demonstrated on corpora of agent-cus-
tomer transcripts [51] and on collection of human-bot inter-
action data [52]. The popularity of batch-RL is rising in not
only in HRI but also in numerous real-world applications
such as safety-critical healthcare treatment [53], [54], risk-
prone self-driving cars [55], [56], large-scale learning for rec-
ommender systems [57], [58], robot navigation [59], and
grasping tasks [60]. QT-Opt [61] is described as a Q-learning
algorithm that can learn effective vision-based robotic
grasping strategies from hundreds of thousands grasping
trials. Fitted Q-iteration is used in [62] on data collected
from clinical trial involving 1460 patients to learn the opti-
mum treatment options for schizophrenia. However, the
use of batch-RL for engagement is yet to be explored. Batch-
RL faces the key challenge of distributional shift when eval-
uated on unseen data outside of training corpus due to both
the change in visited states for the learned policy and the
act of maximizing the expected return [63]. A common way
to address this problem is to impose constraints on the
learning process. In [64], batch-constrained reinforcement
learning is introduced, which restricts the action space so as
to force the agent towards behaving close to on-policy with
respect to a subset of the given data. In [65], conservative Q-
learning (CQL) is proposed, which aims to learn a conserva-
tive Q-function such that the expected value of a policy
under this Q-function lower-bounds its true value. Anothermethod used is to add a penalty term to the reward function
to avoid action decisions that would result in large devia-
tions from the behavior policy of the dataset [66], [67].
To the best of our knowledge, this work (including our
preliminary papers [24], [25]) is the ﬁrst work on batch rein-
forcement learning for engaging backchannel behavior of a
robot. We also note that a very recent work [68] uses conser-
vative Q-learning as a batch-RL algorithm to learn a back-
channel policy that enhances engagement while statistically
matching the human laughter generation in dyadic conver-
sations. It, however, only trains for laugh events and does
not include any user study that validates the proposed
method on a real human-robot interaction setting.
3M ETHODOLOGY
We formulate the problem of backchannel generation in
HRI as a Markov decision process (MDP), which we solve
with ofﬂine batch-RL using a human-human interaction
dataset as the batch of samples. In this section we describe
our MDP formulation and the Sequential Random Deep Q-
Network (SRDQN) algorithm proposed as a batch-RL
method to address this problem.
3.1 MDP Formulation
Markov decision process is commonly used for stochastic
control problems, and is given by the tuple ðS;A;P;RÞ.
Here, Sis the state space, Ais action space, Pis the state
transition probability function, and Ris the reward func-
tion. In our formulation, at time step t, the user (i.e., the
environment) is in a state st2S, the robot takes a backchan-
neling action at2A and a scalar reward (i.e., engagement)
rt/C24R ð st;atÞis generated by the user. At the next time step,
the user transitions to a new state stþ1/C24P ð st;atÞ. The dis-
count factor g2½0;1Þweighs the future rewards, determin-
ing the temporal extent of the action’s effects. The optimal
solution to the MDP is a backchannel policy pðajsÞwhich
maximizes the expectation of the sum of discounted
rewards, i.e., the overall engagement in our case. The policy
is deﬁned only for listening turns of the robot when it pro-
vides feedback to the speaker (user) with backchannels.
When learning the nod behavior, two actions are deﬁned:
’do nothing’ or ’nod’. Similarly, the actions ’do nothing’ or
’smile’ are described when training for smile behavior.
We represent the user state with a statistical summariza-
tion of Mel-Frequency Cepstrum Coefﬁcients (MFCCs) and
prosody features from the user’s speech. The speech features
are determined at a rate of 40 Hz. MFCCs are computed as a
13-dimensional vector using a 40ms Hamming window. Pros-
ody features are deﬁned as a 6-dimensional vector comprising
of speech intensity, pitch and conﬁdence-to-pitch, along with
their ﬁrst derivates. Both features are concatenated to create a
19-dimensional feature vector at every 25ms time step. The
state of the process is deﬁned from a history of speech fea-
tures. Feature summarization is performed over one second
of speech features (40 samples) by calculating 11 statistical
measures over each dimension. These measures are mean,
standard deviation, skewness, kurtosis, range, minimum,
maximum, ﬁrst quartile, third quartile, median and inter-
quartile range [69]. The resultant 209-dimensional state fea-
ture describes the short-term distribution of the speech1842 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:22 UTC from IEEE Xplore.  Restrictions apply. features. The optimum policy of the robot will deﬁne a nod
(or smile) behavior as backchannel, conditioned on the
user speech features, that maximizes the reward, i.e., user
engagement.
3.2 Engagement Measurement
Engagement can be inferred objectively from various observ-
able features and behavior cues [7]. The simplest methods
adopt single variables as proxies of engagement such as
smiles and laughs [50], or disengagement such as face loca-
tion [70] and negative emotions [71]. The more common
approach, however, is to use several verbal and non-verbal
social signals that arise over a certain time window [72]. Pos-
ture, head tilt and orientation, backchannels and gaze are
some of the cues used to measure engagement [20], [73]. In
this work, we follow the method proposed by Rich et al. for
measuring engagement in a dyadic face-to-face interaction
[74]. The authors deﬁne four connection events (CE) to mea-
sure engagement: backchannels, adjacency pair, mutual
facial gaze, and directed gaze. Backchannels are the feedback
gesture events generated from user during listening turn.
Adjacency pair event is triggered when speech turn-taking
occurs with some minimal time gap. The mutual facial gaze
occurs when there is face-to-face eye contact. Lastly, directed
gaze event is deﬁned when both participants look at a nearby
object related to the interaction at the same time. Engage-
ment EðtÞis then deﬁned as the average number of CEs over
a time window as
EðtÞ¼ht/C0hðt/C0DÞ
D; (1)
where htis the number of connection events occurred until
time tandDis the time window size.
Similarly, we extract the connection events from our
dyadic human-human interaction dataset. To describe back-
channel events, we use smiles, laughs, nods and head
shakes. Adjacency pair and mutual gaze events are also
annotated over the dataset. Since in our dataset, we do not
have objects of interest at which both parties look at, we
exclude directed gaze event from our deﬁnition. Note that
the reward rtin our MDP formulation is given by EðtÞ
which is calculated over a 15 seconds window size in our
experiments to avoid abrupt changes in reward function
since human engagement varies slowly over time.
3.3 Off-Policy Batch Data
Batch reinforcement learning algorithms utilize only previ-
ously collected data logged in the form of tuples
ðst;at;rt;stþ1Þ. Such data-driven ofﬂine methods allow the
manipulation of large datasets for sequential decision mak-
ing problems. There exists no human-human interaction
dataset that speciﬁcally aims to increase engagement by
generating backchannels. Any sufﬁciently large dataset
which contains a variability in engagement values with a
range of annotations for determining the states, actions and
rewards may be used as an ofﬂine batch of transitions. We
work with the IEMOCAP dataset [75], which is designed to
analyze expressive human interactions. It consists of ﬁve
sessions acted by ten professional actors performing dyadic
human-to-human conversations. In total, there are 151dialogues performed in pairs of the opposite gender, on 8
hypothetical scenes and 3 scripted plays. This dataset repre-
sents human behavior in a variety of situations where the
policy behind each dialog may vary. The nod and smile ges-
tures (laughs are included in smiles), are annotated in the
dataset1. The ðst;at;rt;stþ1Þtuples are prepared from the
dataset in order to create an off-policy batch of trajectories.
In each dialog between two actors, the backchannel actions
(nods and smiles) are labeled from one actor and state
(speech features) and rewards (engagement) are determined
from the other. Thus, in terms of the RL scheme the ﬁrst
actor executes actions while second behaves like the interac-
tion environment. Fig. 1 shows the relative time windows
for the extraction of states, actions, and rewards. The dia-
logues are processed at 40 Hz and the consecutive 25ms
time windows are labelled with 1 or 0 to indicate the pres-
ence or absence of the backchannel event at. The state of the
user is described by a history of speech features from one
second window preceding the action. The consequence of
the action atin state stis followed with a reward rt, deter-
mined over a 15 second time window.
3.4 Algorithm: Sequential Random Deep Q-Network
(SRDQN)
We introduce the Sequential Random Deep Q-Network
(SRDQN) as an off-policy batch-RL algorithm for learning
a backchannel policy for robot during human-robot inter-
action. It belongs to the class of Q-learning algorithms
where the distributions for rewards and transition dynam-
ics are not explicitly learned. The quantity of interest is
state-action value function, denoted Qpðs; aÞ, which gives
the expected future return starting from a particular state-
action tuple. The goal of SRDQN is to learn the optimum
Q-value function, Q/C3ðs; aÞ, using only previously collected
transition samples ðst;at;rt;stþ1Þstored in an experience
replay buffer D.
SRDQN estimates the Q-values with a multi-layer per-
ceptron function approximator, as proposed earlier by Ried-
miller in the neural ﬁtted Q-iteration (NFQ) algorithm [26].
SRDQN follows the iterative process of ﬁtting the Bellman
control equation to the samples in D. The Q-values are
Fig. 1. Reinforcement learning formulation of speech driven backchannel
generation (not drawn to scale).
1.Head nod and smile gesture annotations on the IEMOCAP data-
set are available on https://mvgl.ku.edu.tr/databases/HUSSAIN ET AL.: TRAINING SOCIALLY ENGAGING ROBOTS: MODELING BACKCHANNEL BEHAVIORS WITH BATCH REINFORCEMENT ... 1843
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:22 UTC from IEEE Xplore.  Restrictions apply. estimated with neural network Qu, parameterized with
weights u. A separate network ^Qu0determines the target
value, parameterized by u0, that is periodically updated after
every Ngradient updates. The Bellman control equation
then takes a form similar to that proposed by deep Q-net-
work (DQN) algorithm [27], and is given by
Qðst;at;uÞ¼rtþgmax a^Qðstþ1;a;u0Þ: (2)
We make the standard assumption that the future
rewards are discounted by a factor of g¼0:99per time-
step. Given the target value yt¼rtþgmax a^Qðstþ1;a;u0Þ
and the error e¼Qðst;at;uÞ/C0yt, the loss function Lis
deﬁned by the smooth L1 loss function (i.e., Huber loss) as
L¼0:5e2; ifjej<1
jej/C00:5;otherwise/C26
(3)
The ﬁrst key idea in SRDQN is that the capacity Cof the
experience replay buffer is initialized much smaller than
the entire batch created from the dataset. While the IEMO-
CAP dataset has 12 hours of recordings, the replay buffer D
stores 2 minutes of past data. The optimum buffer size was
selected after a series of ofﬂine training at various replay
buffer sizes ranging from 30 seconds to 10 minutes. The
buffer size of two minutes resulted in the lowest Bellman
residual values, hence it was selected. In each update step,
samples from the next mtime steps are pushed into Dand
an update is performed with a minibatch of msamples.
Using replay buffer in this manner results in the sampling
distribution mto change gradually between gradient
updates. This resembles DQN where the sampling distribu-
tion changes gradually as new online samples are pushed
intoD. On the other hand, NFQ randomizes samples over
the entire batch. The actors in our human-human interaction
dataset display a range of emotions, resulting in the under-
lying policies in different scenes to vary considerably. Ran-
domly sampling from the entire batch will result in large
differences in the updates. Target updates from slow vary-
ing sampling distribution using short-term replay buffers
stabilizes the training process.
The second key idea in SRDQN is the use of trajectory
ðs1;a1;r1;...;sm;am;rm;smþ1Þof length mto make anupdate. This is contrary to DQN where an update is made
from random decorrelated samples. The replay buffer Din
SRDQN stores samples sequentially. In each update step, a
memory index iis randomly chosen from D(hence random
of SRDQN). Starting from index i,msequential samples are
selected to create the minibatch (hence sequential of
SRDQN). Thus, each update is performed from sequentially
correlated samples. The problem of HRI is inherently a par-
tially observable problem, where only observations ( o) are
available with some state distribution. Introducing sequen-
tiality into the learning allows the Q-network to better esti-
mate the underlying system state, narrowing the gap
between Qðo; ajuÞandQðs; ajuÞ. The importance of sequen-
tial information in partially observable MDPs has been
demonstrated in the work by Hausknecht et al. on deep
recurrent reinforcement learning [76]. Algorithm 1 summa-
rizes the learning steps.
Algorithm 1. Sequential Random Deep Q-Network (Refer
to Table 1 for Notations)
Initialize replay memory Dto capacity C
Initialize Quwith random weights u
Initialize ^Qu0with weights u0¼u
forepoch ¼1toTdo
while (transitions are available in the batch) do
Get the trajectory of samples from next mtime steps.
Push the msamples to D.
Randomly choose a memory index iofD.
Select sequential msamples starting from index i.
Setyt¼rtþgmax a^Qðstþ1;a;u0Þ.
Determine the smooth L1 loss Lusing Equation (3)
Perform a gradient descent step with respect to u.
Every Nsteps reset ^Q¼Q.
end while
end for
4T RAINING AND EVALUATION
4.1 Ofﬂine Training
A multi-layer perceptron (MLP) is used to model the func-
tion approximation network. Identical architectures are
used for the Q-networks of nod and smile behaviors. The
function approximator takes a 209-dimensional input vectorTABLE 1
List of Hyperparamters and Their Values
Hyperparameter Symbol Value Description
minibatch size m 256 Number of training examples over which gradient descent is performed
replay memory size C 4800 Capacity of Dfrom which a minibatch is sampled
history length H 1 sec Duration of audio data used to deﬁne the states.
target network update
frequencyN 100 Frequency with which target network is updated (measured in number of
parameter updates)
discount factor g 0.99 Discount factor used in Q-learning update
learning rate lr 0.00005 Initial learning rate used by the optimizer
reward factor k 0.9 Factor for scaling rewards that are generated from backchannel actions
optimizer - Adam Optimization algorithm
exponential LR scheduler - 0.99 Exponential factor used to decay the learning rate per epoch
activation function - ReLU Non-linearity between each linear layer
weight initialization - normal
randomInitialization method of neural network parameters1844 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:22 UTC from IEEE Xplore.  Restrictions apply. (state feature size), has two hidden layers with 100 and 25
neurons respectively, and outputs two Q-values for the
actions, ’do nothing’ and ’generate backchannel’ (nod or
smile). ReLU activation function is applied between each
linear layer. The Adam optimizer is used with an exponen-
tially decaying learning rate, to update the network weights
during training. We introduce the hyperparameter reward
factor ( k)for scaling rewards during training. For k/C201, the
rewards of the batch data are modiﬁed such that
r0
t¼rt; ifat¼00do nothing00
krt;ifat¼00generate backchannel00/C26
(4)
Our experiments show that with k¼1, the learned policy
pis dominated with the action ”generate backchannel”
when evaluated with the states available in the dataset. On
the other hand, the behavior policy pb, i.e., the human policy
in the dataset scarcely generates a backchannel ( 2:5%of
actions are nods and 1:3%are smiles). We address this large
deviation of learned policy from the behavior policy by
penalizing the rewards. Various works have used measures
like KL-divergence to determine reward penalty during
learning [66], [67]. We use the simple technique of scaling
down the rewards near smiles and nods, as given in Equa-
tion (4).The value of reward factor kwas selected after a
series of experiments such that backchannels were reduced
in number, yet without becoming fewer than those in the
dataset. In all our experiments, kis set to 0.9. The hyper-
parameters of the training process are given in Table 1.
Three batch reinforcement learning methods are imple-
mented to train models for nod/smile behavior: neural ﬁt-
ted Q-learning (NFQ) [26], deep Q-network (DQN) [27],
and Sequential Random Deep Q-Network (SRDQN). The
dataset is partitioned in the ratio 4:1 for train and evaluation
sets respectively. We perform the training ﬁve times for
each method for 1500 epochs to address the variation seen
in the converged policy when training is repeated. The
greedy action from each Q-network is deﬁned as a¼
argmaxaQðs; aÞ. The ﬁnal backchannel policy is determined
from the votes of each of the ﬁve greedy-policies. A majority
vote of 3 or above triggers a backchannel.
4.2 Ofﬂine Evaluation
Prior to deploying a policy in a human-robot interaction
experiment, it is desirable to ﬁrst understand the policy’s
effectiveness using ofﬂine evaluation metrics. We use fourdifferent types of evaluation metrics: Bellman residual,
backchannel frequency, off-policy policy evaluation and
similarity to human behavior.
4.2.1 Bellman Residual
The Bellman residual [77] is calculated as the mean loss of
Equation (3) over the batch of trajectories B. A Q-function
which satisﬁes the Bellman optimality equation, i.e., for
which the Bellman error is zero for all state-action pairs, is
guaranteed to be optimal, and the optimal policy is
extracted by taking the action with the highest Q-value at a
given state. In batch-RL, due to limited samples and Q-func-
tion approximation with neural networks, the Bellman error
can only approach zero. A smaller residual implies that the
learned policy is closer to the optimal policy and represents
a true Q-function since it follows the Bellman equation
more closely. The Bellman residual is computed for the test
set at every epoch of the training. Figs. 2a and 2c show the
Bellman residuals plotted against the epoch number for
nods and smiles respectively. The average value of the 5
training sessions is represented by the solid line. Each curve
shows a decreasing trend in the error, however, the ﬁnal
error for NFQ stabilizes at a value greater than those of
DQN and SRDQN for both nods and smiles. The results of
this ﬁrst metric indicates an improved performance for
DQN and SRDQN.
4.2.2 Backchannel Frequency
We also consider the fraction of states of the dataset where
the new policy triggers a backchannel. Although the RL
training does not aim to explicitly mimic the human in the
dataset, the frequency of nods and smiles by the trained
agent are expected to be comparable. The NFQ policy pre-
dicts a high occurrence of nods ( 63%) and smiles ( 70%)a t
the end of the training. Figs. 2b and 2d show the ratio of
backchannel event prediction as the training progresses and
highlights the clear disadvantage of the NFQ algorithm.
Although DQN improves the nod policy, it produces com-
parative results for smiles. The SRDQN clearly surpasses
both algorithms and predicts the nods for 13% of the states
and smiles for 30% of the states. These values are closer to
the actual percentage of backchannels in the dataset ( 1:3%
smiles and 2:5%nods). Since the behaviors in the dataset
do not try to explicitly maximize engagement, a higher ratio
of backchannels are expected from the learned RL-policy.
Fig. 2. Bellman residual and percentage of backchannels over the test sets at every epoch of the training.HUSSAIN ET AL.: TRAINING SOCIALLY ENGAGING ROBOTS: MODELING BACKCHANNEL BEHAVIORS WITH BATCH REINFORCEMENT ... 1845
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:22 UTC from IEEE Xplore.  Restrictions apply. 4.2.3 Off-Policy Policy Evaluation
Off-policy policy evaluation (OPE) is a class of methods to
estimate the value of a policy using trajectories collected
from one or more independent behavior policies [78]. We
use the step-wise weighted importance sampling (step-wise
WIS) method to evaluate the learned policies. It handles the
distribution mismatch between the behavior policy(ies) and
the evaluation policy with the importance sampling weight
r[79]. The weight ri
tat time tfor the trajectory tiin the
existing historical data is deﬁned between the evaluation
policy peand the behavior policy pbas
ri
t¼Yt
t0¼0peðai
t0jsi
t0Þ
pbðai
t0jsi
t0Þ: (5)
ForNtrajectories available in the batch, each with hori-
zonT, the normalization factor at time tis deﬁned as wt¼
1
NPN
i¼1ri
t. The value of the evaluation policy under step-
wise weighted importance sampling is then given as
^VðpeÞ¼XN
i¼1XT/C01
t¼0gtri
t
wtrt: (6)
After training, the values of the policies resulting from
NFQ, DQN and SRDQN are evaluated with the step-wise
WIS estimator in Equation (6). Keeping in mind the numeri-
cal limitations, we calculate the step-wise WIS estimates
over trajectory lengths of 250 samples. The WIS technique
evaluates a stochastic policy whereas the greedy policy
resulted from the SRDQN algorithm does not provide
action probabilities. The greedy policy is converted to a sto-
chastic policy by assigning 0.9 probability to “generate
backchannel” action if the greedy policy suggests it, other-
wise 0.1 probability is assigned to this action. Following the
discussion of the work in [80], the behavior policy pbis esti-
mated using approximate nearest neighbor [81]. The off-
policy policy evaluation results obtained from step-wise
weighted importance sampling are shown in Table 2. The
value 24.6 for the ’Dataset’ corresponds to the mean sum of
discounted engagement values (i.e., rewards) seen in the
dataset. This value is determined independent of the
actions. While the value of the NFQ policy is estimated
greater than that of the dataset, DQN policy appears to per-
form worse, more notably with smiles. The SRDQN policy,
however, is expected to accumulate more rewards, com-
pared to the dataset and the other two policies. With a dif-
ference of approximately 5 engagement units, SRDQN
policy is expected to elicit 3 more connection events per
minute. We further extend the OPE analysis with two othercommonly used baseline strategies, supervised learning
(SL) and mirroring. When the agent is trained with a super-
vised learning method [43], the estimated value Vpof the
SL-policy is found to be 22.0 for nods and 21.2 for smiles.
When a mirroring policy is evaluated, that mimics speaker’s
smiles and nods [40], the OPE step-wise WIS method
returns a value of 26.4 for nods and 26.9 for smiles. Hence
SRDQN clearly also outperforms these two baselines in
terms of OPE.
4.2.4 Similarity to Human Behavior
The ﬁnal metric we use is the similarity of backchannel
behaviour to humans in terms of duration of the back-
channel. For example, a policy that results in a nod which
lasts several minutes is unnatural and will result in discom-
fort of the user. We analyze the similarity by observing the
statistical metrics of min, max and mean. Table 3 shows the
statistical similarity of backchannel event duration gener-
ated by nod and smile policies to that of a human. For the
nods, even though the mean nod duration for the NFQ pol-
icy matches the closest with the human nods, it also records
the largest difference in the maximum duration. The maxi-
mum length of nod is noted as 140 seconds, which is socially
unacceptable. The SRDQN policy displays the max duration
closest to the human policy for nods. Note that, the mini-
mum duration is bounded below in our deﬁnition of the RL
formulation. We observe even poorer result for smiles with
NFQ and DQN policies, with very large chunks of sequen-
tial smiles. The SL policy, on the contrary, takes fewer nod
and smile decisions when compared to the dataset, giving
smaller max and mean values for nod/smile duration. The
statistics for mirroring policy is identical to the dataset pol-
icy, so it is not included here. Overall, it is observed that
SRDQN outperforms in both measures of max and mean.
5U SERSTUDY
We designed a human-robot interaction (HRI) experiment
to assess the effectiveness of the learned backchannel policy
in keeping the participants engaged. Since the backchannel
policy is implemented in the listening turn of the robot, the
interaction is designed to motivate the participants to speak.
The interaction is built on a story-shaping game where a
story unfolds according to the path selected by the player.
We aim to evaluate the interaction by noting the engage-
ment indicating signals triggered during the interaction
(Section 3.2), and thorough a feedback questionnaire ﬁlled
by the participants.TABLE 2
Off-Policy Policy Evaluation with Step-Wise Weighted Impor-
tance Sampling
Estimated Vp
Dataset NFQ DQN SL Mirror SRDQN
Nods 24.6 24.8 24.2 22.0 26.4 29.8
Smiles 24.6 26.5 20.0 21.2 26.9 27.8TABLE 3
Statistics of Smile & Nod Duration (Sec)
Nods Smiles
Min Max Mean Min Max Mean
Dataset 0.22 7.1 1.20 0.08 5.6 0.92
SL 0.10 1.42 0.42 0.1 4.97 0.55
NFQ 0.10 140 1.50 0.10 120 2.40
DQN 0.10 21 0.84 0.10 160 2.01
SRDQN 0.10 6.3 0.33 0.10 13 0.771846 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:22 UTC from IEEE Xplore.  Restrictions apply. The experiment is designed with the back-projected
robot head Furhat [82], [83]. The hierarchical state machine
implementation of Furhat facilitates the design of a conver-
sation. A backchannel generation policy triggers smile and
nod gestures through Furhat’s event handlers. The practical
implementation of nods and smiles is limited by the capabil-
ities of Furhat. The time taken by Furhat to execute one nod
is 0.5 seconds (the motion pattern of neutral-down-up-neu-
tral). Therefore, the nod events shorter than 0.5 seconds are
triggered once in our implementation. We deﬁne a ‘cool
down’ time of 2 seconds to be used following the execution
of each backchannel, during which no action is taken by the
robot. The same implementation method is practiced with
smiles, but with a single smile event taking 2 seconds to
complete with smooth gradual changes in the lip curvature.
Two models for backchannel behavior are implemented:
baseline rule-based policy and test RL policy. The rule-
based policy is deﬁned such that the backchannels (nods
and smiles) are generated at ﬁxed time intervals. The test
RL policy is selected to be the SRDQN trained policy which
shows the best performance with the ofﬂine evaluation
metrics.
5.1 Experimental Design
5.1.1 Setup
The experiment is designed as a two-person game with the
robot leading the interaction. The language of communica-
tion is English. During the game, Furhat addresses one
participant at a time. Since the players only respond to the
robot and do not talk to each other, the experience is like a
one-on-one interaction similar to the training dataset. The
design of the interaction with a pair of participants was
preferred because sharing the experience with another
u s e rc a nh e l pt oi n c r e a s et h ec o m f o r to ft h ep a r t i c i p a n t s .
Moreover, an inactive participant may be motivated by
his/her active partner in getting more involved in the
experiment. Some snapshots of the experiment with the
participants are shown in Fig. 3. In this setup, the two par-
ticipants sit on ﬁxed chairs next to each other and face Fur-
hat around a circular table. A webcam is set in front of the
participants to capture their video and a camcorder is
placed behind them to record the robot’s behavior. Two
separate microphones are used for audio recordings of
each player.
5.1.2 Procedure
A total of 26 participants took part in this user study, as
pairs forming 13 groups. However, one pair faced tech-
nical issues during the experiment (the details will be
given in Section 5.2.2) and hence that experiment was
discarded. Among the remaining 24 participants, there
were 12 females and 12 males, with ages ranging from
20 years to 40 years and mean age 27/C65years. An
amount equivalent to $7 was given to each participant as
reimbursement for the study. When a group was called
t ot a k ep a r ti nt h ee x p e r i m e n t ,t h ep a r t i c i p a n t sw e r eﬁ r s t
briefed with a short presentation. The presentation
described the robot, the concept of a story-shaping game
a n dt h a tt h e yw o u l db ep l a y i n gt w og a m e s .T h e yw e r e
also briefed about the purpose of the experiment whichwas to assess which interaction with the robot was more
engaging. The participants we re blind to the backchannel
policies implemen ted in the robot.
Each group was called to play the game twice consecu-
tively on the same day, where each game differed in the
generation policy of backchannels. In the baseline game
(game-RB), a rule-based policy was implemented to gener-
ate backchannels in the listening mode: After a random
wait period in range (0,2] seconds, either a smile or a nod
was triggered with equal probability. The two backchannels
were then alternated every 2 seconds till the end of the lis-
tening turn. Since the responses had a mean time length of
4.8 seconds, the average number of backchannels per turn
was around 2. The backchannel generated at the end of the
user’s turn served as an end-of-turn feedback signal to the
user. In the test game with RL agent (game-RL), the nod RL-
policy and smile RL-policy were executed independently
during the listening turns of Furhat. Each policy was
learned with SRDQN and consisted of an ensemble of ﬁve
Q-networks. As explained in Section 4.1, the ﬁnal decision
for backchannel was made using the majority vote from the
greedy decisions of the Q-networks. Half of the groups
received the game-RB as their ﬁrst game and the other half
received the game-RL ﬁrst.
After the initial brieﬁng, the participants were left alone
in the room to run through the game. At the beginning of
the interaction, Furhat introduced itself and displayed some
of its capabilities such as winking and nodding. It then
asked the question ’Do you think I am a well-designed
robot? What do you think of me?’. This initial interaction
was not part of the experiment and aimed to make the
participants more comfortable and prepare them for the
experiment. After the initial warm-up, the story-shaping
experiment started. A video recording of the experiment
Fig. 3. Human-robot interaction experiment with the robot Furhat and two
participants.HUSSAIN ET AL.: TRAINING SOCIALLY ENGAGING ROBOTS: MODELING BACKCHANNEL BEHAVIORS WITH BATCH REINFORCEMENT ... 1847
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:22 UTC from IEEE Xplore.  Restrictions apply. was saved for post-experiment feedback2. After the experi-
ments, the participants were asked to watch the recordings
and provide their feedback by ﬁlling a questionnaire. The
average time spent by participants in the ﬁrst round was
7.08 minutes, and the second round was 4.85 minutes. The
ﬁrst round was longer because a warm-up session was con-
ducted before the experiment.
5.1.3 Story Shaping Game
The story-shaping game is a survival adventure sketched on
a desert island where the captain and a sailor are stuck after
a shipwreck. The plot is designed using Furhat’s dialog
ﬂow structure as a binary tree, with each node representing
a scene from the story with two possible paths. Concealed
behind a partition, a single trained staff controls the story
transition based on the participants’ responses using a Wiz-
ard of Oz interface. To enrich the interaction, along with
decision questions, Furhat also asks discussion questions
(e.g., the rationale behind their decisions, how they feel). In
the instances when Furhat is in listening mode, the back-
channel policy takes the decision of generating a nod or
smile. The game tree is 5 levels deep, with half of the leaves
resulting in surviving the island. An example dialogue
between the participants and Furhat is shown in Table 4.
5.1.4 Architecture
The implementation of the RL backchannel policy during
the experiment is shown in Fig. 4. The input data stream is
the audio captured from the microphone of the participant
to whom Furhat is currently attending. The audio signal is
ﬁrst processed to extract audio features at a rate of 40 Hz
(Section 3.1). A buffer stores the audio features from past
one second of data. The ﬁnal state vector is created from the
statistical measures as described in Section 3.1. The fre-
quency for the backchannel decision is selected to be 4Hz.
At this rate, the nod and smile policies output an action
decision during Furhat’s listening mode based on the
state vector. Additionally, Furhat receives the audio stream
along with the webcam’s video stream, for its key functions
such as multiple user tracking, head pose detection, auto-
matic speech recognition etc. The audio streams from bothparticipants and video streams from the webcam and cam-
corder are logged for post-experiment analysis. To measure
user engagement, the connection events ’mutual gaze’,
’adjacency pair’ and ’backchannels’ are used (refer to Sec-
tion 3.2). The backchannels (laughs, smiles and nods) are
annotated ofﬂine after the experiment from the video
recordings and cross-checked by a second annotator. The
OpenFace software [84] is used to record the mutual gaze
events in real-time and the adjacency pair CE is determined
online using speech events from the Furhat platform.
Finally, user engagement values are calculated at 4 Hz after
all the CEs are either logged online or annotated ofﬂine.
5.2 Experimental Outcome
5.2.1 Measures
The evaluation of the human-robot interaction session is
performed with subject-oriented (i.e qualitative) measures
and object-oriented (i.e quantitative) measures. A user feed-
back questionnaire is used to analyse the experimental out-
come qualitatively. After the players complete the two
games (game-RB & game-RL), they are asked to watch their
own recordings and provide feedback with a questionnaire.
The convenience of revising the interaction experience later,
allows the participants to interact with the robot without
any distractions during the experiment. The questions are
shown in Table 5 and have similarity with those used by
Sidner et al. in their user study [85]. They are divided into
two categories: effectiveness of the policy and reliability of the
experiment . The participants score each question on a 5-point
Likert scale in the following ascending order: strongly dis-
agree, disagree, neutral, agree and strongly agree. The ques-
tions in the category ’reliability of the experiment’ are used
to discard the sessions which are unreliable due to failure of
a component of the experiment. These problems may arise
due to issues in speech detection, Furhat’s head motion/
facial expression capabilities or user understanding of the
game. The results shown here are for the subjects who score
the questions 6 to 9 with a rating greater or equal to 4. The
questions in the category ’effectiveness of the policy’ are
designed to assess the effectiveness of the two policies in
terms of engagement and naturalness.
For the object-oriented analysis, we determine the engage-
ment values of each participant using the annotated laughs,
smiles, nods, and the online logged values of adjacency pairTABLE 4
Sample Dialog Between Furhat and the Players
Furhat [Looks at Player1] Sailor, should we explore around the beach or should we rather go and discover the forest.
Player1 We should go the forest. It is better.
Furhat Somebody seems like a nature lover. [Looks at Player2] Captain, what do you say?
Player2 I agree with Sailor. Let’s discover the forest.
Furhat It might be pretty scary in the forest. Why did you choose to go there?
Player2 Well it might have some food and fresh water.
Furhat Looks like you both agree. [Looks at Player1]. Now we are in the forest. Sailor, what do you hear and see?
Player1 Umm. I hear a lot of birds chirping and there are many trees. And some squirrels are seen on the trees. Oh hey, there
is a monkey.
Furhat Wow. Guys, I hear some animal noises that may lead to meat. [Looks at Player2] But look in the other direction I see
some fruit trees across a river. What do you think? Shall we go towards the river or hunt for animals?
Player2 Let’s go towards the river. We don’t have the tools to kill or capture animals.
2.Clips from the HRI experiment: https://mvgl.ku.edu.tr/demo-
2020-backchannel-generation-with-batch-rl/1848 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:22 UTC from IEEE Xplore.  Restrictions apply. and mutual gaze events. The values are recorded at a rate of
4Hz. Therefore, with each interaction lasting about 5 minutes,
there are on average 1200 engagement values per interaction
for each player. T-test analysis is performed to determine the
signiﬁcance of the results.
5.2.2 Results
As mentioned earlier in Section 5.1.2, one interaction was
discarded based on the ‘reliability of experiment’ questions.
The failure was reported due to a technical problem which
occurred in Furhat’s head motion during the experiment. It
caused confusion in which of the two participants the robot
was addressing. Besides this single discarded experiment,
12 groups (24 participants) participated in the experiment
and successfully completed their interactions.
User Feedback. The 24 participants provided their feed-
back by ﬁlling out the post-experiment questionnaires for
game-RB and game-RL. Fig. 5 shows the medians and inter-
quartile ranges in a box plot of the ratings of each of the ﬁve
questions in the category ’effectiveness of the policy’. Table 6
gives the mean ratings along with their standard deviations
for each question. Overall the participants agree that both
games are engaging, giving high mean scores in Q1 for
game-RL (4.13) and game-RB (3.96). In order to assess each
type of backchannel, Q2 inquires about the time appropri-
ateness of nods and Q3 asks the same for smiles. Game-RLagain received higher mean ratings for Q2 (3.83) and Q3
(3.58), while game-RB had lesser mean scores in Q2 (3.33)
and Q3 (2.75). The average ratings of the smiles generated
by the RL agent improve by a greater margin relative to the
nods. While Q1 inquires in general about engagement, Q4
and Q5 are designed to understand the role of backchannels
in enhancing engagement. In these questions, game-RL
receives better average ratings than game-RB with a greater
margin relative to Q1. The participants perceive the interac-
tion in game-RL as more natural (4.25) relative to game-RB
(3.79). Also, the higher average ratings for game-RL (4.21)
relative to game-RB (3.86) in Q5 shows that the players dis-
play more interest and engagement when they interact with
the RL-agent.
Furthermore, the statistical paired t-test was used to
determine whether the difference between the mean ratings
of game-RB and game-RL was signiﬁcant. The null hypothe-
sis stated that the values from the two groups come from the
same distribution. The paired t-test results for each of the
ﬁve questions are shown in Table 6. Q4 and Q5 have p<
0:05and hence support the signiﬁcance of the RL agent pol-
icy in ensuring more naturalness and engagement during
the interaction. While the results for smiles were signiﬁcant
(Q3), the same was not concluded about nods. As earlier, a
key ﬁnding from this analysis was that nods are less com-
monly perceived as ill-timed and may be accepted in a
wider range of scenarios. On the other hand, smiles need to
Fig. 4. Backchannel policy implementation during robot’s listening turn in the human-robot interaction experiment.
TABLE 5
Post-Experiment Questionnaire Measuring ’Effectiveness of the
Policy’ and ’Reliability of the Experiment’
Effectiveness of the policy
Q1 The interaction was engaging.
Q2 The robot’s nods were timed appropriately.
Q3 The robot’s smiles were timed appropriately.
Q4 The nods and smiles increased naturalness of the
interaction.
Q5 The nods and smiles increased my interest in
conversing with the robot.
Reliability of the experiment
Q6 I knew what I was doing during the game.
Q7 I understood the robot well.
Q8 The robot understood me well.
Q9 The interaction went smoothFig. 5. Box plot for the category ’effectiveness of the policy’ in the post-
experiment questionnaire for the game-RB and game-RL.HUSSAIN ET AL.: TRAINING SOCIALLY ENGAGING ROBOTS: MODELING BACKCHANNEL BEHAVIORS WITH BATCH REINFORCEMENT ... 1849
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:22 UTC from IEEE Xplore.  Restrictions apply. be generated with more caution because context-less smiles
result in a socially unacceptable behavior.
Engagement Metric. The engagement values for the two
types of games (game-RB and game-RL) were compared to
get an objective assessment of our method’s performance.
We deﬁned a running-mean engagement EðtÞto observe
long-term accumulation at time tas
EðtÞ¼ht
t: (7)
where htis the number of CEs until time tin a given
session. Fig. 6 plots the running-mean engagement EðtÞ
a g a i n s tt i m ef o rt h et w ot y p e so fg a m e sa v e r a g e do v e r
all the sessions belonging to that type. The plot for initial
1 5s e c o n d si sn o ts h o w ns i n c ew ed e ﬁ n e dt h eﬁ r s tv a l u e
of engagement over 15 seconds. To address different
interaction lengths, all interactions were truncated to the
length of the shortest interac tion. The difference in the
two curves shows the higher engagement of the partici-
pants playing game-RL.
We also deﬁned a session engagement as a single scalar to
represent engagement in each interaction, represented by
EðTÞwhere Tis the session length. Each of the 48 interactions
was represented with a single session engagement value,
where 24 samples belong to game-RB condition (M = 0.080,
SD = 0.020) and 24 samples to game-RL (M = 0.089,
SD = 0.022). A paired t-test was performed and indicated that
game-RL sessions had signiﬁcantly higher engagement val-
ues,tð23Þ¼2:19;p¼0:04.
Lastly, in order to highlight the difference between the
two types of interaction, we noted the total number of
smiles and nods performed by the participants during the
interactions. This was manually annotated on the videos of
the interactions. It was observed that total 241 smiles were
generated by the users for the RL based policy versus 201
for the rule-based policy. In the case of head nods, these
numbers were almost equal for the two policies.
5.2.3 Discussion
In the design of our experiment, the selection of baseline
policy was an important factor. Our choice was based on
the strengths and limitations of various previous techniques
available in the literature. The rule-based policy employed
in our user study, which is independent of user state, may
look simple and repetitive. Yet, alternating between smiles
and nods during interactions with an average of two back-
channels per speaking turn helped alleviating the repetitivenature of the policy. During our trial experiments, the users
(blind to the policies) reported that they were unable to tell
if there was a policy that triggered events at regular inter-
vals. Another common rule-based policy found in the litera-
ture is generation of a backchannel at the end of each turn.
This technique is inherently part of our rule-based policy.
Other baselines like ‘mirroring policy’ and ‘supervised
learned policy’ are more intelligent behavioral strategies.
However, our rule-based method offers some strengths
over these baselines. For example, in ‘mirroring policy’ a
backchannel is generated by the robot only after a human
generates a backchannel. This puts limitation on the user to
be the initiator of all backchannels. In cases where user is
disengaged and barely backchanneling, the robot too will
not produce any backchannels. In our baseline, it is ensured
that backchannels are generated (on average twice a turn),
irrespective of user’s state. Similarly, as discussed earlier in
the paper, since the IEMOCAP dataset is not explicitly
designed to maximize engagement, a supervised learned
policy will be a weak reference policy. Thus, the rule-based
policy was selected as the baseline which was simple to
implement and was not reported as unnatural or robot-like.
An analysis on the logs of the user study shows that Fur-
hat remains in listening mode for approximately 30.83
minutes over the 12 experiments for testing RL-learned poli-
cies. With the RL-smile policy, a total of 138 smiles were
triggered, giving an average of 4.5 events per minute. The
time gap between two consecutive smile events had min = 2
seconds, max = 11.7 seconds and mean = 4.53 seconds. With
the RL-nod policy, a total of 189 nod decisions were made,
making an average of 6.1 events per minute. The time gap
between two consecutive nods had min = 2 seconds, max =
14.5 seconds and mean = 3.6 seconds. These statistics show
that while a cool-down time of 2 seconds inﬂuences the
minimum gap between identical events, the average gap is
still dominated by the RL policy. It is noteworthy here that
while the cool-down time imposes restrictions between two
smiles or two nods, there is no limitation on time gap
between a smile and a nod.
The feedback questionnaire presented after the experi-
ments provides us with an insight on how the players per-
ceived the robot’s behavior. Since participants ﬁlled the
questionnaire after watching the videos of their interaction,
their opinions about the robot can be thought of as a recol-
lection of the experience. Some important conclusions mayTABLE 6
Statistical Analysis of Questionnaire: Mean, Standard Deviation
& T-Value, P-Value of T-Test
Game-RL Game-RB t-test
M SD M SD t(23) p-value
Q1 4.13 0.68 3.96 0.69 1.16 0.250
Q2 3.83 0.92 3.33 1.05 1.86 0.076
Q3 3.58 0.97 2.75 1.07 3.39 0.003
Q4 4.25 0.79 3.79 0.83 2.54 0.018
Q5 4.21 0.78 3.88 0.80 2.33 0.029
Fig. 6. Running-mean engagement EðtÞaveraged over all sessions.1850 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:22 UTC from IEEE Xplore.  Restrictions apply. still be drawn from the feedback. The ratings from Q1 show
that for both games the participants felt the interaction was
engaging. Even though the t-test gave p>0.05 for the rat-
ings of Q1, the minor non-signiﬁcance is interesting and
encourages further research. An interesting observation
here is the greater tolerance of a simple repetitive policy for
nods relative to a similar policy for smiles. In the rule-based
agent, the smiles score a mean value below the ’neutral’
response mark. This indicates the higher complexity in the
design of smile behaviors. Correspondingly, the ratings of
the smiles generated by the RL agent improve by a greater
margin. On the other hand, the engagement values calcu-
lated from the detection of connection events generated by
the players provide us with an objective estimation of how
engaged the users were during the game. Since each player
experienced both games, and a signiﬁcant difference was
observed in the engagement values for the two types of
games, we can conclude that the game with the RL policy
was perceived as more engaging.
6C ONCLUSION
We have proposed a generation model for non-verbal back-
channel behaviors of a robot, smiles and nods, to engage
humans during HRI. We have demonstrated the use of
recorded human-human interaction data to learn near-opti-
mal policies with batch-RL algorithms. The results of our
user study provide evidence that socially acceptable back-
channel policies can be trained using only ofﬂine data. The
learned policy may also serve as an initial policy for further
online robot learning with humans, and hence the deploy-
ment of a possibly bothersome random policy can be
avoided. The user study also highlights that different back-
channel behaviors, with a similar generation pattern, are
received with different levels of acceptance by users. While
the policy for backchannels like nods is more ﬂexible, more
sensitive behaviors such as smiles demand a more vigorous
training process. Furthermore, a key issue faced while train-
ing with batch-RL for backchannels, is the occurrence of
counterfactual queries which may result in policies favoring
the actions not available in the dataset, hence the distribu-
tional shift problem. To avoid this problem, we propose
SRDQN with constraint as one possible solution for learning
a policy that remains close to the dataset policy. Yet, more
viable solutions that combine constraints on policy learning
and supervision with reinforcement learning need to be
explored to address the distributional shift problem as
future work.
Our ofﬂine formulation for robot backchanneling is a
generic framework for any robot behavior. Hence, this
research work can be extended to train a variety of social
behaviors such as non-verba l behaviors including gaze
control, facial expressions and body gestures, and verbal
backchannels like ‘hmms’ and ‘yeahs’. We need not restrict
ourselves to the goal of engagement, but multiple design
objectives can be aimed. Steinfeld et al. list the common
metrics used in the design of human-robot interaction,
such as persuasiveness, trus t, and compliance [86]. With
accurate quantiﬁcation of these metrics, they may be incor-
porated as the reward function into the reinforcement
learning formulation.We have demonstrated the strength of audio-based state
representation for policy decisions. The state deﬁnition may
be enriched with more complex user signals such as facial
expressions, emotions and verbal content of speech so that
partial observability of the HRI process will be alleviated.
We should however note that the size of the solution space
of an RL problem grows exponentially with each additional
feature describing the state [87]. Hence the challenge of “the
curse of dimensionality” is inevitable. Larger collections of
ofﬂine trajectories, along with RL methods that identify
more signiﬁcant regions of state space, can be used to solve
for MDPs with large solution spaces.
REFERENCES
[1] K. Darling, “Extending legal rights to social robots,” in Proc. We
Robot Conf., Univ. Miami , 2012, pp. 1–25.
[2] T. Fong, I. Nourbakhsh, and K. Dautenhahn, “A survey of socially
interactive robots,” Robot. Auton. Syst. , vol. 42, no. 3/4, pp. 143–
166, 2003.
[3] T. Komatsubara, M. Shiomi, T. Kanda, H. Ishiguro, and N. Hagita,
“Can a social robot help children’s understanding of science
in classrooms?,” in Proc. Int. Conf. Human-Agent Interact. , 2014,
pp. 83–90.
[4] F. Jimenez, T. Yoshikawa, T. Furuhashi, and M. Kanoh, “An emo-
tional expression model for educational-support robots,” J. Artif.
Intell. Soft Comput. Res. , vol. 5, no. 1, pp. 51–57, 2015.
[5] H. L. O’Brien and E. G. Toms, “What is user engagement? A
conceptual framework for deﬁning user engagement with
technology,” J. Amer. Soc. Informat. Sci. Technol. , vol. 59, no. 6,
pp. 938–955, 2008.
[6] S. M. Anzalone, S. Boucenna, S. Ivaldi, and M. Chetouani,
“Evaluating the engagement with social robots,” Int. J. Soc. Robot. ,
vol. 7, no. 4, pp. 465–478, 2015.
[7] K. Doherty and G. Doherty, “Engagement in HCI: Conception,
theory and measurement,” ACM Comput. Surv. , vol. 51, no. 5,
pp. 1–39, 2018.
[8] R. Campa, “The rise of social robots: A review of the recent liter-
ature,” J. Evol. Technol. , vol. 26, no. 1, pp. 106–113, 2016.
[9] A. Gravano and J. Hirschberg, “Backchannel-inviting cues in task-
oriented dialogue,” in Proc. 10th Annu. Conf. Int. Speech Commun.
Assoc. , 2009, pp. 1019–1022.
[10] A. Hjalmarsson and C. Oertel, “Gaze direction as a back-channel
inviting cue in dialogue,” in Proc. Workshop Realtime Conversational
Virtual Agents , 2012, pp. 1–8.
[11] J. J. Lee, C. Breazeal, and D. DeSteno, “Role of speaker cues in
attention inference,” Front. Robot. AI , vol. 4, 2017, Art. no. 47.
[12] C. Clavel, A. Cafaro, S. Campano, and C. Pelachaud, “Fostering
user engagement in face-to-face human-agent interactions: A
survey,” in Toward Robotic Socially Believable Behaving Systems .
vol. 2, Berlin, Germany: Springer, 2016, pp. 93–120.
[13] K. Lambertz, “Back-channelling: The use of yeah and mm to por-
tray engaged listenership,” Grifﬁth Work. Papers Pragmatics Inter-
cultural Commun. , vol. 4, no. 1/2, pp. 11–18, 2011.
[14] L. J. Hess and J. R. Johnston, “Acquisition of back channel listener
responses to adequate messages,” Discourse Processes , vol. 11,
no. 3, pp. 319–335, 1988.
[15] L. C. Miller, R. E. Lechner, and D. Rugs, “Development of conver-
sational responsiveness: Preschoolers’ use of responsive listener
cues and relevant comments,” Devlop. Psychol. , vol. 21, no. 3, 1985,
Art. no. 473.
[16] N. Ward and W. Tsukahara, “Prosodic features which cue back-
channel responses in english and japanese,” J. Pragmatics , vol. 32,
no. 8, pp. 1177–1207, 2000.
[17] V. H. Yngve, “On getting a word in edgewise,” in Proc. Chicago
Linguistics Soc., 6th Meeting , 1970, pp. 567–578.
[18] I. Leite, G. Castellano, A. Pereira, C. Martinho, and A. Paiva,
“Empathic robots for long-term interaction,” Int. J. Soc. Robot. ,
vol. 6, no. 3, pp. 329–341, 2014.
[19] C. Moro, S. Lin, G. Nejat, and A. Mihailidis, “Social robots and
seniors: A comparative study on the inﬂuence of dynamic social
features on human–robot interaction,” Int. J. Soc. Robot. , vol. 11,
no. 1, pp. 5–24, 2019.HUSSAIN ET AL.: TRAINING SOCIALLY ENGAGING ROBOTS: MODELING BACKCHANNEL BEHAVIORS WITH BATCH REINFORCEMENT ... 1851
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:22 UTC from IEEE Xplore.  Restrictions apply. [20] H. Ritschel, T. Baur, and E. Andr /C19e, “Adapting a robot’s linguistic
style based on socially-aware reinforcement learning,” in Proc.
IEEE 26th Int. Symp. Robot Hum. Interactive Commun. , 2017,
pp. 378–384.
[21] G. Gordon et al., “Affective personalization of a social robot tutor
for children’s second language skills,” in Proc. 13th AAAI Conf.
Artif. Intell. , 2016, pp. 1–7.
[22] D. Ernst, P. Geurts, and L. Wehenkel, “Tree-based batch mode
reinforcement learning,” J. Mach. Learn. Res. , vol. 6, no. Apr,
pp. 503–556, 2005.
[23] S. Lange, T. Gabel, and M. Riedmiller, “Batch reinforcement
learning,” in Reinforcement Learning . Berlin, Germany: Springer,
2012, pp. 45–73.
[24] N. Hussain, E. Erzin, T. M. Sezgin, and Y. Yemez, “Speech driven
backchannel generation using deep Q-network for enhancing
engagement in human-robot interaction,” Proc. 20th Annu. Conf.
Int. Speech Commun. Assoc. , 2019, pp. 4445–4449.
[25] N. Hussain, E. Erzin, T. M. Sezgin, and Y. Yemez, “Batch recurrent
Q-learning for backchannel generation towards engaging agents,”
inProc. 8th Int. Conf. Affect. Comput. Intell. Interact. , 2019, pp. 1–7.
[26] M. Riedmiller, “Neural ﬁtted Q iteration–ﬁrst experiences with a
data efﬁcient neural reinforcement learning method,” in Proc. Eur.
Conf. Mach. Learn. , 2005, pp. 317–328.
[27] V. Mnih et al., “Human-level control through deep reinforcement
learning,” Nature , vol. 518, no. 7540, 2015, Art. no. 529.
[28] T. Belpaeme, J. Kennedy, A. Ramachandran, B. Scassellati, and F.
Tanaka, “Social robots for education: A review,” Sci. Robot. , vol. 3,
no. 21, 2018, Art. no. eaat5954.
[29] O. Mubin, M. I. Ahmad, S. Kaur, W. Shi, and A. Khan, “Social
robots in public spaces: A meta-review,” in Proc. Int. Conf. Soc.
Robot. , 2018, pp. 213–220.
[30] I. Leite, C. Martinho, and A. Paiva, “Social robots for long-term
interaction: A survey,” Int. J. Soc. Robot. , vol. 5, no. 2, pp. 291–308,
2013.
[31] H. Robinson, B. MacDonald, and E. Broadbent, “The role of
healthcare robots for older people at home: A review,” Int. J. Soc.
Robot. , vol. 6, no. 4, pp. 575–591, 2014.
[32] D. Feil-Seifer and M. J. Mataric, “Deﬁning socially assistive robot-
ics,” in Proc. 9th Int. Conf. Rehabil. Robot. , 2005, pp. 465–468.
[33] Y. Zhang et al., “Could social robots facilitate children with autism
spectrum disorders in learning distrust and deception?,” Comput.
Hum. Behav. , vol. 98, pp. 140–149, 2019.
[34] M. Saerbeck, T. Schut, C. Bartneck, and M. D. Janse, “Expressive
robots in education: Varying the degree of social supportive
behavior of a robotic tutor,” in Proc. SIGCHI Conf. Hum. Factors
ComputingComput. Syst. , 2010, pp. 1613–1622.
[35] L. Brown, R. Kerwin, and A. M. Howard, “Applying behavioral
strategies for student engagement using a robotic educational
agent,” in Proc. IEEE Int. Conf. Syst., Man, Cybern. , 2013,
pp. 4360–4365.
[36] I. de Kok and D. Heylen, “The multilis corpus–dealing with indi-
vidual differences in nonverbal listening behavior,” in Toward
Autonomous, Adaptive, and Context-Aware Multimodal Interfaces.
Theoretical and Practical Issues . Berlin, Germany: Springer, 2011,
pp. 362–375.
[37] L.-P. Morency, I. de Kok, and J. Gratch, “A probabilistic multi-
modal approach for predicting listener backchannels,” Auton.
Agents Multi-Agent Syst. , vol. 20, no. 1, pp. 70–84, 2010.
[38] M. Schroder et al. , “Building autonomous sensitive artiﬁcial
listeners,” IEEE Trans. Affective Comput. , vol. 3, no. 2, pp. 165–183,
Apr.–Jun. 2011.
[39] K. P. Truong, R. Poppe, and D. Heylen, “A rule-based back-
channel prediction model using pitch and pause information,” in
Proc. 11th Annu. Conf. Int. Speech Commun. Assoc. , 2010.
[40] B. B. T €urker, Z. Buc ¸inca, E. Erzin, Y. Yemez, and M. Sezgin,
“Analysis of engagement and user experience with a laughter
responsive social robot,” in Proc. 18th Annu. Conf. Int. Speech Com-
mun. Assoc. , 2017, pp. 844–848.
[41] I. de Kok, D. Heylen, and L.-P. Morency, “Speaker-adaptive multi-
modal prediction model for listener responses,” in Proc. 15th ACM
Int. Conf. Multimodal Interact. , 2013, pp. 51–58.
[42] K. P. Truong, R. Poppe, I. de Kok, and D. Heylen, “A multimodal
analysis of vocal and visual backchannels in spontaneous dia-
logs,” in Proc. Int. Speech Commun. Assoc. , 2011, pp. 2973–2976.
[43] B. B. Turker, E. Erzin, T. M. Sezgin, and Y. Yemez, “Audio-visual
prediction of head-nod and turn-taking events in dyadic inter-
actions,” in Proc. Int. Speech Commun. Assoc. , 2018, pp. 1741–1745.[44] J. Lee and S. C. Marsella, “Predicting speaker head nods and the
effects of affective information,” IEEE Trans. Multimedia , vol. 12,
no. 6, pp. 552–562, Oct. 2010.
[45] R. Meena, G. Skantze, and J. Gustafson, “Data-driven models for
timing feedback responses in a map task dialogue system,” Com-
put. Speech Lang. , vol. 28, no. 4, pp. 903–922, 2014.
[46] I. Leite, A. Pereira, G. Castellano, S. Mascarenhas, C. Martinho,
and A. Paiva, “Modelling empathy in social robotic companions,”
inProc. Int. Conf. User Model., Adapt., Personalization , 2011,
pp. 135–147.
[47] A. H. Qureshi, Y. Nakamura, Y. Yoshikawa, and H. Ishiguro,
“Show, attend and interact: Perceivable human-robot social inter-
action through neural attention Q-network,” in Proc. IEEE Int.
Conf. Robot. Automat. , 2017, pp. 1639–1645.
[48] N. Mitsunaga, C. Smith, T. Kanda, H. Ishiguro, and N. Hagita,
“Robot behavior adaptation for human-robot interaction based on
policy gradient reinforcement learning,” J. Robot. Soc. Jpn. , vol. 24,
no. 7, pp. 820–829, 2006.
[49] S. Lathuili /C18ere, B. Mass /C19e, P. Mesejo, and R. Horaud, “Neural net-
work based reinforcement learning for audio–visual gaze control
in human–robot interaction,” Pattern Recognit. Lett. , vol. 118,
pp. 61–71, 2019.
[50] K. Weber, H. Ritschel, I. Aslan, F. Lingenfelser, and E. Andr /C19e,
“How to shape the humor of a robot-social behavior adaptation
based on reinforcement learning,” in Proc. 20th ACM Int. Conf.
Multimodal Interact. , 2018, pp. 154–162.
[51] L. Zhou, K. Small, O. Rokhlenko, and C. Elkan, “End-to-end off-
line goal-oriented dialog policy learning via policy gradient,”
2017, arXiv:1712.02838 .
[52] N. Jaques et al., “Way off-policy batch deep reinforcement learning
of implicit human preferences in dialog,” 2019, arXiv:1907.00456 .
[53] H.-H. Tseng, Y. Luo, S. Cui, J.-T. Chien, R. K. Ten Haken, and I. El
Naqa, “Deep reinforcement learning for automated radiation
adaptation in lung cancer,” Med. Phys. , vol. 44, no. 12, pp. 6690–
6705, 2017.
[54] X. Nie, E. Brunskill, and S. Wager, “Learning when-to-treat
policies,” J. Amer. Statist. Assoc. , vol. 116, no. 533, pp. 392–409, 2021.
[55] A. E. Sallab, M. Abdou, E. Perot, and S. Yogamani, “Deep rein-
forcement learning framework for autonomous driving,” Electron.
Imag. , vol. 2017, no. 19, pp. 70–76, 2017.
[56] A. Kendall et al. , “Learning to drive in a day,” in Proc. Int. Conf.
Robot. Automat. , 2019, pp. 8248–8254.
[57] A. Swaminathan et al., “Off-policy evaluation for slate recommen-
dation,” in Proc. Adv. Neural Informat. Process. Syst. , 2017,
pp. 3632–3642.
[58] A. Gilotte, C. Calauz /C18enes, T. Nedelec, A. Abraham, and S. Doll /C19e,
“Ofﬂine A/B testing for recommender systems,” in Proc. 11th
ACM Int. Conf. Web Search Data Mining , 2018, pp. 198–206.
[59] G. Kahn, P. Abbeel, and S. Levine, “BADGR: An autonomous
self-supervised learning-based navigation system,” 2020,
arXiv:2002.05700 .
[60] L. Pinto and A. Gupta, “Supersizing self-supervision: Learning to
grasp from 50K tries and 700 robot hours,” in Proc. IEEE Int. Conf.
Robot. Automat. , 2016, pp. 3406–3413.
[61] D. Kalashnikov et al. , “Scalable deep reinforcement learning for
vision-based robotic manipulation,” in Proc. Conf. Robot Learn. ,
2018, pp. 651–673.
[62] S. M. Shortreed, E. Laber, D. J. Lizotte, T. S. Stroup, J. Pineau, and
S. A. Murphy, “Informing sequential clinical decision-making
through reinforcement learning: An empirical study,” Mach.
Learn. , vol. 84, no. 1/2, pp. 109–136, 2011.
[63] S. Levine, A. Kumar, G. Tucker, and J. Fu, “Ofﬂine reinforcement
learning: Tutorial, review, and perspectives on open problems,”
2020, arXiv:2005.01643 .
[64] S. Fujimoto, D. Meger, and D. Precup, “Off-policy deep reinforce-
ment learning without exploration,” in Proc. Int. Conf. Mach.
Learn. , 2019, pp. 2052–2062.
[65] A. Kumar, A. Zhou, G. Tucker, and S. Levine, “Conservative Q-
learning for ofﬂine reinforcement learning,” Adv. Neural Inf. Pro-
cess. Syst. , vol. 33, pp. 1179–1191, 2020.
[66] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, “Soft actor-critic:
Off-policy maximum entropy deep reinforcement learning with a
stochastic actor,” in Proc. Int. Conf. Mach. Learn. , 2018, pp. 1861–
1870.
[67] M. Geist, B. Scherrer, and O. Pietquin, “A theory of regularized
markov decision processes,” in Proc. Int. Conf. Mach. Learn. , 2019,
pp. 2160–2169.1852 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:22 UTC from IEEE Xplore.  Restrictions apply. [68] O. Z. Bayramo /C21glu, E. Erzin, T. M. Sezgin, and Y. Yemez,
“Engagement rewarded actor-critic with conservative Q-learning
for speech-driven laughter backchannel generation,” in Proc. Int.
Conf. Multimodal Interact. , 2021, pp. 163–168.
[69] A. Metallinou, A. Katsamanis, and S. Narayanan, “Tracking con-
tinuous emotional trends of participants during affective dyadic
interactions using body language and speech information,” Image
Vis. Comput. , vol. 31, no. 2, pp. 137–152, 2013.
[70] D. Bohus and E. Horvitz, “Managing human-robot engagement
with forecasts and... um... hesitations,” in Proc. 16th Int. Conf. Mul-
timodal Interact. , 2014, pp. 2–9.
[71] A. Schmitt, T. Polzehl, and W. Minker, “Facing reality: Simulating
deployment of anger recognition in IVR systems,” in Proc. Int.
Workshop Spoken Dialogue Syst. Technol. , 2010, pp. 122–131.
[72] C. Peters, C. Pelachaud, E. Bevacqua, M. Mancini, I. Poggi, and
U. R. Tre, “Engagement capabilities for ECAs,” in Proc. AAMAS
Workshop Creating Bonds ECAs , 2005, pp. 1–8.
[73] R. Ishii and Y. I. Nakano, “Estimating user’s conversational
engagement based on gaze behaviors,” in Proc. Int. Workshop Intell.
Virtual Agents , 2008, pp. 200–207.
[74] C. Rich, B. Ponsler, A. Holroyd, and C. L. Sidner, “Recognizing
engagement in human-robot interaction,” in Proc. IEEE/ACM 5th
Int. Conf. Hum.-Robot Interact. , 2010, pp. 375–382.
[75] C. Busso et al. , “IEMOCAP: Interactive emotional dyadic motion
capture database,” Lang. Resour. Eval. , vol. 42, no. 4, 2008,
Art. no. 335.
[76] M. Hausknecht and P. Stone, “Deep recurrent Q-learning for par-
tially observable MDPs,” CoRR , vol. 7, no. 1, pp. 1–9, 2015.
[77] L. Baird, “Residual algorithms: Reinforcement learning with func-
tion approximation,” in Proc. Mach. Learn. Proc. , 1995, pp. 30–37.
[78] R. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduc-
tion. Cambridge, MA, USA: MIT press, 2018.
[79] C. Voloshin, H. M. Le, N. Jiang, and Y. Yue, “Empirical study of
off-policy policy evaluation for reinforcement learning,”
2019, arXiv:1911.06854 .
[80] A. Raghu et al., “Behaviour policy estimation in off-policy policy
evaluation: Calibration matters,” 2018, arXiv:1807.01066 .
[81] V. Hyv €onen et al. , “Fast nearest neighbor search through sparse
random projections and voting,” in Proc. IEEE Int. Conf. Big Data ,
2016, pp. 881–888.
[82] S. Al Moubayed, J. Beskow, and G. Skantze, “The Furhat social
companion talking head,” in Proc. 14th Annu. Conf. Int. Speech
Commun. Assoc. , 2013, pp. 747–749.
[83] S. A. Moubayed, G. Skantze, and J. Beskow, “The Furhat back-pro-
jected humanoid head–lip reading, gaze and multi-party inter-
action,” Int. J. Humanoid Robot. , vol. 10, no. 01, 2013, Art. no.
1350005.
[84] T. Baltrusaitis, A. Zadeh, Y. C. Lim, and L. Morency, “OpenFace
2.0: Facial behavior analysis toolkit,” in Proc. IEEE 13th Int. Conf.
Autom. Face Gesture Recognit. , 2018, pp. 59–66.
[85] C. L. Sidner, C. Lee, C. D. Kidd, N. Lesh, and C. Rich, “Explorations
in engagement for humans and robots,” Artif. Intell. ,v o l .1 6 6 ,
no. 1–2, pp. 140–164, 2005.
[86] A. Steinfeld et al. , “Common metrics for human-robot inter-
action,” in Proc. 1st ACM SIGCHI/SIGART Conf. Human-Robot
Interact. , 2006, pp. 33–40.
[87] L. P. Kaelbling, M. L. Littman, and A. W. Moore, “Reinforcement
learning: A survey,” J. Artif. Intell. Res. , vol. 4, pp. 237–285, 1996.
Nusrah Hussain received the BSc degree in
electrical engineering from the University of Engi-
neering and Technology Lahore, in 2010 and the
MS degree in systems engineering from the Paki-
stan Institute of Engineering and Applied Scien-
ces, Islamabad, in 2012. She is currently working
toward the PhD degree with the Electrical & Elec-
tronics Engineering Department, Koc ¸ University,
Istanbul, Turkey . Her research interests include
human-robot interaction, social robotics, audio-
visual signal processing and deep learning.
Engin Erzin (Senior Member, IEEE) received the
BSc, MSc, and PhD degrees from the Bilkent Uni-
versity , Ankara, Turkey, in 1990, 1992 and 1995,
respectively , all in electrical engineering. During
1995-1996, he was a postdoctoral fellow with Sig-
nal Compression Laboratory , University of Califor-
nia, Santa Barbara. He joined Lucent Technologies
in September 1996, and he was with the Consumer
Products for one year as a member of technical
staff of the Global Wireless Products Group. From
1997 to 2001, he was with the Speech and Audio
Technology Group of the Network Wireless Systems. Since January 2001,
he is with the Electrical & Electronics Engineering and Computer Engineer-
ing Departments of Koc ¸ University , Istanbul, Turkey . Engin Erzin is currently
a member of the IEEE Speech and Language Processing Technical Com-
mittee and associate editor for the IEEE Transactions on Multimedia ,h a v -
ing previously served an associate editor of the IEEE Transactions on
Audio, Speech & Language Processing (2010-2014). His research inter-
ests include speech signal processing, audio-visual signal processing,
human-computer interaction and pattern recognition.
T. Metin Sezgin received the graduate summa
cum laude degree with honors from Syracuse
University, in 1999, the MS degree from MIT AI
Lab, in 2001, and the PhD degree from MIT , in
2006. He subsequently joined the Rainbow group
, University of Cambridge Computer Laboratory
as a postdoctoral research associate. He is cur-
rently an associate professor with the College of
Engineering, Koc ¸ University . His research inter-
ests include intelligent human-computer interfa-
ces, and HCI applications of machine learning.
His research has been supported by international and national grants
including grants from DARPA (USA), and Turk Telekom. He is a recipient
of the Career Award of the Scientiﬁc and Technological Research Coun-
cil of Turkey .
Y€ucel Yemez received the BS degree from Mid-
dle East Technical University, Ankara, in 1989,
and the MS and PhD degrees from Bo /C21gazic¸i Uni-
versity, Istanbul, respectively, in 1992 and 1997,
respectively, all in electrical engineering. From
1997 to 2000, he was a postdoctoral researcher
with the Image and Signal Processing Depart-
ment of Telecom Paris (ENST). Since September
2000, he is with the Computer Engineering
Department, Koc ¸ University as a faculty member .
He is currently an associate editor of Elsevier’s
Graphical Models Journal . His research interests include computer
vision, human-computer interaction, machine learning and multimedia
signal processing.
"For more information on this or any other computing topic,
please visit our Digital Library at www.computer .org/csdl.HUSSAIN ET AL.: TRAINING SOCIALLY ENGAGING ROBOTS: MODELING BACKCHANNEL BEHAVIORS WITH BATCH REINFORCEMENT ... 1853
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:22 UTC from IEEE Xplore.  Restrictions apply. Semantic-Rich Facial Emotional
Expression Recognition
Keyu Chen , Xu Yang, Changjie Fan, Wei Zhang , and Yu Ding
Abstract— The ability to perceive human facial emotions is an essential feature of various multi-modal applications, especially in the
intelligent human-computer interaction (HCI) area. In recent decades, considerable efforts have been put into researching automatic
facial emotion recognition (FER). However, most of the existing FER methods only focus on either basic emotions such as the seven/
eight categories (e.g., happiness, anger and surprise ) or abstract dimensions ( valence, arousal, etc. ), while neglecting the fruitful
nature of emotion statements. In real-world scenarios, there is deﬁnitely a larger vocabulary for describing human’s inner feelings as
well as their reﬂection on facial expressions. In this work, we propose to address the semantic richness issue in the FER problem, with
an emphasis on the granularity of the emotion concepts. Particularly, we take inspiration from former psycho-linguistic research, which
conducted a prototypicality rating study and chose 135 emotion names from hundreds of English emotion terms. Based on the 135
emotion categories, we investigate the corresponding facial expressions by collecting a large-scale 135-class FER image dataset and
propose a consequent facial emotion recognition framework. To demonstrate the accessibility of prompting FER research to a ﬁne-
grained level, we conduct extensive evaluations on the dataset credibility and the accompanying baseline classiﬁcation model. The
qualitative and quantitative results prove that the problem is meaningful and our solution is effective. To the best of our knowledge, this
is the ﬁrst work aimed at exploiting such a large semantic space for emotion representation in the FER problem.
Index Terms— Facial emotion recognition, affective computing, image analysis
Ç
1I NTRODUCTION
UNDERSTANDING and recognizing human facial emotional
expressions has been an attractive research topic for
decades, lying in the intersection area of affective science
and human-computer interaction. Despite the natural per-
ception ability that humans obtained from evolution [1], it
is never straightforward for computer-based systems to
sense and interpret emotions from human facial performan-
ces automatically. On one side, the challenge of facial emo-
tion recognition (FER) problem partially comes from the
sophisticated facial muscle system, leading to complicated
facial behaviors w.r.t. individual’s emotional statements,
especially under the in-the-wild uncontrolled conditions.
On the other side, most of the current FER researches only
focus on the abstract level of emotion concepts, but are
struggling to cover the entire emotion space [2] sufﬁciently.Typically, the categorical model is one of the most popular
representations in the FER area, composed of several basic
emotion classes, e.g., happiness, anger and surprise . Depending
on the psychological conceptual ization on speciﬁc natural emo-
tions, multiple emotion theoris ts suggest a variety of category
lists individually [3], [4], [5], [6]. However, due to the highly
abstract manner of such deﬁniti ons, there are some arguable
ambiguities. For example, given two individual emotion terms,
amazement and astonishment , which are both subject to the sur-
prise class [7], their triggered fac ial expressions are obviously
different as amazement is rather positive and close to happiness
while astonishment is more negative and associated with fear
(Fig. 1). Therefore, simply categorizing the various facial
expressions into several abstrac t classes is incapable of repre-
senting the numerous and ﬁne-g rained emotional statements.
To tackle this issue, several annotated FER datasets are
proposed by mixing the basic expressions into compound
ones [8], [9], [10], replacing the discrete representations with
multi-label distributions [11], [12], or enlarging the emotion
sets with a few more classes [13], [14]. Besides, another cate-
gory of methods follows the circumplex emotion modeling
idea [15], whose dimensions are represented by the principle
emotion factors, i.e., valence ,arousal ,dominance , etc. The
shortcoming of the dimensional model comes from its difﬁ-
culty of annotating accurate continuous labels, such as [16],
[17]. Nevertheless, the semantic richness issue of recogniz-
able emotion concepts still remains an open problem, which
is really challenging to the whole FER community.
In this paper, we aim at studying the FER problem on a
semantic-rich level. Different from the previous methods
that simply blend or add more emotion classes to enhance
the FER quality, we thoroughly exploit the linguistic space
and leverage a reasonable lexicon to describe the emotion/C15Keyu Chen, Changjie Fan, Wei Zhang, and Yu Ding are with Netease Fuxi
AI Lab, Beijing 100084, China. E-mail: chern9511@gmail.com, {fanchangjie,
zhangwei05, dingyu01}@corp.netease.com.
/C15Xu Yang is with PALM Lab, Department of Computer Science, Southeast
University (SEU), Nanjing, Jiangsu 211189, China.
E-mail: xuyangseu@ieee.org.
Manuscript received 22 February 2022; revised 19 July 2022; accepted 17
August 2022. Date of publication 24 August 2022; date of current version 15
November 2022.
This work was supported in part by the Key Research and Development Program
of Zhejiang Province under Grant 2022C01011, and in part by the Hangzhou
Science and Technology Ofﬁce through the 2022 Key Artiﬁcial Intelligence Sci-
ence and Technology Innovation Project.
(Corresponding author: Keyu Chen.)
Recommended for acceptance by S. Wang.
This article has supplementary downloadable material available at https://doi.
org/10.1109/TAFFC.2022.3201290, provided by the authors.
Digital Object Identiﬁer no. 10.1109/TAFFC.2022.32012901906 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022
1949-3045 ©2022 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See ht _tps://www.ieee.org/publications/rights/index.html for more information.
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:22 UTC from IEEE Xplore.  Restrictions apply. concepts. Inspired by previous psychological research [7],
we extend the recognizable emotions to an exhaustive set,
covering 135 English words which can semantically
describe most of all distinctive emotional feelings or inner
statements of humankind. From the perspective of psycho-
logical and linguistic research, the 135 words expand an
almost complete semantic atlas of the emotion domain [18],
[19]. Accordingly, we argue that the 135-class emotion
model is desirable for semantic-rich FER research.
Based on the 135-class emotion model, we construct a
large-scale FER dataset in a labor-free manner. First, we use
the 135 emotion terms as class labels, collect more than one
million web images from the internet. Then, we design an
automatic data cleaning process by efﬁciently evaluating
the expression consistency of the collected images. To evalu-
ate the label credibility of our categorical dataset, we set up
a manual veriﬁcation test in which multiple participants are
required to give their judgments on given images and dif-
ferent emotion labels. In this way, we successfully build up
theEmo135 dataset, which contains 135 emotion categories
and 728,946 facial images in total.
Next, we propose a baseline method to validate the feasi-
bility of conducting FER on the semantic-rich representa-
tion. Considering the number of emotions to be recognized,
there inevitably exist synonyms among the 135 emotion
concepts/terms, making it neither reasonable nor possible
to regard these categories as individual sets. Our corre-
sponding solution is to evaluate the cross-label correlations
via two metrics, i.e., computing the word embedding and
facial expression embedding similarity distances. The simi-
larity scores are then transformed into two weight matrices
for storing the correlations among 135 emotion classes.
Finally, we make the weight matrices as prior knowledge
and inject them into the recognition network training softly.
To the best of our knowledge, this is the ﬁrst work aimed
at handling the FER problem with such a large number of
emotion categories. The psychological backing of the uti-
lized 135 emotion concepts makes adequate support on our
claimed semantic richness of the FER problem. In sum, the
contributions of this research are three-fold:
/C15We propose the ﬁrst semantic-rich facial emotional
expression recognition work, with an exhaustiveemotion set including 135 concepts comprehensively
described the entire emotion domain.
/C15We automatically construct a large-scale FER dataset
Emo135 , containing 135 ﬁne-grained emotion catego-
ries and 728,946 facial images. We believe the open-
released dataset could beneﬁt the other research
works in the FER community.
/C15We carefully design a correlation-guided method for
ﬁne-grained facial emotional expression recognition.
The quantitative and qualitative experiment results
indicate that our method can well handle the compli-
cated nature of so many emotions and generate reli-
able FER predictions with rich semantics.
2R ELATED WORK
This section brieﬂy reviews some related literature to our
work, including facial emotion expression representations,
datasets, and automatic recognition methods.
2.1 FER Representation and Dataset
Facial emotional expression embodies non-verbal commu-
nication of our daily life. In order to technically model the
inner emotion statements that are conveyed by facial
expressions, there are three common used emotion repre-
sentations being proposed, including the categorical model
[6], the action unit model [20]), and the circumplex model
[15]. Among these models, the categorical one consisting of
several basic emotion terms is most popular. Typically, it is
deﬁned by seven or eight universal recognizable emotions,
namely neutral, anger, disgust, fear, happiness, sadness, surprise,
contempt, etc. As a matter of fact, most current FER datasets
are built upon these discrete categories, varying on the spe-
ciﬁc deﬁnition of emotion concepts, such as JAFFE [21], CK+
[22], KDEF [23],SFEW [24], FER2013 [25], FER-Wild [26],
AffectNet [16], and Aff-Wild2 [27].
However, until recent years, the basic categorical model
has been challenged for its incapability of modeling ﬁne-
grained emotion variances. The following researches sug-
gest improving the representation capacity of the emotion
model, for example, introducing compound emotion classes
[8] and transferring the discrete emotion labels into continu-
ous distributions [11]. Based on these idea, some novel FER
datasets are proposed, like RAF-DB [10] and EmotioNet [9]
which includes 18 and 23 basic/compound emotion classes
respectively, and RAF-ML [12] with continuous label distri-
bution annotations. Furthermore, the latest research work
tries to extend the emotion concepts to 54 classes and pro-
poses a corresponding dataset F2ED[14].
2.2 Facial Expression Features and Classiﬁers
Image-based facial emotion recognition has been exten-
sively studied for decades. In general, a complete FER
method is composed of two algorithm modules, i.e., feature
extractor and classiﬁer. Traditional FER approaches usually
apply hand-crafted features, such as Gabor Wavelets [28],
Local Binary Patterns [29], and Histogram of Oriented Gra-
dients [30]. With the rapid development of deep learning
techniques, some pre-trained backbones like ResNet [31]
are adopted for extracting high-level features. In terms of
the speciﬁcity of FER tasks, there are also some expression
Fig. 1. Facial image samples belong to the same category ( surprise )
deﬁned by the basic emotion model but with contrastive emotional expres-
sions ( amazement and astonishment ). The obviously different facial
performances indicate the necessity of proposing more ﬁne-grained
representation model to handle the abundant emotion semantics.CHEN ET AL.: SEMANTIC-RICH FACIAL EMOTIONAL EXPRESSION RECOGNITION 1907
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:22 UTC from IEEE Xplore.  Restrictions apply. embedding models [32], [33] which can eliminate the invari-
ant attributes like pose, identity, and image background
from the captured features.
On the other hand, the classiﬁers integrated into the FER
systems have also achieved promising performances in
recent years. To solve the occlusion issue caused by large
poses, the region-based network [34] is proposed with an
attention mechanism. Besides, there are some FER methods
considering the facial priors, such as the muscle moving
masks [35] and the geodesic distance on 3D shapes [36].
Except for the extensive methods [37], [38], [39], [40], [41]
which focus on solving the FER problem independently,
there is also another category of methods trying to explore
the beneﬁts from multi-task settings [42], [43].
3M ETHODOLOGY
In this section, we ﬁrst review the background of our lever-
aged emotion model, which contains 135 lexicon terms rep-
resenting the semantic atlas of the emotion domain. Then
we introduce the data acquisition and processing details
that help us construct a large-scale facial image dataset.
Finally, we propose a baseline approach for ﬁne-grained
facial emotion recognition by considering the cross-label
relationships among the multiple emotions.
3.1 Semantic-Rich Emotion Categories
Emotion knowledge plays an important role in social inter-
action. Without too much training, even infants can natu-
rally perceive and express emotional feelings at a basic
level, e.g., happiness ,fear, and anger . Although numerous
empirical cognitive studies have demonstrated that ordi-
nary people can reliably name the emotions being expressed
from facial images [44], it has been a struggle for psycholo-
gists to agree on a formal semantic structure of the human
emotion space. To efﬁciently associate the cognitive emo-
tions with linguistic descriptions, some emotion theorists
suggest applying the prototype approach [45] to determine
the emotion concepts with a ﬁnite set of words [3].
Depending on the deﬁnition of emotion varieties, there
are different kinds of emotion taxonomies, i.e., emotion rep-
resentations consisting of different sets of semantic terms
(words with speciﬁc emotion meanings). Some research
works focus on the abstract level of emotion episodes,
claiming several universal categories forming the overall
structure of the emotion space, such as the seven or eight
basic emotions [6].
Another branch of methods digs into the language space
to search for every distinctive word representing a particu-
lar emotion concept. It is ﬁrst proposed by Averill who col-
lects 558 English words conveying emotion connotations
[46]. Then the 558 words are further cleaned up by grammar
roots and evaluated with the emotion-sorting study [7]: one
hundred twelve participants make their judgments on each
word, with a prototypical rating from “I deﬁnitely would call
this an emotion.” to“I deﬁnitely would not call this an emotion.”
[7]. Finally, there are 135 words left with high enough rat-
ings, and it is responsible for saying that they extensively
form the semantic space of the emotion domain. Therefore,
in this paper, we refer to the 135 emotion words/categories
as the semantic-rich emotion representation. A full list ofthe 135 words given by Shaver et al. is transcribed in the
appendix section.
We would like to point out that, given the human lan-
guage is a living entity, some recent studies [47], [48], [49]
propose a variety of emotion classes that defend/revise the
Shaver’s model [7]. Nevertheless, as language research goes
on, the ideal emotion semantic atlas shall be updated as well.
3.2Emo135 Dataset
With the semantic-rich emotion representation, we establish
a facial image dataset Emo135 according to the 135 emotion
words. Technically, our dataset construction process
involves two automatic steps, data collection and cleaning.
Data Acquisition. We use the 135 emotion category names
as keywords, accompanying several other sufﬁx words
such as expression ,feeling , and face, to query for web images
with matching titles by internet search engine indexing.
While downloading the valid images from the internet, we
also apply face detection by dlib library2and crop the face
area from the entire image into 224/C2224size. In this way,
we collect 135 image categories with more than one million
facial expression images. An illustration of our data collec-
tion results is shown in Fig. 2.
Data Cleaning. In order to eliminate the noisy samples of
each emotion category (which could be titled or indexed
with wrong words), we design a data post-processing strat-
egy to clean the collected image dataset introduced above.
The basic idea of our strategy is to identify the anomaly
images if their facial expressions are different from the
majority of the belonging class.
Speciﬁcally, we adopt an advanced facial expression
embedding model [33] for expression similarity evaluation.
The advantage of the embedding model is that it can pro-
duce expression embedding codes that are invariant to the
other facial attributes like identities, poses, or image back-
grounds. Even more, it can capture subtle expression varia-
tions between different faces. For example, if two faces have
similar expressions, they will be mapped closely in the
latent space, and vice versa (See Fig. 4). Within that latent
space, we gather the expression embedding codes of all
Fig. 2. Facial image samples of six categories within the Emo135 data-
set. All images are collected from four photo stock websites1.
1.https://www.{bigstockphoto;alamy;photocase;shutterstock}.com
2.http://dlib.net1908 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:22 UTC from IEEE Xplore.  Restrictions apply. images in the same category and evaluate the embedding
density of these codes. Based on the k-nearest neighbors algo-
rithm, we can efﬁciently detect the embedding outliers by
ﬁltering the mean distance between each sample and its K
nearest neighbors with a predeﬁned threshold. The detailed
algorithm steps are described in Algorithm 1. In practice,
the expression embedding codes are lying on a 16-dimen-
sional unit sphere.
While we employing the knn-based data cleaning
method, we ﬁnd it is quite sensitive to the speciﬁed hyper-
parameters, i.e., the neighborhood size Kand the threshold
distance s. Generally, larger neighborhood size and smaller
threshold distance will encourage more strict elimination
policy and thus reduce the dataset size (eliminating even
matched images), and vice versa. To preserve the label qual-
ity as well as the image quantities, we empirically choose
K¼10and the threshold distance s¼0:2.
Algorithm 1. Data Cleaning Process
Input: Image category C¼f IngN
n¼1; Neighbouring number K;
Filter threshold s;
Output: Remove/keep decision on each image sample; Step:
1: Generate expression embedding of each image,
E:In7!Vn,
EðCÞ ¼ f VngN
n¼1.
2: Find the Knearest neighbors for each embedding vector,
KNN ðVn;EðCÞÞ ¼ f VnkgK
k¼1.
3: Calculate the mean distance between Vnand its Knearest
neighbors,
dn¼1
KPK
k¼1kVn/C0Vnkk2.
4: Compare dnwith s,
dn<s: keep image IninC,
dn/C21s: remove image Infrom C.
End
In Fig. 3, we visualize the expression embedding distri-
butions of the single class hope before/after the data clean
process. It can be observed that our designed approach is
signiﬁcantly helpful in terms of reducing data noise and
improving label accuracy to the dataset. Besides, we also
visualize the multi-class expression embedding distribu-
tions before/after the automatic data cleaning procedure in
Fig. 3. After the removal operation, each emotion class is
more compact, which makes it possible for us to analyzetheir correlations. In sum, 31% of original data is automati-
cally cleaned during this process, leaving in total of 728,946
valid facial images.
Dataset Statistics. In Table 1, we compare our proposed
Emo135 dataset with some other existed FER datasets. It can
be observed that our dataset Emo135 has the most ﬁne-
grained annotations in terms of emotion classes and compa-
rable large quantities of facial images.
Besides, we also illustrate the image quantity distribution
per each emotion category in Fig. 5. Considering the differ-
ent emotions may involve different degree of presence in
our daily life, some rareemotion classes (e.g., vengefulness )
generally contain less image samples than those common
ones (e.g., excitement ). Therefore, our dataset distribution is
not absolutely uniform but including the maximum cate-
gory with 12,794 images and the minimum category with
994 images.
3.3 Modeling Correlation Matrix for 135 Emotions
After obtaining the Emo135 dataset, we propose to analyze
the cross-emotion similarities for the 135 emotions. Differ-
ent from the previous FER methods established on only a
few discrete emotion classes, the problem setting of this
work is more challenging since there are as many as 135
emotion categories, and most of them do not have sharp
Fig. 3. Illustration of single-class and multi-calss expression embedding distributions before/after the automatic data cleaning procedure.
TABLE 1
List of Some Existed FER Datasets and the Associated
Characteristics
Dataset Annotation #Image In-the-wild
JAFFE 7 basic class 213 ✗
CK+ 8 basic class 593 ✗
KDEF 7 basic class 4,900 ✗
SFEW 7 basic class 700 ✓
FER2013 7 basic class 35,887 ✓
FER-Wild 7 basic class 24,000 ✓
AffectNet 8 basic class 450,000 ✓
Aff-Wild2 7 basic class 2,800,000 ✓
RAF-ML 7-class distribution 4,908 ✓
RAF-DB 18 basic/compound class 29,672 ✓
EmotioNet 23 basic/compound class 1,000,000 ✓
F2ED 54 ﬁne-grained class 219,719 ✗
Emo135 135 ﬁne-grained class 728,946 ✓CHEN ET AL.: SEMANTIC-RICH FACIAL EMOTIONAL EXPRESSION RECOGNITION 1909
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:22 UTC from IEEE Xplore.  Restrictions apply. boundaries, which means it is even impossible to absolutely
distinguish any emotion term from the others. This phe-
nomenon suggests we have to carefully consider the corre-
lations of different classes. The idea of modeling cross-
emotion distances can date back to the 1980s [15], [50]. Until
recently, the emotion correlations are considered in several
automatic facial affective recognition methods [51], [52],
[53]. In this task, the similarity of different emotions can be
evaluated in two ways, one based on their triggered facial
expressions and the other by the semantic word embed-
dings. Speciﬁcally, the facial expression evaluations focus
on the performance/expressiveness level of different emo-
tions, which is useful for extracting solid facial image fea-
tures. In contrast, the semantic word embeddings are
adopted from the language modeling area, capable of
revealing the intrinsic synonymous distances between dif-
ferent emotion labels. Thus we can make them facilitate the
emotion label prediction process.To begin with, let us denote the 135 emotion categories as
C1;C2; :::;C135. For each category Ckðk¼1;2; :::;135Þ, it con-
tains in total of Nkfacial images In
kðn¼1;2; :::; N kÞ. Because
of some inevitable synonyms like anger and fury existed in
the 135 emotion classes, and even words in hierarchical rela-
tionship like astonishment and amazement which are both
subject to surprise , we are motivated to quantitatively calcu-
late the distances between different emotions and moreover
apply this knowledge to help our network training.
Facial Expression Similarities. We ﬁrst adopt the facial
expression embedding model [33] to evaluate the facial
images between different emotion categories. Speciﬁcally,
we ﬁrst send every image In
kinto the pre-trained facial
expression embedding model and generate the correspond-
ing expression embedding vector Vn
k2R16.
With the expression similarity structure of the embed-
ding space, we are now able to evaluate the cross-emotion
relationships and model the distances among 135 classes.
For emotion category Ci¼fIn
ijn¼1; :::; N igandCj¼fIm
jjm
¼1; :::; N jg, their corresponding embedding vectors are
given as EðCiÞ¼f Vn
ijn¼1; :::; N igand EðCjÞ¼f Vm
jjm¼
1; :::; N jg. We utilize Directed Hausdorff Distance to measure
the one-sided similarity from CitoCj, which can be formu-
lated as following:
dHðCi;CjÞ¼max
nmin
mkVn
i/C0Vm
jk2
2: (1)
Notably, this metric is asymmetric as dHðCi;CjÞdoes not
necessarily equal to dHðCj;CiÞ, and thus it is suitable for the
similarity modeling purpose. This is because sometimes the
emotion Cicould be absolutely recognized as Cj(e.g., amaze-
ment!surprise ) but the inverse is not true (e.g., suprise !
{amazement, astonishment, etc. }).
Next, we pack the calculated results into a facial expres-
sion similarity matrix Fexp2R135/C2135, in which each
Fig. 4. Demonstration of our adopted facial expression embedding
model. By capturing the expression-related features, the embedding
model is capable of mapping similar expression images closely in the
latent space, while enforcing the dissimilar ones away from each other .
Fig. 5. Distribution of image quantities from each emotion category in our constructed Emo135 dataset.1910 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:22 UTC from IEEE Xplore.  Restrictions apply. element Fexp
ijis given as:
Fexp
ij¼1
½dHðCi;CjÞ/C1382ði6¼jÞ: (2)
Particularly, the diagonal element Fexp
iiis computed by
adding the rest entries within the ith row like:
Fexp
ii¼X
j6¼iFexp
ij;i2f1;2; :::;135g: (3)
Finally, we normalize the correlation matrix along each
row into sum 1.0. The technical meaning of our constructed
matrix is that, if emotion Ciis very close to emotion Cj, the
value of element Fexp
ijwould be signiﬁcantly large. In this
way, we can use Fexpto efﬁciently guide the recognition
process to be aware of the cross-emotion relationships.
Word Embedding Distances. The other metric we adopted
for evaluating cross-emotion similarities is the semantic
word embedding. First, we adopt a Word2Vec model [54],
[55] which is pre-trained on large-scale dataset including
English blogs, texts, and comments. Then the model will
take input as every pair of 135 emotion terms and output the
corresponding embedding distance of each. Similar to the
facial expression similarity matrix, the word embedding dis-
tances are stored in a coefﬁcient matrix Fword. For every pair
of emotion categories ðCi;CjÞ, the word embedding distance
is calculated as follows and then normalized in rows as well:
Fword
ij¼Word 2VecðCi;CjÞði6¼jÞ: (4)
Fword
ii¼X
j6¼iFword
ij;i2f1;2; :::;135g: (5)
In practice, we deem the word embedding distances as
label correlations. For example, if emotion Ciand Cjare
close in the word embedding space, then when an image is
classiﬁed into class Ciwith a high probability, it should also
possess a similarly high probability within class Cj.3.4 Baseline Approach
Network Design. Our proposed ﬁne-grained emotion recog-
nition network includes two modules, a pre-trained facial
expression embedding model and a correlation-guided clas-
siﬁcation model (Fig. 6). The expression embedding model
is responsible for extracting expression-related features,
and the classiﬁcation model aims to regress the target emo-
tion distributions with the help of the calculated correlation
matrix Fexp. Notably, the pre-trained facial expression
embedding model [33] is trained for ﬁne-grained expression
similarities. The model incorporates many expression triplet
data and learns a continuous expression embedding space.
It is capable of capturing minor expression similarities and
thus suitable for building the 135-class representations. We
also illustrate the detailed network structures in the appen-
dix section.
Given a training image I2R224/C2224/C23and its one-hot
ground-truth vector Y2R135/C21, we ﬁrst input the image
into the expression embedding model for feature extraction.
The expression feature will then be sent into the classiﬁca-
tion model consisting of several alternative fully connected
and correlation layers. In particular, we implement the three
correlation layers by initializing them with Fexp. The
detailed network design can be found in the appendix.
Loss Function. The ﬁnal output of our model is a predic-
tion vector Pin size of R135/C21. Before comparing Pwith the
one-hot ground-truth label Y, we take the word embedding
correlation matrix Fwordinto consideration by applying the
transformed cross entropy (TCE) loss:
LTCE¼ /C0½ðFword/C1YÞlogPþð1/C0Fword/C1YÞlogð1/C0PÞÞ/C138:
(6)
It is worth noting that despite the formulation of Eq. 6 is
similar to the standard label smoothing strategy [56], they are
different in terms of relaxing weights. In the label smoothing
operation, each zero-value term within the ground-truth Y
is uniformly modiﬁed with the same soft parameter, e.g.,
Fig. 6. The pipeline of our proposed baseline approach for ﬁne-grained facial emotion recognition on the Emo135 dataset.CHEN ET AL.: SEMANTIC-RICH FACIAL EMOTIONAL EXPRESSION RECOGNITION 1911
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:22 UTC from IEEE Xplore.  Restrictions apply. a¼0:1. While in our method, the label relaxing is depen-
dent on the emotion word embedding analysis. Therefore
our produced correlation label Fword/C1Ycan better satisfy
the emotion nature and enhance the recognition process
with the prior semantic knowledge.
4E XPERIMENT
In this section, we ﬁrst give some implementation details
about our experiments. Then we report the subjective sur-
vey results on evaluating the Emo135 dataset. Finally, we
compare our proposed baseline framework with other feasi-
ble approaches and prove the efﬁciency of our method.
4.1 Implementation Detail
We randomly split the Emo135 dataset into training, valida-
tion, and testing set by 70%;15%;15%, respectively. There
are in total of 510,262 images for training and 109,342 for
validation/testing. We implement our training framework
based on Pytorch3. The training costs around 30 hours on an
NVIDIA RTX 3090 graphics card of 24 GB memory, with a
learning rate of 0.005 and batch size 240. We use a stochastic
gradient (SGD) optimizer for optimization and train the
entire framework for 100 epochs.
4.2 Dataset Evaluation
To ensure the image emotion labels are convincing and
credible, it is necessary to conduct a manual evaluation on
theEmo135 dataset. Therefore, we make a subjective survey
by recruiting 62 participants to validate the semantic corre-
spondence of our collected facial images and their emotion
labels. Speciﬁcally, we offer the participants three rating
choices including “I agree the given word faithfully conveys the
facial emotions” ,“I prefer another similar word to describe the
facial emotion” , and “I prefer another dissimilar word to describe
the facial emotion” , respectively standing for different accu-
racy levels of the emotion terms.
Considering the large size of our constructed dataset
(roughly 700k images), we choose to carry out the aforemen-
tioned manual evaluation by sampling the whole Emo135
dataset. We blindly select examples from each category
according to its size. For instance, we extract 198 images
from the largest category (containing 12,794 images) and 16
images from the smallest category (containing 994 images).
In return, we receive 33,651 ratings on 11,217 images, in
which each image and its label are evaluated by three differ-
ent raters.
As the histograms shown in Fig. 7, the image labeling
results are, in most cases, in agreement with the common
sense of human affective cognition. On average, 81:2%
raters agree that the given emotion labels perfectly match
the corresponding images. Besides, there are 12:9%raters
suggesting similar words for description, while 5:9%dis-
agree with the given labels and suggest something different.
Among the 135 emotion categories, pride gets the lowest sat-
isfying rate at 41:8%from raters. The other less satisfying
(<50%) classes are annoyance, humiliation, and suffering .I n
contrast, there are relatively more consistent categories,such as amusement (92:6%),amazement (91:1%),grumpiness
(91:1%) and sorrow (89:6%).
Furthermore, to demonstrate the test validity of the rat-
ing experiment, we also evaluate the inter-rater reliability
byKrippendorff’s alpha test [57]. Following the open-source
code4forKrippendorff’s alpha calculation, we compute the a
efﬁcient of our collected results, with a score of 0.776
(a¼1:0indicates perfect reliability, a¼0:0indicates the
absence of reliability, and a<0means systematically dis-
agreement). In conclusion, the statistical results of the sub-
jective survey indicate that our adopted 135 emotion
concepts, as well as the corresponding facial images, are
compatible with the human emotional cognition knowl-
edge. Nevertheless, it is worth noting that the above conclu-
sion comes from sample survey. The annotation cost limits
us to conduct a full survey at the current stage. We believe
it would be meaningful to increase the manual evaluation
scale in the future.
4.3 Model Evaluation
To evaluate our proposed model on recognizing facial emo-
tional expressions, we conduct both performance compari-
son and ablative study. Since this is the ﬁrst work trying to
handle the 135 emotion classiﬁcation problem, there is noFig. 7. Histogram plot of the collected subjective rating results. We show
the percentage of the three judgements that each category receives,
arranging from “ absolutely agree ”t o“ disagree ”.
3.https://pytorch.org/ 4.https://github.com/grrrr/krippendorff-alpha1912 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:22 UTC from IEEE Xplore.  Restrictions apply. existing method for direct comparison. Therefore, we
choose two kinds of competitive models and train them on
theEmo135 dataset. First, we adopt two commonly used
pre-trained backbones, ResNet-50 and VGGFace2, assem-
bled with Multilayer Perceptron (MLP) classiﬁers. Second,
we adopt two latest FER models, ARM [58] and DACL [59],
which have achieved state-of-the-art performance on basic
emotion recognition tasks, and modify their output layers
for 135-class recognition.
ResNet-50 Baseline. As a popular image pre-training
model in computer vision community, ResNet-50 [31] has
achieved signiﬁcant performance in a wide range of appli-
cations, especially on the image classiﬁcation/recognition
topic. Therefore we integrate the most recent released
ResNet-50 model trained on ImageNet-1k [60] dataset as
backbone and a 5-layer MLP to regress the image features
to 135 dimensional logits.
VGGFace2 Baseline. Compared with ResNet-50 [31],
VGGFace2 [61] is trained on speciﬁc human face images
and gained even better performances in several human face
centric applications, e.g., facial landmark detection, re-iden-
tiﬁcation, and facial expression recognition. We also design
a baseline consisting of a pre-trained VGGFace2 model [61]
and the same MLP layers as the ResNet-50 baseline.
ARM [58]. ARM is one of the state-of-the-arts reaching
impressive scores on the public benchmarks for 7-class dis-
crete facial expression recognition. It introduces an auxiliary
block for feature map rearrangement and enhances the de-
albino effect. Moreover, a minimal random re-sampling
scheme is also introduced to solve the data unbalancing
issue. To fairly compare our model with ARM [58], we mod-
ify its regression module (ﬁnal layer dimension) to make it
compatible with 135-class FER.
DACL [59]. DACL also reaches comparable good perfor-
mance in public FER benchmarks. The core idea includes a
novel sparse center loss design and an attention mechanism
to weight the contribution of metric learning loss functions.
Similar as ARM [58], we adopt the main structure of DACL
[59] including the attention net and the sparse center loss
calculation module but change the target output expression
dimension to 135.
Ablative Study. Besides, we also conduct ablative studies
to evaluate some key component including the expression
embedding model, correlation layer, and label transforma-
tion loss of our framework. For those components, we pro-
vide vanilla alternatives to evaluate the effectiveness of ourdesign. For example, the facial expression embedding
model is replaced with VGGFace2 [61], the correlation layer
is compared with fully-connected MLP, and the label trans-
formation loss is changed to cross entropy loss.
In Table 2, we compare the prediction results from each
method on the test set, including F1 score and accuracy for
the top 1, 5, 10 classes. It can be observed that our approach
reaches the best performance of all the others, with top-1
prediction accuracy at 28:3%, top-5 accuracy at 66:4%, and
top-10 accuracy at 78:7%.
4.4 Semantic Evaluation
To evaluate the semantic relationships between the emotion
labels and the predicted results, we design several experi-
ments in this section. First, we compare different word
embedding models in terms of their inﬂuences on the
semantic similarity distances. By our problem setting, we
choose three popular pre-trained word embedding model,
including Word2Vec [54], GloVe [55], and BERT [62], to study
the corresponding correlations between emotion word
semantics. We use the original prototypical rating results [7]
as reference and calculate the Pearson Correlation Coefﬁ-
cients (PCC) between each model’s output and the original
matrix (Table 3). The results shows that Word2Vec and GloVe
are both capable of extracting the true semantic relation-
ships for emotion words, while BERT performs poor on it.
The reason could be that BERT is not design for speciﬁc
word-level but contextual embedding.
Then we show some example testing results in Fig. 8. It is
interesting to ﬁnd that, even if the actual label is not recog-
nized as the top one, our model can still produce reasonable
predictions that are semantically close to the ground-truth
(in red). Furthermore, we employ the chosen word embed-
ding model [54], [55] to calculate the semantic distances
between the ground-truth label and our predictions/the
rest of 135 emotion terms (See Table 4). This phenomenonTABLE 2
Quantitative Comparison Between our Method and the Other Approaches, Including Pre-Trained Backbones, Modiﬁed SOTAs, and
Ablative Models
Approach F1-Score Top-1 Acc. Top-5 Acc. Top-10 Acc.
Pre-trained Model ResNet50 + MLP 0.061 0.096 0.306 0.454
VGGFace2 + MLP 0.062 0.099 0.322 0.465
Modiﬁed SOTA ARM [58] 0.147 0.205 0.559 0.708
DACL [59] 0.133 0.183 0.477 0.667
Ablative Study Ours w/o Embedding 0.082 0.126 0.458 0.564
Ours w/o Correlation Layers 0.175 0.247 0.604 0.735
Ours w/o Correlation Label 0.219 0.272 0.605 0.710
Ours 0 :247 0 :283 0 :664 0 :787
TABLE 3
Pearson Correlation Coefﬁcient (PCC) Between Word Embed-
ding Model Output and Original Emotion Rating Matrix [7]
PCC " Word2Vec [54] GloVe [55] BERT [62]
Original rating 0:532 0 :515 /C00:412
Positive means correlated and negative means uncorrelated.CHEN ET AL.: SEMANTIC-RICH FACIAL EMOTIONAL EXPRESSION RECOGNITION 1913
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:22 UTC from IEEE Xplore.  Restrictions apply. suggests that our analyzed semantic emotion relationship is
helpful and reliable to improve the semantic richness of the
FER results.
5L IMITATION AND DISCUSSION
Despite the fact that we have evaluated the emotion labeling
results of Emo135 dataset by conducting the subjective sur-
vey in the experiment, there still remains a lot of potential
improvements in the future. For example, it would be
meaningful to apply manual veriﬁcation on the full dataset,
i.e., making multi-person vote for the 135 emotion labels on
every facial expression image. Regarding the complicated
nature of human facial emotions, it is also necessary to
enlarge the FER dataset, such as adding more subjects and
image conditions, to support more robust research in this
area.
Besides, we would like to point out that the 135-cate-
gorical emotion representa tion, which stands for the
semantic richness in this paper, is not a ﬁxed standard.
With continuing innovative research works in the psycho-
linguistic ﬁeld, the semantic deﬁnition of human emotion
concepts is also changeable. In the future, if any more
dedicated emotion categori cal model is proposed, the
basic idea and the technical approach of this work can be
adapted to the new one.
6C ONCLUSION
In this work, we address the semantic-rich facial emotional
expression recognition problem. Unlike the existing FER
researches that only focus on a few basic emotion catego-
ries, we aim at the granularity of emotion concepts and the
entire emotion space. To this end, we construct a novelFER dataset by leveraging a 135-class categorical model
which can exhaustively represent the semantic atlas for
t h ee m o t i o nd o m a i n .W ef u r t h e rp r o p o s eab a s e l i n e
approach for the emotion recognition task on our built
dataset. The core idea of our method is to model the fuzzy
relationships between ﬁne-grained emotions and then
make it guide the network training process. We conduct
thorough evaluations on both the dataset labeling quality
and the baseline recognition method. The quantitative and
qualitative results suggest the beneﬁts of pushing FER to a
semantic-rich level. In the future, we believe it would be
meaningful to propose more dedicated methods and
large-scale datasets to promote the understanding and
analysis of ﬁne-grained facial emotions.
REFERENCES
[1] C. Darwin, The Expression of the Emotions in Man and Animals . Chi-
cago, IL, USA: Univ. Chicago Press, 2015.
[2] R. Plutchik, The Emotions . Lanham, MD, USA: Univ. Press Amer.,
1991.
[3] S. Epstein, “Controversial issues in emotion theory,” in Review of
Personality and Social Psychology , Beverly Hills, CA, USA: Sage
Publications, Inc., vol. 5, 1984, pp. 64–88.
[4] I. J. Roseman, “Cognitive determinants of emotion: A structural
theory,” in Review of Personality and Social Psychology , Beverly
Hills, CA, USA: Sage Publications, Inc., vol. 5, 1984, pp. 11–36.
[5] C. E. Izard, “Basic emotions, relations among emotions, and emo-
tion-cognition relations,” in Psychological Review , Washington, DC,
USA: Amer. Psychol. Assoc., vol. 99, no. 3, 1992, pp. 561–565.
[6] P. Ekman, “An argument for basic emotions,” Cogn. Emotion ,
vol. 6, no. 3/4, pp. 169–200, 1992.
[7] P. Shaver, J. Schwartz, D. Kirson, and C. O’connor, “Emotion
knowledge: Further exploration of a prototype approach,” J. Pers.
Social Psychol. , vol. 52, no. 6, 1987, Art. no. 1061.
[8] S. Du, Y. Tao, and A. M. Martinez, “Compound facial expressions
of emotion,” Proc. Nat. Acad. Sci. , vol. 111, no. 15, pp. E1454–E1462,
2014.
[9] C. F. B. Quiroz, R. Srinivasan, and A. M. Martinez, “EmotioNet:
An accurate, real-time algorithm for the automatic annotation of a
million facial expressions in the wild,” in Proc. IEEE Conf. Comput.
Vis. Pattern Recognit. , 2016, pp. 5562–5570.
[10] S. Li and W. Deng, “Reliable crowdsourcing and deep locality-
preserving learning for unconstrained facial expression recog-
nition,” IEEE Trans. Image Process. , vol. 28, no. 1, pp. 356–370, Jan.
2019.
[11] E. Barsoum, C. Zhang, C. Canton Ferrer, and Z. Zhang, “Training
deep networks for facial expression recognition with crowd-
sourced label distribution,” in Proc. ACM Int. Conf. Multimodal
Interact. , 2016, pp. 279–283.
[12] S. Li and W. Deng, “Blended emotion in-the-wild: Multi-label
facial expression recognition using crowdsourced annotations
and deep locality feature learning,” Int. J. Comput. Vis. , vol. 127,
no. 6/7, pp. 884–906, 2019.
[13] F. Zhou, S. Kong, C. C. Fowlkes, T. Chen, and B. Lei, “Fine-
grained facial expression analysis using dimensional emotion
model,” Neurocomputing , vol. 392, pp. 38–49, 2020.
[14] W. Wang et al., “Learning to augment expressions for few-shot
ﬁne-grained facial expression recognition,” 2020, arXiv:2001.06144 .
[15] J. A. Russell, “A circumplex model of affect,” J. Pers. Social Psy-
chol., vol. 39, no. 6, 1980, Art. no. 1161.
[16] A. Mollahosseini, B. Hasani, and M. H. Mahoor, “AffectNet: A
database for facial expression, valence, and arousal computing in
the wild,” IEEE Trans. Affect. Comput. , vol. 10, no. 1, pp. 18–31, Jan.
2017.
[17] D. Kollias and S. Zafeiriou, “Affect analysis in-the-wild: Valence-
arousal, expressions, action units and a uniﬁed framework,”
2021, arXiv:2103.15792 .
[18] H. R. Markus and S. Kitayama, “Culture and the self: Implications
for cognition, emotion, and motivation,” Psychol. Rev. , vol. 98,
no. 2, 1991, Art. no. 224.
[19] P. Ekman, “Facial expression and emotion,” Amer. Psychol. ,
vol. 48, no. 4, 1993, Art. no. 384.
Fig. 8. Illustration of the top-10 results predicted by our method. The
ground-truth label is indicated in red.
TABLE 4
Semantic Distances Between Ground-Truth Label and our
Predictions versus Ground-Truth Label and the Rest of 135
Emotion Terms, the Smaller the Better
Word embedding distances # Top-1 Top-5 Top-10
Ground-truth $Our predictions 0 :461 0 :433 0 :412
Ground-truth $The rest of 135 terms 0.639 0.637 0.6351914 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:22 UTC from IEEE Xplore.  Restrictions apply. [20] P. Ekman and W. Friesen, “Facial action coding system: A tech-
nique for the measurement of facial movement,” Consulting Psy-
chol. Press Palo Alto , vol. 12, Jan. 1978.
[21] M. Lyons, S. Akamatsu, M. Kamachi, and J. Gyoba, “Coding facial
expressions with gabor wavelets,” in Proc. IEEE 3rd Int. Conf.
Autom. Face Gesture Recognit. , 1998, pp. 200–205.
[22] T. Kanade, J. F. Cohn, and Y. Tian, “Comprehensive database for
facial expression analysis,” in Proc. IEEE 4th Int. Conf. Autom. Face
Gesture Recognit. , 2000, pp. 46–53.
[23] E. Goeleven, R. De Raedt, L. Leyman, and B. Verschuere, “The
karolinska directed emotional faces: A validation study,” Cogn.
Emotion , vol. 22, no. 6, pp. 1094–1118, 2008.
[24] A. Dhall, R. Goecke, S. Lucey, and T. Gedeon, “Static facial expres-
sion analysis in tough conditions: Data, evaluation protocol and
benchmark,” in Proc. IEEE Int. Conf. Comput. Vis. Workshops , 2011,
pp. 2106–2112.
[25] I. J. Goodfellow et al., “Challenges in representation learning: A
report on three machine learning contests,” in Proc. Int. Conf. Neu-
ral Informat. Process. , 2013, pp. 117–124.
[26] A. Mollahosseini, B. Hasani, M. J. Salvador, H. Abdollahi, D.
Chan, and M. H. Mahoor, “Facial expression recognition from
world wild web,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.
Workshops , 2016, pp. 58–65.
[27] D. Ko llias and S. Zafeiriou, “Expression, affect, action unit recognition:
Aff-wild2, multi-task learning and arcface,” 2019, arXiv:1910.04855 .
[28] C. Liu and H. Wechsler, “Gabor feature based classiﬁcation using
the enhanced ﬁsher linear discriminant model for face recognition,”
IEEE Trans. Image Process. , vol. 11, no. 4, pp. 467–476, Apr. 2002.
[29] C. Shan, S. Gong, and P. W. McOwan, “Facial expression recogni-
tion based on local binary patterns: A comprehensive study,”
Image Vis. Comput. , vol. 27, no. 6, pp. 803–816, 2009.
[30] P. Carcagn /C18ı, M. Del Coco, M. Leo, and C. Distante, “Facial expres-
sion recognition and histograms of oriented gradients: A compre-
hensive study,” SpringerPlus , vol. 4, no. 1, pp. 1–25, 2015.
[31] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for
image recognition,” in Proc. IEEE Conf. Comput. Vis. Pattern Recog-
nit., 2016, pp. 770–778.
[32] R. Vemulapalli and A. Agarwala, “A compact embedding for
facial expression similarity,” in Proc. IEEE/CVF Conf. Comput. Vis.
Pattern Recognit. , 2019, pp. 5683–5692.
[33] W. Zhang, X. Ji, K. Chen, Y. Ding, and C. Fan, “Learning a facial
expression embedding disentangled from identity,” in Proc. IEEE/
CVF Conf. Comput. Vis. Pattern Recognit. , 2021, pp. 6759–6768.
[34] Y. Li, J. Zeng, S. Shan, and X. Chen, “Occlusion aware facial
expression recognition using CNN with attention mechanism,”
IEEE Trans. Image Process. , vol. 28, no. 5, pp. 2439–2450, May 2018.
[35] Y. Chen, J. Wang, S. Chen, Z. Shi, and J. Cai, “Facial motion prior
networks for facial expression recognition,” in Proc. IEEE Vis.
Commun. Image Process. , 2019, pp. 1–4.
[36] Y. Chen, G. Song, Z. Shao, J. Cai, T.-J. Cham, and J. Zheng,
“Geoconv: Geodesic guided convolution for facial action unit rec-
ognition,” 2020, arXiv:2003.03055 .
[37] H. Yang, U. Ciftci, and L. Yin, “Facial expression recognition by
de-expression residue learning,” in Proc. IEEE Conf. Comput. Vis.
Pattern Recognit. , 2018, pp. 2168–2177.
[38] J. Zeng, S. Shan, and X. Chen, “Facial expression recognition with
inconsistently annotated datasets,” in Proc. Eur. Conf. Comput.
Vis., 2018, pp. 222–237.
[39] F. Zhang, T. Zhang, Q. Mao, and C. Xu, “Joint pose and expression
modeling for facial expression recognition,” in Proc. IEEE Conf.
Comput. Vis. Pattern Recognit. , 2018, pp. 3359–3368.
[40] K. Wang, X. Peng, J. Yang, S. Lu, and Y. Qiao, “Suppressing uncer-
tainties for large-scale facial expression recognition,” in Proc.
IEEE/CVF Conf. Comput. Vis. Pattern Recognit. , 2020, pp. 6897–6906.
[41] S. Li and W. Deng, “Deep facial expression recognition: A survey,”
IEEE Trans. Affective Comput. , to be published, doi: 10.1109/TAFFC.
2020.2981446 .
[42] D. Ko llias, I. Kotsia, E. Hajiyev, and S. Za feiriou, “Analysing affective
behavior in the second ABAW2 competition,” 2021, arXiv:2106.15318 .
[43] W. Zhang et al., “Prior aided streaming network for multi-task
affective analysis,” in Proc. IEEE/CVF Int. Conf. Comput. Vis. Work-
shops , 2021, pp. 3539–3549.
[44] B. Fehr and J. A. Russell, “Concept of emotion viewed from a pro-
totype perspective,” J. Exp. Psychol. Gen. , vol. 113, no. 3, 1984,
Art. no. 464.
[45] E. Rosch, “Principles of categorization,” Concepts Core Readings ,
vol. 189, pp. 312–322, 1999.[46] J. R. Averill, “A constructivist view of emotion,” in Theories of
Emotion . New York, NY, USA: Elsevier, 1980, pp. 305–339.
[47] K. Sailunaz and R. Alhajj, “Emotion and sentiment analysis from
twitter text,” J. Comput. Sci. , vol. 36, 2019, Art. no. 101003.
[48] Z. Wang, S.-B. Ho, and E. Cambria, “A review of emotion sensing:
Categorization models and algorithms,” Multimedia Tools Appl. ,
vol. 79, no. 47, pp. 35 553–35 582, 2020.
[49] L. F. Barrett, R. Adolphs, S. Marsella, A. M. Martinez, and S. D.
Pollak, “Emotional expressions reconsidered: Challenges to infer-
ring emotion from human facial movements,” Psychol. Sci. Public
Int., vol. 20, no. 1, pp. 1–68, 2019.
[50] M. A. Conway and D. A. Bekerian, “Situational knowledge and
emotions,” Cogn. Emotion , vol. 1, no. 2, pp. 145–191, 1987.
[51] T. Song, L. Chen, W. Zheng, and Q. Ji, “Uncertain graph neural
networks for facial action unit detection,” in Proc. AAAI Conf.
Artif. Intell. , 2021, pp. 5993–6001.
[52] P. Antoniadis, P. P. Filntisis, and P. Maragos, “Exploiting emo-
tional dependencies with graph convolutional networks for facial
expression recognition,” 2021, arXiv:2106.03487 .
[53] H. Yang, L. Yin, Y. Zhou, and J. Gu, “Exploiting semantic embed-
ding and visual feature for facial action unit detection,” in Proc.
IEEE/CVF Conf. Comput. Vis. Pattern Recognit. , 2021, pp. 10482–
10491.
[54] T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Efﬁcient estimation
of word representations in vector space,” 2013, arXiv:1301.3781 .
[55] J. Pennington, R. Socher, and C. D. Manning, “Glove: Global vec-
tors for word representation,” in Proc. Conf. Empirical Methods Nat-
ural Lang. Process. , 2014, pp. 1532–1543.
[56] R. M €uller, S. Kornblith, and G. E. Hinton, “When does label
smoothing help?,” in Proc. 33rd Int. Conf. Neural Inf. Process. Syst. ,
2019, pp. 4694–4703.
[57] A. F. Hayes and K. Krippendorff, “Answering the call for a stan-
dard reliability measure for coding data,” Commun. Methods Meas-
ures, vol. 1, no. 1, pp. 77–89, 2007.
[58] J. Shi and S. Zhu, “Learning to amend facial expression represen-
tation via de-albino and afﬁnity,” 2021, arXiv:2103.10189 .
[59] A. H. Farzaneh and X. Qi, “Facial expression recognition in the
wild via deep attentive center loss,” in Proc. IEEE/CVF Winter
Conf. Appl. Comput. Vis. , 2021, pp. 2402–2411.
[60] O. Russakovsky et al., “ImageNet large scale visual recognition
challenge,” Int. J. Comput. Vis. , vol. 115, no. 3, pp. 211–252,
2015.
[61] Q. Cao, L. Shen, W. Xie, O. M. Parkhi, and A. Zisserman,
“VGGFace2: A dataset for recognising faces across pose and age,”
inProc. IEEE 13th Int. Conf. Autom. Face Gesture Recognit. , 2018,
pp. 67–74.
[62] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-
training of deep bidirectional transformers for language under-
standing,” 2018, arXiv:1810.04805 .
Keyu Chen received the BEng and MEng degree
from the University of Science and Technology of
China, in 2018 and 2021 respectively . He is cur-
rently afﬁliated with Netease Fuxi AI Lab. His
research interests include facial expression anal-
ysis, animation, and geometry learning.
Xu Yang received the BEng degree in communi-
cation engineering from the Nanjing University of
Posts and Telecommunications, in 2013, the
MEng degree in information processing from
Southeast University, in 2016, and the PhD degree
in computer science from Nanyang Technological
University, in 2021. He is currently an associate
professor with the School of Computer Science
and Engineering of Southeast University , China.
His research interests mainly include computer
vision, machine learning, and Image Captioning.CHEN ET AL.: SEMANTIC-RICH FACIAL EMOTIONAL EXPRESSION RECOGNITION 1915
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:22 UTC from IEEE Xplore.  Restrictions apply. Changjie Fan received the doctor’s degree in
computer science from the University of Science
and Technology of China. He is the director of
NetEase Fuxi AI Lab. His research interest is in
machine learning, including multiagent systems,
deep reinforcement learning, game theory, and
knowledge discovery .
Wei Zhang received the BE degree in communi-
cation engineering from the Nanjing University of
Posts and Telecommunications, Jiangsu, China,
in 2017 and the MS degree in electronic and
information engineering from Zhejiang University,
Zhejiang, China, in 2020. She is currently a
research scientist working with Netease Fuxi AI
Lab, Hangzhou, China. Her current research
interests include computer vision, expression
embedding, and facial affective analysis.
Yu Ding received the PhD degree in computer
science from the Telecom Paris tech, in Paris
(France), 2014, the MS degree in computer sci-
ence with Pierre and Marie Curie University
(France), and the BS degree in Automation with
Xiamen University (China). He is currently an arti-
ﬁcial intelligence expert with Netease Fuxi AI Lab,
Hangzhou, China. His research interests include
deep learning, image and video processing, talk-
ing-head generation, animation generation, multi-
modal computing, affective computing, nonverbal
communication (face, gaze, and gesture), and embodied conversational
agent.
"For more information on this or any other computing topic,
please visit our Digital Library at www.computer .org/csdl.1916 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:22 UTC from IEEE Xplore.  Restrictions apply. Causal Narrative Comprehension: A New
Perspective for Emotion Cause Extraction
Wei Cao , Kun Zhang , Shulan Ruan , Hanqing Tao , Sirui Zhao, Hao Wang,
Qi Liu ,Member, IEEE , and Enhong Chen ,Senior Member, IEEE
Abstract— Emotion Cause Extraction (ECE) aims to reveal the cause clauses behind a given emotion expressed in a text, which has
become an emerging topic in broad research communities, such as affective computing and natural language processing. Despite the
fact that current methods about the ECE task have made great progress in text semantic understanding from lexicon- and sentence-
level, they always ignore the certain causal narratives of emotion text. Signiﬁcantly , these causal narratives are presented in the form of
semantic structure and highly helpful for structure-level emotion cause understanding. Nevertheless, causal narrative is just an abstract
narratological concept and its involving semantics is quite different from the common sequential information. Thus, how to properly
model and utilize such particular narrative information to boost the ECE performance still remains an unresolved challenge. To this end,
in this paper, we propose a novel Causal Narrative Comprehension Model (CNCM) for emotion cause extraction, which learns and
leverages causal narrative information smartly to address the above problem. Speciﬁcally, we develop a Narrative-aware Causal
Association (NCA) unit, which mines the narrative cue about emotional results and uses the semantic correlation between causes and
results to model causal narratives of documents. Besides, we design a Result-aware Emotion Attention (REA) unit to make full use of
the known result of causal narrative for multiple understanding about emotional causal associations. Through the ingenious
combination and collaborative utilization of these two units, we could better identify the emotion cause in the text with causal narrative
comprehension. Extensive experiments on the public English and Chinese benchmark datasets of ECE task have validated the
effectiveness of CNCM with signiﬁcant margin by comparing with the state-of-the-art baselines, which demonstrates the potential of
narrative information in long text understanding.
Index Terms— Emotion cause extraction, causal narrative, attention mechanism, semantics understanding
Ç
1I NTRODUCTION
ASa sub-task of emotion analysis [1], Emotion Cause
Extraction (ECE) has aroused extensive research inter-
ests in recent years [2], [3], [4]. It has signiﬁcant potentials in
various research communities ( e.g., affective computing [5]
and natural language processing [6]) and wide applications
(e.g., social media and business intelligence [7], [8]). The key
goal of ECE lies in identifying the cause clauses from a textwith a given emotion. Fig. 1 illustrates an example of the
ECE task in this paper.
Traditional approaches to the ECE task include rule-
based methods [9], [10] and machine learning methods [11],
[12]. Through making full utilization of linguistic rules and
feature engineering, these methods have achieved quite
good results in earlier years. However, most of these tradi-
tional methods still have limitations due to the lack of
semantic understanding of the emotional text. With the
development of deep learning technologies, many deep
neural models [13], [14] have been introduced into ECE,
attempting to alleviate the above problem by performing an
in-depth semantic understanding of emotional context. The
representative studies such as RTHN [14], MANN [15]
which integrated attention mechanism into Recursive Neu-
ral Networks (RNN) [16] or Convolutional Neural Network
(CNN) [17] for better text representation and emotion cause
detection. Besides, there are also some other valuable
attempts, such as the causes boundaries detection model
SECA [18], knowledge fusion method RHNN [19] and so
on. With these efforts and contributions, ECE has been
pushed a large step forward.
However, most current methods pour attention to text
semantic understanding from word-level and sentence-
level, while ignoring causal narrative comprehension of the
causal texts. Actually, each causal text for ECE usually con-
tains multiple clauses ( i.e., emotion result, emotion cause,
and others) that naturally have complex semantics as well
as certain causal narrative structures. Moreover, these/C15Wei Cao is with the School of Computer Science and Technology, Univer-
sity of Science and Technology of China, Hefei, Anhui 230027, China, and
also with Xinjiang Normal University, Urumqi, Xinjiang 830054, China.
E-mail: cw0808@mail.ustc.edu.cn.
/C15Kun Zhang is with the Hefei University of Technology, Hefei, Anhui
230002, China. E-mail: zhang1028kun@gmail.com.
/C15Shulan Ruan, Hanqing Tao, Sirui Zhao, Hao Wang, Qi Liu, and Enhong
Chen are with the School of Computer Science and Technology, University
of Science and Technology of China, Hefei, Anhui 230027, China.
E-mail: {slruan, hqtao, sirui}@mail.ustc.edu.cn, {wanghao3, qiliuql,
cheneh}@ustc.edu.cn.
Manuscript received 23 January 2022; revised 8 August 2022; accepted 4 Sep-
tember 2022. Date of publication 15 September 2022; date of current version
15 November 2022.
This work was supported in part by the National Natural Science Foundation
of China under Grant 61727809, in part by the Young Scientists Fund of the
National Natural Science Foundation of China under Grant 62006066, in part
by the Open Project Program of the National Laboratory of Pattern Recogni-
tion (NLPR), and in part by the Fundamental Research Funds for the Central
Universities under Grant JZ2021HGTB0075.
(Corresponding author: Enhong Chen.)
Recommended for acceptance by E. Cambria.
Digital Object Identiﬁer no. 10.1109/TAFFC.2022.3206960IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022 1743
1949-3045 ©2022 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See ht _tps://www.ieee.org/publications/rights/index.html for more information.
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:22 UTC from IEEE Xplore.  Restrictions apply. causal narratives could have a considerable impact on text
comprehension[20], [21]. In literature, causal narrative
refers to the statement about causality during event evolu-
tion [22]. It inﬂuences the way of humans to conceptualize
events [23], [24], [25] and contributes to the representation
and comprehension of long texts [26], [27]. Generally, causal
narratives could be divided into chronological narrative
(cause–before–effect) and ﬂashback narrative (effect–
before–cause) according to narratology [22], [28]. Taking
Fig. 1 as an example, according to the cue of the given emo-
tion phrase “grateful” ,clause 4 could be regarded as the emo-
tion result clause in the document. Thus, we could
preliminarily locate the possible area of emotion causes,
namely two alternative cause regions ( i.e.,clause 1-4 and
clause 4-6 ). Further, with the causal narrative, we could efﬁ-
ciently mine emotional causal correlations between the
result clause and other clauses for the ECE task.
Furthermore, in literature, some studies have also
observed that there is an inherently strong correlation and
coherence between the cause and result in a causal narrative
[29], [30], [31], [32]. They demonstrate the result clause is
most associated with the unrecognized cause clauses within
the alternative cause regions. Based on this assertion, the
grasp of causal narratives and semantic relations between
result clauses and other clauses within causal narratives are
considerable critical for causal text understanding [21], [33].
Consequently, we in this paper focus on causal narrative
comprehension and exploring the emotional semantics cor-
relations within causal narratives for better emotion cause
extraction.
Inspired by the above observations, the speciﬁc solution in
this paper includes two aspects. For one thing, we leverage
the causal structure of causal narrative to perceive the possi-
ble scope of emotion cause clauses. For another, based on the
guidance of causal structure, we focus on the clauses that
have strong causal correlations with the known emotion
result clause in a causal narrative to predict emotion cause
clauses. To achieve the above solutions, we must consider the
following challenges: 1) How to properly represent the tex-
tual causal structure via the causal narrative understandingof a document; 2) Under the guidance of causal narrative,
how to explore and understand the causal association
between cause clauses and result clauses within the docu-
ment for emotion cause extraction.
To address the above challenges, in this paper, we pro-
pose a Causal Narrative Comprehension Model (CNCM)
for emotion cause extraction. For the ﬁrst challenge, we
design a Narrative-aware Causal Association (NCA) unit,
which uses the narrative cue about the known emotion
result to learn the semantic correlation between causes and
results for causal narrative representation. For the second
challenge, we develop a Result-aware Emotional Attention
(REA) unit to acquire the cognition of emotional causal cor-
relation through the attention mechanism between the
known result clause and other clauses within the causal nar-
rative. Speciﬁcally, the REA unit is ﬁrstly performed for the
preliminary cognition of emotional causal correlation of
documents. With this preliminary cognition, we utilize the
NCA unit for the representation of causal narrative struc-
ture for good comprehension of causality and perception
about the possible scope of cause clauses. Third, guided by
the representation of the causal narrative structure, the REA
unit is performed again to acquire accurate comprehension
of the emotional causal correlation for the prediction of
emotion cause clauses.
As an emphasis, the main contributions of our work can
be concluded as follows:
/C15We propose a model based on causal narrative com-
prehension for emotion cause extraction. To the best
of our knowledge, it is the ﬁrst time to introduce
causal narrative information into the ECE task.
/C15We develop NCA to analyze and model the causal
narrative information of ECE documents. Then we
utilize REA to help understand the emotional causal
correlations guided by the causal narrative informa-
tion. In this way, we can grasp causality and identify
the emotion causes of documents accurately.
/C15The experimental analysis of results on the bench-
mark datasets validates the effectiveness of the pro-
posed CNCM for ECE. And the model achieves
considerable performance by comparing with sev-
eral state-of-the-art methods.
The remainder of this paper is organized as follows. The
related work is introduced in Section 2, and the problem
deﬁnition is stated in Section 3. Then, in Section 4, we dem-
onstrate the model details and training techniques of the
developed CNCM. Subsequently, the experimental results
are reported in Section 5. Finally, we conclude this paper in
Section 6.
2R ELATED WORK
In this section, we will review the related works from three
aspects: Emotion Analysis ,Emotion Cause Extraction and Nar-
rative Understanding , which are closely related to our work
in this paper.
2.1 Emotion Analysis
With the boom of the artiﬁcial intelligence ﬁeld, emotion
analysis has attracted a large of research attention. In the
Fig. 1. An instance of the emotion cause extraction task. The document
contains six clauses including one emotion phrase ( i.e.,“grateful” ) indi-
cating the overall emotion. Thus, we regard clause 4 as result clause
and aim to ﬁnd out the cause clause ( i.e.,clause 3 ) among all the
clauses in this text.1744 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:22 UTC from IEEE Xplore.  Restrictions apply. ﬁeld of text emotion analysis, scholars tend to mine effective
text semantic information to improve the accuracy of emo-
tion recognition [34], [35], [36]. For example, Li et al. [37]
focused on the strong context dependence of each sentence
in a discourse. They designed an appropriate framework
named bidirectional emotional re-current unit (BiERU) to
effectively encode the strong contextual information for
conversational sentiment analysis. Simultaneously, there
are also many meaningful explorations on visual emotion
analysis [1], [38], [39]. For example, Ruan et al. [5] proposed
a novel architecture named color enhanced cross correlation
net (CECCN) for image sentiment analysis. As multimedia
technology advances, multimodal data is rapidly growing
and available to scholars. Many researchers began to pay
attention to the research of multimodal emotion analysis
[40], [41], [42]. Zhang et al. [43] noted that emotion in con-
versation videos happens step by step. Thus, they proposed
a multimodal emotion recognition model based on rein-
forcement learning and domain knowledge for conversation
videos. All these efforts exploit the characteristics of emo-
tion information from various meaningful perspectives to
promote emotion analysis and task implementation in cer-
tain scenarios.
2.2 Emotion Cause Extraction
As an important sub-task of emotion analysis, there has
been an increasing amount of literature on ECE in recent
years. Generally, previous methods can be grouped into
three categories, i.e., rule-based methods, machine learning
methods anddeep learning methods .
2.2.1 Rule-Based Methods
Earlier studies of ECE are mainly rule-based methods [9],
[11], [44]. Lee et al. [44] pioneered the construction of a pub-
licly available corpus for the ECE task and conducted a
detailed analysis of its content. To make full utilization of
the special language expressions in this corpus to detect
emotional causes, Lee et al. [11] generalized sets of linguistic
rules well by deﬁning linguistic cues. Subsequently, they
further extracted cause expressions and speciﬁc construc-
tions via linguistic rules to improve their previous solution
[9]. In addition, there are also some novel solutions based
on events analysis [45] and common sense knowledge [46],
which have also been demonstrated quite good perfor-
mance for ECE. However, these methods are not sufﬁciently
generalized for practical applications, since artiﬁcial rules
cannot cover all complex linguistic phenomena of texts in
real situations.
2.2.2 Machine Learning Methods
Considering the weak generalization capability of rule-
based methods, a variety of machine learning methods have
also been developed for this task successively. For instance,
to deal with the special linguistics features of Weibo1texts,
Gao et al. [10] proposed a conditional random ﬁelds model
based on syntactic and semantic characteristics, which
could effectively mine the relation between emotion expres-
sion and cause in Weibo text to detect emotion causes ofsocial texts. Then, Gao et al. [47] presented an Emotion-
Cause-OCC model to address emotion cause extraction in
micro-blog posts. Specially, this approach focused on inves-
tigating factors for eliciting kinds of emotions and could
acquire the proportions of these cause components under
different emotions. Additionally, there have also been many
other valuable studies, such as the CRF-based model [12],
the event-driven multi-kernel SVMs method [48]. All these
studies have made large contributions to the development
of ECE. However, these methods rely on the utilization of
effective statistical features about the texts, ignoring text
semantics understanding.
2.2.3 Deep Learning Methods
Owing to the development of deep learning technology,
deep neural networks have attracted more and more
research attention for their excellent semantic representa-
tion ability. Particularly, the Bi-directional Long Short-Term
Memory (BiLSTM) [49] and attention mechanism are widely
used in these studies since they could model good seman-
tics and capture useful emotional information for the ECE
task. For example, based on a co-attention deep neural net-
work, Li et al. [50] took account into attention mechanism
[51] and proposed a co-attention deep neural network to
exploit the correlation among clauses which is helpful for
emotion cause extraction. Following this work, MANN was
proposed in [15], which substituted multi-attention-based
framework for a co-attention network to mine correlations
between emotion phrases and candidate clauses and
achieved comparable results. To further improve the perfor-
mance of the ECE task, Fan et al. [19] presented a novel solu-
tion called RHNN, which ingeniously utilized sentiment
lexicon and common knowledge as restrained parameters
to promote model training. Owing to the superiority of
deep semantic representations and attention mechanisms,
these models have gained great performance improvement.
In addition, some other novel solutions have also been pro-
posed, including hierarchical network methods [13], [14],
question-answering solution [52], reordered prediction
framework [53], retrieval rank framework [2] and docu-
ment-level context idea [54], which have also provided
some new insights for this task. Notably, there are also
some derivative tasks about the ECE task. For example,
some researchers innovatively improved the benchmark
corpus of the ECE task to accommodate the derivative task
of emotion-cause pair extraction and acquired many valu-
able results [55], [56], [57], [58], [59]. While some others
addressed the ECE task as a boundary detecting task of
emotion cause spans at the span-level by manually annotat-
ing cause spans on the original datasets [18].
To summarize, current advanced works about the ECE
task have emphasized semantic representations of sentences
[15], [19], [60]. They employed attention mechanisms to
obtain emotion correlations based on emotion phrases for
emotion cause extraction while ignoring causal narratives
of documents. Unlike the above studies, in this paper, we
deal with this task as an issue of causal narrative compre-
hension for documents. Particularly, we dig deeply into
documents information to subtly model causal narratives of
documents. In this way, we can efﬁciently localize emotion 1.https://weibo.com/CAO ET AL.: CAUSAL NARRATIVE COMPREHENSION: A NEW PERSPECTIVE FOR EMOTION CAUSE EXTRACTION 1745
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:22 UTC from IEEE Xplore.  Restrictions apply. cause regions and accurately acquire emotional causal cor-
relations, facilitating the task of emotion cause extraction.
2.3 Narrative Understanding
Narrative understanding aims to identify the key elements
of narrative structure in a story, that is, the relationship
between critical elements and context to which the narrative
belongs [61]. It is usually applied to some understanding
tasks, such as ﬁlm analysis [62], [63], text extraction [64],
[65], [66], reading comprehension [67], [68], [69], and so on.
Generally, the narrative content of a text involves descrip-
tions of daily activities, discourses, or stories [70]. While
given the development logic of events and the completeness
of discourses, the sentences within a narrative text ( e.g.,a
discourse or story) must show coherence in semantics [30],
[32]. Nowadays, coherence has been introduced in many
tasks related to narrative understanding and promotes these
studies [71], [72], [73]. Similar studies have also been done
in other ﬁelds such as the research about coherence in music
generation [74] and the work about viewpoint coherence in
ﬁlm [75]. Considering that coherent narrative is bound to be
semantically relevant, some tasks tend to deal with narra-
tive coherence from the perspective of semantic correlation.
For example, Hu et al. [76] proposed a model based on man-
ual rules, which utilized causal potential to conclude event
pairs with narrative causality relations from ﬁlm scenes.
Further, Chen et al. [73] focused on modeling sequential
semantics of clauses in documents for story completing.
Notably, as a special narrative structure, the causal narra-
tive also presents a strong semantic coherence in its sen-
tence sequence. As mentioned by Wellner et al. [31], the
cause and result within a causal narrative are one of the
broad classes of coherence relations. Moreover, this coher-
ence in cause and result is much stronger than the one in
the general narrative due to the strong dialecticity and dual-
ity between the cause and result of causal narrative.
Inspired by these studies, we introduce the idea of narra-
tive understanding into the ECE task. Speciﬁcally, consider-
ing that causal narrative involves two possibilities ofchronological narrative and ﬂashback narrative, our pro-
posed approach improves the sequential semantics model-
ing in current studies and focuses on learning the two
possible semantic information of causal narrative. Note
that, this is the ﬁrst work that uses causal narratives of dis-
course to address this task.
3P ROBLEM DEFINITION
The formal deﬁnition of the ECE task is listed below. The
inputs are the texts of document Dand an emotion phrase
ep, where Drepresents a causal text with nclauses, and ep
refers to the overall emotion of this document. Here, Dis
denoted as follows:
D¼c1;c2; :::; c n fg ; (1)
where ciis the ithclause in document D. And the emotion of
epis uniquely consistent with that of a clause in D. The goal
of the ECE task is to learn a function Fto identify the cause
clauses in document Dthat trigger the emotion ep:
yyyyyyy¼FðD; ep Þ; (2)
here, yyyyyyy¼fy1;y2; :::; y ngdenotes the emotion cause labels of
the clauses in D, where yi= 1 if the ithclause is an emotion
cause clause, else yi= 0. In this paper, we regard the clause
inDwhose emotion is consistent with that of epas the emo-
tion result clause.
The above deﬁnition about ECE could be illustrated via
the instance in Fig. 1. In terms of the form for the input texts,
the inputs to be processed are a document containing multi-
ple clauses and a phrase. As for the textual content, this
instance is a causal text with an overall emotion category
“grateful” . Based on the above, this article aims to reveal the
clauses that trigger the emotion “grateful” of the document:
if a clause triggers the overall emotion of its document, we
will mark it as the emotion cause clause of this document;
otherwise, we will mark it as a non-cause clause.
For ease of explanation, relevant notations used in this
article are summarized in Table 1.
4M ETHOD
In this section, we will introduce model details as well as
model training of our proposed Causal Narrative Compre-
hension Model (CNCM).
4.1 Overall Architecture
The architecture of CNCM is shown in Fig. 2, which consists
of: 1) Input Embedding : accomplishing the clause embed-
dings of the input texts; 2) Emotion Causality Understanding :
acquiring emotional causal correlations in causal narratives
via the REA unit; 3) Cause Narrative Representation : repre-
senting textual cause narratives through the NCA unit; 4)
Emotion Causality Re-understanding : re-understanding emo-
tional causal correlation under the guidance of cause narra-
tive representation via the REA unit; 5) Cause Prediction :
predicting the emotion cause label for each clause.
The process is summarized below. First, we mark the
emotion result clause according to the cue of the given emo-
tion phrase and realize the embedded representation of
each clause for the document. Second, we adopt the REATABLE 1
Mathematical Notations
Symbol Description
D the original document with several clauses
ep the emotion phrase of D
ce
k thekthclause of Dthat contains emotion phrase
ep, namely the emotion result clause of D
y the cause label of the clauses in D
a the emotional attention vector in REA
r the causal narrative association vector of D
Eb the feature representation of Dwith BERT
Eh the hidden state by processing EEEEEEEbwith LSTM in
REA
Eu the representation of Dthat contains the
preliminary Emotion Causality Understanding by
REA
Ec the representation of Dthat fuses the
representation EEEEEEEuand the causal narrative
information rrrrrrr
Eru the ﬁnal representation of Dwith Emotion
Causality Re-understanding1746 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:22 UTC from IEEE Xplore.  Restrictions apply. unit to mine the emotional causal correlation between the
known result clause and other clauses for the preliminary
emotion causality understanding. Next, we perform the
NCA unit, which utilizes the semantic coherence of causal
narrative to learn the document’s possible causal narrative
for realizing the causal narrative representation of the docu-
ment. Finally, imitating the human habit of repeated com-
prehension while reading long text, we execute the REA
unit again to re-understand the emotional causality of the
document. Based on the ﬁnal effective text representations,
we predict the emotion cause clause to realize this task. The
details are introduced in the following parts.
4.2 Input Embedding
As stated in the problem deﬁnition above, to cognize the
causal narrative of document Dfor the subsequent narrative
comprehension, we need to mark the result clause at the
ﬁrst step. For our benchmark datasets, the English dataset
has been annotated with the information about the emotion
result clause, and the Chinese dataset can realize the loca-
tion of the emotion result clause by ﬁnding phrase epin doc-
ument Dthrough string matching operation. Afterward, the
mathematical form of document Din Eq. (1) can be modi-
ﬁed as below:
D¼fc1;c2; :::; c k/C01;ce
k;ckþ1; :::; c ng; (3)
where ce
kis the emotion result clause containing the expres-
sion consistent with ep. After that, the input texts would be
fed into the encoder for vector forms. Considering that the
pre-trained language model BERT [77] has excellent capa-
bility in semantic representations, especially its evolution
model BERT-wwm [78] achieves considerable performance
in Chinese based on whole-word masking technology and a
large-scale Chinese corpus. we adopt BERT-wwm to accom-
plish the vectorization of input texts:
EEEEEEEb¼BERT ðDÞ¼f xxxxxxx1;xxxxxxx2; :::; xxxxxxxk/C01;xxxxxxxe
k;xxxxxxxkþ1; :::; xxxxxxxng; (4)
where EEEEEEEb2Rn/C2dwrepresents the documental semantic
embeddings at sentence level. dwis the dimension of the
output embeddings in BERT-wwm. xxxxxxxiis the embedding of
theithclause. Specially, xe
irefers to the embedding of the
result clause ce
kwhen i¼k.4.3 Emotion Causality Understanding
Inspired by the human habit of ﬁrst preferring an initial
grasp when reading a complex text [79], at the beginning of
CNCM, we also try to make a preliminary understanding of
the text, especially its causal narrative. Currently, a plethora
of studies have established the importance of correlations
between causes and results in causal texts understanding
[20], [21], [80], [81]. Hence, as depicted in Fig. 3, we focus on
emotional causal correlations and utilize the known result
clause ce
kto design a Result-aware Emotional Attention
(REA) unit to explore this emotional causal association for
emotion causality understanding.
REA Unit. Considering the foundational role of clause
representational quality in causal association modeling, we
ﬁrst focus on representing the clauses accurately before
achieving the emotional causal association among these
clauses. Since BiLSTM is good at modeling long texts and
capturing context information, we employ it to cope with
document D, ensuring that each clause’ semantics would
not deviate from the context information of D:
EEEEEEEh¼BiLSTM E EEEEEEbðÞ; (5)
where EEEEEEEh¼fhhhhhhh1; :::; hhhhhhhe
k; :::; hhhhhhhng2Rn/C2dhis the hidden state of
each clause in document Dprocessed by BiLSTM. dhis the
dimension of the hidden state in BiLSTM. hhhhhhhe
kis the hidden
state of the result clause. By this way, the context informa-
tion of document Dis incorporated into the representation
of each clause.Fig. 2. The overall architecture of Causal Narrative Comprehension Model (CNCM) for emotion cause extraction.
Fig. 3. The architecture of REA.CAO ET AL.: CAUSAL NARRATIVE COMPREHENSION: A NEW PERSPECTIVE FOR EMOTION CAUSE EXTRACTION 1747
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:22 UTC from IEEE Xplore.  Restrictions apply. After acquiring the good representation by integrating
document context, we construct the emotional causal corre-
lation based on the known result clause as follows. So far,
narratological researches have shown that sentences of a
causal narrative varied in the extent to which the causal rela-
tions are close or distant [20], [80]. Speciﬁc to the clauses of
causal narrative in document D, the ones that are more corre-
lated with the result clause in semantics are more likely to be
cause clauses. Considering that attention mechanism can
simulate humans’ visual attention and highlight critical
information by concentrating on differences of its input, we
adopt an attention mechanism to capture different causal
associations between the result clause and each clause:
MMMMMMM¼tanh WWWWWWW1EEEEEEEhþWWWWWWW2hhhhhhhe
k/C0/C1
;
aaaaaaa¼softmax W WWWWWW3MðÞ ; (6)
where aaaaaaa2Rnis the attention weight scores, standing for
each clause’s emotional causal association to the current
representation hhhhhhhe
kof the result clause. WWWWWWW1,WWWWWWW2andWWWWWWW3are
the trainable parameters. Afterward, aaaaaaais fused into the doc-
ument representation EEEEEEEh, promoting CNCM focusing on
the semantics more relevant to the emotion result:
EEEEEEEu¼a/C1EEEEEEEh; (7)
where EEEEEEEu¼fsssssss1;sssssss2; :::; ssssssse
k; :::; sssssssng2Rn/C2dhis the text represen-
tation containing emotional causal association, which refers
to the preliminary cognition of emotion causality of docu-
ment D.
4.4 Causal Narrative Representation
To address the ﬁrst challenge of our work stated before, this
layer purposes to model the causal narrative information of
document D. Currently, most of the narratological studies
focus on modeling sequential semantics of clauses in docu-
ments [73]. However, these sequential semantics-based
models are not suitable for causality modeling of causal
texts since the cause and result clauses in causal texts are
not always narrated sequentially. Fortunately, causal narra-
tives contain special narrative properties, which could deal
with this issue. According to narratology, causal narrative
usually includes chronological narrative (cause–before–
effect) and ﬂashback narrative (effect–before–cause) [22],
[28]. It implies the regions before or after the result in a
causal text could be regarded as alternative cause regions of
the text.
Special to document D, we leverage the known result
clause ssssssse
kto speculate the alternative cause regions EEEEEEEu1and
EEEEEEEu2of document D, as shown in Fig. 4. Namely, the regions
EEEEEEEu1and EEEEEEEu2, which are adjacent and contain the result
clause ssssssse
k, are two alternative cause regions of D:
EEEEEEEu1¼fsssssss1;sssssss2; :::; ssssssse
kg;
EEEEEEEu2¼fssssssse
k;ssssssskþ1; :::; sssssssng: (8)This reveals that document Dmay contain two alternative
causalities: one is the narrative between the region EEEEEEEu1and
the emotion result clause ssssssse
k, and the other is the narrative
between EEEEEEEu2andssssssse
k.
According to the role of causality in narrative understand-
ing [24], [25], the above awareness of causal narrative about
document Dcan be helpful in determining its precise region
of cause clauses. Thus, we ﬁrstly devise a narrative-aware
causal association (NCA) unit to model the two possible
causal narratives in regions EEEEEEEu1andEEEEEEEu2respectively. Subse-
quently, the causal narrative information of these two regions
is integrated together for the causal narrative association vec-
torrof its whole document D. To pursue effective text repre-
sentations, we integrate the causal narrative information rrrrrrr
into the document feature EEEEEEEufrom the previous layer:
EEEEEEEc¼rrrrrrr/C1WWWWWWW4EEEEEEEu; (9)
where EEEEEEEc2Rn/C2dmrefers the causal narrative representation
of document D.dmis the dimension of the hidden layer in
CNCM. WWWWWWW4is the trainable parameters. Guiding by causal
narrative representation, CNCM can capture causal narra-
tive information of ECE documents, which is conducive to
emotion cause detection.
CNA Unit. Here, we take the alternative cause region
EEEEEEEu1:fsssssss1;sssssss2; :::; ssssssse
kgas an example to illustrate the imple-
ment process of CNA, as shown in Fig. 5. Considering that
the clause ssssssse
kis the known result of region EEEEEEEu1, this region
may involve a causal narrative. Namely, there may be a pos-
sible causal association between the sequence ½sssssss1;sssssss2; :::; sssssssk/C01/C138
andssssssse
kof this region. Inspired by the semantic coherence in
causal narratives [32], [82], this possible causal association
is manifested by the semantic coherence between the two.
Hence, we utilize the semantic correlation between the
sequence ½sssssss1;sssssss2; :::; sssssssk/C01/C138and result clause ssssssse
kto measure the
possible causal association between the two for the causal
narrative modeling of the region EEEEEEEu1.
First, according the semantic consistency within dis-
course [25], [30], [32], the semantics of the sequence
½sssssss1;sssssss2; :::; sssssssk/C01/C138is can be represented by its subsequent
semantics because the semantic consistent of the two. Spe-
ciﬁcally, considering that LSTM [16] is good at processing
and understanding the semantics of sequence data, we uti-
lize LSTM to handle the sequential clauses of this sequence
by time step for the representation of this sequence:
fffffffr1¼LSTM ½sssssss1;sssssss2; :::; sssssssk/C01/C138 ðÞ ; (10)Fig. 4. The causal narrative in a document.
Fig. 5. The architecture of CNA. Here, we take the region EEEEEEEu1as an
example to illustrate its process.1748 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:22 UTC from IEEE Xplore.  Restrictions apply. where fffffffr12Rdhis the output of LSTM at its ﬁnal step and
can be regarded as the representation of ½sssssss1;sssssss2; :::; sssssssk/C01/C138. Sec-
ond, we refer to the narrative modeling in the study about
story completion [73] and adopt cosine similarity to ﬁgure
out the semantic correlation between this sequence and the
result clause ssssssse
k:
b1¼Similarity f ffffffr1;ssssssse
k/C0/C1
¼fr1/C2ssssssse
k/C0/C1
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
fffffffr1ðÞ2q
/C2ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ssssssse
k/C0/C12q ; (11)
where b12Ris used to measure the degree of likelihood of
causality in the alternative cause region EEEEEEEu1.
Similarly, we utilize the CNA unit to handle the other
alternative cause region EEEEEEEu2to acquire the value b22R,
which is used to measure the possibility of causal narrative
inEEEEEEEu2. And for the sake of calculation, we convert b1andb2
to the corresponding vector form by copying and padding
operations, respectively:
rrrrrrrui¼ðb1;b1; :::;b1|ﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄ};0;0; :::;0|ﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄ}Þ;i ¼1;
kn /C0k
ð0;0; :::;0|ﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄ};b2;b2; :::;b2|ﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄ}Þ;i ¼2;
k/C01n/C0kþ18
>>>>>>><
>>>>>>>:(12)
where rrrrrrrui2Rnstands for the causal narrative association
vector of the region EEEEEEEui.kis the number of clauses in EEEEEEEu1,
while n/C0kþ1is the number of clauses in EEEEEEEu2.
Moreover, considering that each discourse segment has
both local and global coherence [30], rrrrrrru1and rrrrrrru2are just
involve the possibilities of local causal narratives in docu-
ment D. Thus, it is necessary to integrate the two together
for the global causal narrative of this document. Since rrrrrrru1
andrrrrrrru2contain the desired information at the position of
clause ssssssse
k, we realize this integration by the mean operation
as below:
rrrrrrr¼ðrrrrrrru1þrrrrrrru2Þ=2; (13)
where rrrrrrr2Rnrefers to the causal narrative association vec-
tor of document D. As stated before, rrrrrrris integrated into the
document representation Euto achieve the causal narrative
modeling of this document and obtain the effective docu-
ment representation Ec.
4.5 Emotion Causality Re-Understanding
Inspired by humans’ multiple understanding of long texts
[79], we in this section employ the REA unit again to re-
understand the causality of document D. Speciﬁcally, the
above text representation EEEEEEEcis fed into REA as below:
EEEEEEEru¼REA ðEEEEEEEcÞ; (14)
where EEEEEEEru2Rn/C2dhstands for the ﬁnal text representation
after this re-cognition of emotional causality of document
D. It could facilitate CNCM to comprehend the emotional
causality of this document very well. The relevant details
are similar to those of the preliminary understanding of
emotional causal correlation for document D.4.6 Cause Prediction
In this layer, we leverage the ﬁnal representation EEEEEEEruof doc-
ument Dto predict emotion cause clauses. Speciﬁcally, we
fed each clause embedding of EEEEEEErusuccessively into a single-
layer fully connected (FC) network and a sigmoid function
for cause clauses prediction:
ppppppp¼Sigmoid FC E EEEEEEruðÞðÞ ; (15)
where ppppppp2Rnindicates the probability vector of emotion
cause labels for the clauses in document D.
4.7 Model Learning
Since the ECE task is a classiﬁcation problem, we employ
thecross /C0entropy function as our loss function.
Loss ¼/C01
m1
nXm
j¼1Xn
i¼1yj
ilogpj
i; (16)
where mindicates the number of documents in the datasets.
nmeans the number of clauses in a random document. yj
i
refers to the true cause label of the clause ciin the jthdocu-
ment of the datasets. To minimize the loss, we use the
Adam optimizer to update the parameters of each layer.
Additionally, we conduct the dropout operation and K-fold
(k=10) cross validation trick to prevent our model training
from overﬁtting.
5E XPERIMENTS STUDY
In this section, we ﬁrst introduce the experiment prepara-
tion, involving the dataset details, evaluative criteria, and
experimental settings. Then, we list some baseline methods,
analyze the experimental results in detail, and conduct
some ablation studies. Subsequently, we make some
detailed analysis of some meaningful issues for our pro-
posed model. Finally, we present several visualization cases
to illustrate the workﬂow of CNCM.
5.1 Dataset Description
Following the general practice in many previous studies [4],
[14], we conduct experiments on two publicly ECE cor-
puses: a Chinese benchmark dataset [48] based on Sina City
News2and an English benchmark dataset [3], [19], [83]
based on an English novel. To provide an intuitive sense of
the datasets, we also present a document example from the
Chinese dataset as shown in Fig. 6. It contains 5 clauses and
is annotated with an emotion phrase. Additionally, we pres-
ent some key information about these two datasets in
Table 2. Particularly, most of the documents in both datasets
contain one cause clause, accounting for more than 90.20%
of the total. In contrast, merely a few documents contain 2
or more cause clauses. Noticeably, according to the num-
bers of the cause clause and general clause (the non-cause
clause) shown in Table 2, we can conclude that the size of
each dataset is not large enough and the number of cause
clauses in documents is extremely uneven.
Given that document size is closely related to document
narrative complexity, we perform statistical analysis on the
2.https://city.sina.com.cn/CAO ET AL.: CAUSAL NARRATIVE COMPREHENSION: A NEW PERSPECTIVE FOR EMOTION CAUSE EXTRACTION 1749
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:22 UTC from IEEE Xplore.  Restrictions apply. document size of the two datasets according to the number
of clauses, shown in Figs. 7 and 8. It is can be observed that
the document size distribution of the Chinese dataset is rela-
tively centralized, while that of the English dataset is very
scattered. Speciﬁcally, the clause count of documents in the
Chinese dataset is mainly distributed from 3 to 11. In con-
trast, the clause count of documents in the English dataset is
mainly distributed from 1 to 25. In short, the document size
of the Chinese dataset is relatively simple and moderate
compared to that of the English dataset.
5.2 Evaluative Criteria
Following [14], [19], [52], we also adopt Precision, Recall,
and F1 to evaluate the performance of models in this paper.
Precision ¼P
ccause 1P
pcause 1; Recall ¼P
ccause 1P
acause 1;F1
¼2/C3Precision /C3Recall
Precision þRecall;
whereP
ccauseis the predicted correct cause clause.P
pcause
means all the predicted cause clauses.P
acause stands for
the annotated cause clauses.
5.3 Experimental Settings
For better training, we initialize the related parameters of
our model by the following settings. First, we split thedataset into training (80%) and test (20%) sets. In terms of
data vectorization, consistent with [18], we adopt the
advanced pre-trained language model BERT [78] to encode
each input clause into an embedding vector. By investiga-
tion of BERT-wwm used in other works, we ﬁne-tune the
pre-trained model on the training set and acquire the text
vectors with the dimension dw= 1,024. During training, the
initial weights are assigned according to the uniform distri-
bution suggested in [84]. During intermediate steps, we set
dh= 512 to be the dimensions of hidden state in the LSTM
and BiLSTM of CNCM. Empirically, the dimension dmof
ordinary hidden layers in CNCM is set to 64 to ensure the
consistency of the dimension of text representation. Besides,
we use the Adam optimizer with the learning rate of 0.005
to train the networks on an NVIDIA Tesla K80 GPU with
the batch size of 128. And an early stop strategy is employed
to stop the training process when the validated indicators
fail to improve after 10,000 loops. Considering that the data-
sets are not very large and the number of cause clauses in
documents is extremely uneven, we adopt 10-folds cross-
validation and the dropout rate of 0.5 to mitigate possible
overﬁtting.
5.4 Baseline Methods
We compare our proposed CNCM with the following
groups of baselines:
/C15Rule-based methods: CB [46] is also a traditional
method that introduces commonsense knowledge
into the ECE task to reveal emotion causes. RB[11] is
an original ECE method that is based on manual
rules for emotion cause detection.
/C15Machine learning-based methods: SVM+word2vec [85]
combines the strength of SVM classiﬁer and word2-
vec embedding [86] to recognize cause clauses. SVM
+RB+CB [83] is an SVM framework [87] fused with
rules and knowledge for emotion cause identiﬁca-
tion. Multi-kernel [48] is a machine learning approach
which employs the multi-kernel classiﬁer for emo-
tion cause extraction. LambdaMART [53] transforms
the ECE task to a ranking problem and selects cause
clauses by rank criterion.Fig. 6. A document example in the Chinese dataset. The original text is
on the left, while the corresponding English translation for each clause is
listed on the right.
TABLE 2
Key Information about the Datasets
Item Number Percentage
Chinese Dataset
Documents 2,105 -
Clauses 11,799 -
Emotion cause clauses 2,167 -
Documents with 1 cause clause 2,046 97.20%
Documents with 2 cause clauses 56 2.66%
Documents with 3 cause clauses and more 3 0.14%
English Dataset
Documents 2,156 -
Clauses 13, 838 -
Emotion cause clauses 2,421 -
Documents with 1 cause clause 1,949 90.40%
Documents with 2 cause clauses 164 7.61%
Documents with 3 cause clauses and more 43 2.00%Fig. 7. The statistics of document size by clause count in the Chinese
dataset.
Fig. 8. The statistics of document size by clause count in the English
dataset.1750 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:22 UTC from IEEE Xplore.  Restrictions apply. /C15Deep learning-based methods: Memnet [52] designs a
question answering framework which understands
emotion information well by memory network to
address the ECE task. CANN [50] employs attention
mechanism to enhance text representations by cap-
turing the mutual impacts between the emotion
clause and the other clauses for the ECE task. HCS
[13] develops a hierarchical network-based clause
selection framework to model multi-granularity
semantic features to identify emotion cause. MANN
[15] is a multi-attention-based network which real-
izes the emotion cause attention and candidate
clause attention for good text understanding and
emotion cause extraction. FSS-GCN [4] is a graph
convolutional networks incorporating text semantics
and structural information. EF-BHA [18] models the
document-level context and mines clause relations
based on emotion clauses the ECE task. RHNN [14]
is a novel hierarchical neural model that fuses the
sentiment lexicon and common knowledge via
restraining parameters for the ECE task.
5.5 Effectiveness Comparison
The experimental results on the Chinese and English data-
sets are demonstrated in Tables 3 and 4. Here, we analyze
these two tables, respectively.
Performance on the Chinese Dataset. Table 3 shows the
effectiveness comparison related to the Chinese dataset. It
can be found that the Rule-based methods perform rela-
tively poorly. It is probably because they rely heavily on lin-
guistic rules or commonsense, but manual rules or
knowledge can not cover all complex linguistic phenome-
nons. Besides, the indicators of the CB method differ sub-
stantially. It may imply this approach requires more
appropriate commonsense knowledge. In contrast, machine
learning-based approaches can better mine the information
features of text corpuses, so the corresponding performan-
ces are improved to some extent. However, the performance
gains of such methods are limited because they rely on com-
plex artiﬁcial features. Evidently, the performances of deep
learning-based methods have generally been further
improved. It is probably because deep neural networks cansimulate human brains to understand semantics very well.
Among these methods, the results of RHNN and EF-BHA
are quite competitive. RHNN incorporates discourse con-
text and prior knowledge, which can contribute to emotion
semantics understanding. While EF-BHA adopts the bound-
aries detecting method of emotion cause spans, which can
reduce the range of detection clauses and improve the efﬁ-
ciency of the model. Noticeably, CNCM achieves a perfor-
mance improvement of 6.6% about the F1 score than the
advanced RHNN model. The superiority of CNCM may
attribute to its grasp of causal narrative in documents,
which enables CNCM to understand the overall emotional
causality of the document more directly and accurately.
Performance on the English Dataset. The performance com-
parison related to the English dataset is shown in Table 4. It
can be observed that the indicators of these approaches in
Table 4 are generally inferior to their indicators in Table 3.
And so does our model CNCM. That is, the models do not
perform as well on the English dataset as they do on the
Chinese dataset. It may be due to textual differences
between the two datasets. As mentioned before, the Chinese
and English datasets are derived from a news corpus and
an English novel respectively. Compared with the Chinese
dataset, the writing style of text in the English dataset is rel-
atively free and literary, which leads to its relatively compli-
cated semantics and narratives. This may account for the
generally poor performance of models in English datasets.
Moreover, as demonstrated in Section 5.1, the distribution
of document sizes in the English dataset is much more com-
plex than that in the Chinese dataset. It indicates a greater
diversity of document structures in the English dataset and
also implies more complex narratives in this dataset. Obvi-
ously, this would hinder the performance of CNCM.
Because CNCM is based on a causal narrative understand-
ing and relatively sensitive to the narrative characteristics of
texts. In summary, the characteristics of language expres-
sion and document structure in the English dataset together
lead to performance degradation of CNCM. As shown in
Table 4, although the overall performance of CNCM is
slightly weaker than that of the most advanced model EF-
BHA, it is comparable to that of the suboptimal model
RHNN.
Comprehensive Comparison. Combining Tables 3 and 4
together, we can ﬁnd that EF-BHA and RHNN are the
SOTA benchmark models. Furthermore, it can be observed
that these two SOTA benchmark models exhibit different
performance characteristics compared with CNCM on the
Chinese dataset and the English dataset, which are in line
with the analysis above. Thus, to scientiﬁcally evaluate
whether our CNCM model is superior to existing models,TABLE 3
Performances of Different Models on the Chinese Dataset
Model Precision Recall F1
(1) CB [46] 26.72% 71.30% 38.87%
(2) RB [11] 67.47% 42.87% 52.43%
(3) SVM+word2vec [85] 43.01% 42.33% 41.36%
(4) SVM+RB+CB [83] 59.21% 53.07% 55.97%
(5) Multi-kernel [48] 65.88% 69.27% 67.52%
(6) LambdaMART [53] 77.20% 74.99% 76.08%
(7) Memnet [52] 70.76% 68.38% 69.55%
(8) CANN [50] 77.21% 68.91% 72.66%
(9) HCS [13] 73.88% 71.54% 72.69%
(10) MANN [15] 78.43% 75.87% 77.06%
(11) FSS-GCN [4] 78.61% 75.72% 77.14%
(12) EF-BHA [54] 79.38% 78.08% 78.68%
(13) RHNN [14] 81.12% 77.25% 79.14%
(14) CNCM 93.97% 78.85% 85.75%TABLE 4
Performances of Different Models on the English Dataset
Model Precision Recall F1
(1) Memnet [52] 46.05% 41.77% 43.81%
(2) MANN [15] 79.33% 40.81% 53.28%
(3) FSS-GCN [4] 67.43% 53.03% 59.48%
(4) RHNN [14] 69.01% 52.67% 59.75%
(5) EF-BHA [54] 72.77% 53.05% 61.37 %
(6) CNCM 57.69% 62.10 % 59.79%CAO ET AL.: CAUSAL NARRATIVE COMPREHENSION: A NEW PERSPECTIVE FOR EMOTION CAUSE EXTRACTION 1751
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:22 UTC from IEEE Xplore.  Restrictions apply. we perform holistic t-tests of the overall performance for
CNCM and these two SOTA benchmark models (EF-BHA
and RHNN). Through the t-tests, we can acquire over 95%,
and 99% of conﬁdence that CNCM has signiﬁcant improve-
ment over EF-BHA and RHNN, respectively, which indi-
cates that CNCM has certain superiority.
5.6 Ablation Study
To conﬁrm the effectiveness of each unit in CNCM, we con-
duct ablation experiments via removing or replacing the fol-
lowing three aspects: the causal narrative representations of
NCA, the pre-training model for text embeddings, and the
context-aware emotion attention of REA. The related results
are shown in Table 5.
5.6.1 Causal Narrative Representation of NCA
As stated above, the essential innovation of our proposed
model is the causal narrative representations of the NCA
unit. To examine the effect of this innovation on the ﬁnal
experimental performance, we conduct experiments on the
ablation model CNCM (w/o NCA), which is derived from
CNCM by removing the NCA unit. As shown in Table 5,
CNCM (w/o NCA) without causal narrative representa-
tions has the worst performance. It suggests that the causal
narrative representations of causal texts could greatly con-
tribute to the performance improvement of the ECE task. It
might be because the causal narrative representation of the
NCA unit could guide CNCM to focus on the emotion cause
region which is more related to the emotion result clause.
5.6.2 Pre-Trained Model for Text Embedding
As we know, BERT can output outstanding text representa-
tions which have been shown to be very effective in many
downstream tasks of natural language processing. Owing to
this reason, we achieve the initial vectorization of texts in
CNCM through BERT’s evolution model BERT-wwm. To
explore the importance of the pre-trained language model
on the ECE task, we replace BERT-wwm with another pop-
ular pre-trained language model, word2vec [86] to conduct
experiments. The corresponding ablation model is CNCM
(w/o BERT). As can be seen from Table 5, the performance
degradation of CNCM (w/o BERT) is very small. It seems
to indicate the superiority of CNCM may not be mainly
attributed to BERT-wwm.
Besides, it can be observed that CNCM (w/o BERT) per-
forms quite well without BERT. For one thing, this may ben-
eﬁt from the contribution of the NCA unit. As shown in
Table 5, CNCM (w/o BERT) has the smallest performance
degradation, while CNCM (w/o NCA) has the largestperformance degradation. It demonstrates that the NCA
unit might contribute more to the effectiveness of CNCM
than BERT. For another, due to the nature of high complex-
ity and lack of large scale corpora, it is usually difﬁcult to
train BERT-based models on small scale datasets of the ECE
task. Therefore, our proposed CNCM can still perform well
on the ECE task without BERT.
5.6.3 Causal Association Cognition of REA
Inspired by the reading habits of human beings [79], CNCM
twice utilizes the REA unit to understand the emotional
causal association of causal narrative. Actually, the REA
unit in the layer of emotion causality understanding is
designed to obtain preliminary cognition about the emo-
tional causal association of documents. By comparison, the
REA in the layer of emotion causality re-understanding
aims to better understand the emotional causal association
with the guidance of the preliminary cognition and the
causal narrative representations of documents. The pro-
posed ablation models are the model CNCM (w/o REA_1)
and CNCM (w/o REA_2), where CNCM (w/o REA_1) is
the evolution model without the REA unit of the prelimi-
nary understanding phrase, and CNCM (w/o REA_2) is the
one without the REA unit of the emotion causality re-under-
standing phrase. As shown in Table 5, after removing one
REA unit respectively, these two evolution models achieve
lower performance than CNCM. It implies that the REA
unit has a great inﬂuence on our model.
Another meaningful observation in Table 5 is that the
performance degradation of CNCM (w/o REA_2) is higher
than the one of CNCM (w/o REA_1). It demonstrates that
the second REA unit is something more important than the
ﬁrst one. Perhaps because the second REA unit is in the last
stage of CNCM, while the ﬁrst REA is in the initial stage of
CNCM. Thus, the second REA unit has a more direct effect
on CNCM than the ﬁrst REA.
5.7 Detailed Analysis
In this subsection, we carry out some supplementary experi-
ments to give a detailed analysis of our proposed model
from multiple aspects.
5.7.1 Effects of Cause-Result Order
According to studies about narratology [22], [28], the cause-
result order of causal narrative is either chronological narra-
tive (cause–before–effect) or ﬂashback narrative (effect–
before–cause). Especially in our Chinese dataset, as shown
in Table 6, the former and the latter account for 65.75% and
34.25%. This imbalance may be due to the fact that events
typically are presented in chronological order in narratives
[88]. Because this is consistent with the laws of events
development.
Furthermore, we discuss the effects of the balance about
cause-result order on the performance of the ECE task. As
the results demonstrated in Fig. 9, “the ratio” denotes the
ratio of chronological narrative and ﬂashback narrative in
the corresponding experiment. While “Rate_Actual” refers
to this ratio in the original Chinese dataset. Obviously,
CNCM performs best when the contents of ECE documents
all conform to the chronological narrative. It may indicateTABLE 5
Ablation Performances of CNCM
Model Precision Recall F1
(1) CNCM (w/o NCA) 90.22% 74.25% 80.89%
(2) CNCM (w/o BERT) 89.71% 80.38% 84.75%
(3) CNCM (w/o REA_1) 82.72% 82.53% 82.62%
(4) CNCM (w/o REA_2) 87.11% 77.70% 82.14%
(5) CNCM 93.97% 78.85% 85.75%1752 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:22 UTC from IEEE Xplore.  Restrictions apply. that CNCM is better at understanding texts with chronolog-
ical narrative. This perhaps has something with the fact that
chronological order is more consistent with the general
norms and basic organization principles of narratives [88].
Signiﬁcantly, balanced data usually has a positive effect
on experimental performance. Yet, as shown in Fig. 9,
CNCM performs worst when there is an equal proportion
of chronological narrative and ﬂashback narrative. This
anomaly may be because the complex narrative in texts
would increase the difﬁculty of semantic understanding.
Comparatively speaking, the other experiments perform
better than the one with balanced data. The reason may be
that text semantics is easier to learn and master when the
narrative mode of the corpus is relatively simple. The analy-
sis of this anomaly may also be useful for the semantic
understanding of other long texts. Namely, the simpler the
linguistic patterns of the text are, the easier it is to learn.
5.7.2 Effects of Clause Distance
It is worthwhile to mention that the distance between emo-
tion result clauses and cause clauses is one of the signiﬁcant
attributes of causal structure. Thus, we conduct relevant
data statistics and experiments to explore the effects of this
factor on the performance of CNCM.
Fig. 10 shows the distance statistics of emotion cause
clauses relative to their emotion result clauses in the Chi-
nese dataset. It can be observed that the positions of emo-
tion cause clauses relative to the emotion result clauses
within a document are usually no more than 3 clauses.
Based on this observation, we choose the documents whose
emotion cause clauses are no more than 1 or 2 or 3 clauses
away from the emotion result clauses to construct three
sub-datasets. The performance of CNCM on these 3 datasets
is shown in Table 7. In this table, the term “Radius_1” indi-
cates the document set, in which the distance of emotion
cause clauses relative to emotion result clauses is no more
than 1 clause. The same principle goes for the term“Radius_2” and “Radius_3”. Obviously, the data involved
in “Radius_3” covers the vast majority of documents in the
Chinese dataset, which is not difﬁcult to explain why the
performance of the “Radius_3” shown in Table 7 is close to
that of the “Original Dataset”. Additionally, the table also
shows that the closer the distance between emotion cause
clauses and emotion result clauses, the better the perfor-
mance of CNCM. It may be due to that the closer distance
between cause and result in a document, the tighter the
causal semantics of the document. While the tight causal
semantics implies that there is less information loss in the
process of modeling causal structures. This would help to
understand causal associations and identify emotion causes.
5.7.3 Effects of Document Size
Considering that document size is one of the important fac-
tors in determining the narrative complexity of a document,
we also explore the effects of document size on CNCM.
According the statistics of document size by clause count in
Fig. 7, we select the document size in four ranges, i.e,3-4, 5,
6, 7-12 to conduct experiments. The corresponding results
are shown in Fig. 11. Notably, CNCM performs best on all
evaluation indicators when the document size amounts to 6
clauses. When the document size is less than or greater than
6, the results on all evaluation indicators correspondinglyTABLE 6
Statistics of Cause-Result Order in the Chinese Dataset
Item Number Percentage
Documents with chronological narrative 1,384 65.75%
Documents with ﬂashback narrative 721 34.25%
All documents 2105 100.00%
Fig. 9. The effects of cause-result order on the performance of CNCM.
Here, the term “ratio” denotes the ratio of chronological narrative
(cause–before–effect) and ﬂashback narrative (effect–before–cause) in
the Chinese dataset.Fig. 10. The distance statistics of cause clauses relative to result clauses
in the Chinese dataset. Here, the positive distance indicates that the
cause clause comes before the result clause, while the negative distance
is the opposite. If a document contains multiple cause clauses, we select
its ﬁrst cause clause for statistics.
TABLE 7
Performances of CNCM on Datasets with Different Distances of
Emotion Cause Clauses Relative to Emotion Result Clauses
Dataset Precision Recall F1
(1) Radius_1 93.00% 89.12% 90.97%
(2) Radius_2 92.25% 87.56% 89.75%
(3) Radius_3 91.89% 80.86% 85.91%
(4) Original Dataset 93.97% 78.85% 85.75%
Fig. 11. The effects of document size on the performance of CNCM.
Here, the document size refers to the clause count of documents in the
Chinese dataset.CAO ET AL.: CAUSAL NARRATIVE COMPREHENSION: A NEW PERSPECTIVE FOR EMOTION CAUSE EXTRACTION 1753
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:22 UTC from IEEE Xplore.  Restrictions apply. decrease. It seems that moderate document size could help
to document understanding. On one hand, the few clauses
the document contains, the semantic information is less.
Evidently, this case is not conducive to emotion cause iden-
tiﬁcation of the ECE task because there is less information
available for document understanding. On the other hand,
the more clauses a document contains, the more complex its
semantics and narrative become. In this case, document
understanding becomes difﬁcult, which may lead to the
performance degradation of the ECE task.
Also note that the set of best results in Fig. 11 is even bet-
ter than the results under the actual distribution of docu-
ment size in the original dataset. It may be because various
document sizes lead to complex narratives, which increase
the difﬁculty of emotional semantics understanding. Fur-
thermore, although the training data size of the former is
smaller than that of the latter, the performance of the former
is better than that of the latter. This is consistent with the
ﬁndings in the previous analysis that the amount of training
data does not signiﬁcantly affect the performance of CNCM.
5.7.4 Effects of Emotion Category
In order to study whether CNCM had a bias for emotion
categories when performing the ECE task, we also under-
take a statistical analysis of emotion category. Taking the
Chinese dataset as an example, ﬁrst, we use the dictionary-
based approach to identify the emotion of the documents in
the dataset. Here, we use the Chinese emotion ontology
database [89] to classify the emotions of these documents
into seven categories: “glad”, “good”, “angry”, “sad”,
“afraid”, “bad” and “amazed”. Second, we conduct statis-
tics of emotion category for documents in the Chinese data-
set and its test set, respectively. Finally, we perform the
same statistics for the documents whose emotion cause
clauses are correctly predicted in the test set. The statistical
results are shown in Fig. 12. It can be remarked that there is
little difference in the distribution of emotion categories of
documents on the three datasets. This may suggest that our
proposed model is insensitive to emotion categories.
5.7.5 Effects of Training Dataset Scale
To present the performance of CNCM systematically, we
compare the results under different scales of the training
dataset for our developed model. Taking the Chinese data-
set as an example, the corresponding performances areshown in Table 8. Considering that the training dataset of
the original experiment accounts for 80% of the total data in
the Chinese dataset, we take into account the other three
training data settings: 20%, 40%, and 60% of the total data.
To be speciﬁc, we implement experiments under these
training data settings and compare the corresponding per-
formances to the original experiment. It can be found from
Table 8 that the experimental performances of CNCM under
different scales of the training dataset have little difference.
These ﬁndings could indicate that our proposed model is
still effective in the case of small training data.
5.8 Case Studies
To provide some intuitive demonstrations of how causal
narrative representation and emotional causal association
improve the effectiveness of our model, we show some case
studies in Fig. 13 to interpret what is happening in the
working ﬂow of CNCM. Fig. 13 A presents a correct
instance, whose predicted cause label is the same as the
truth label. In particular, as shown by the causal narrative
representation of CNCM in the third column of this ﬁgure,
the weight of the ﬁrst alternative cause region is greater
than that of the second region. It suggests the cause clause
may be located in the ﬁrst alternative cause region. This
inference about the cause region is consistent with the
ground truth. Moreover, in the fourth column of Fig. 13 A,
the data highlighted by color represent the distribution of
emotional attention of the last REA unit. Evidently, the dis-
tribution of emotional attention at clause 4 is larger than
others. It can be conjectured that clause 4 should be the
cause clause since it is most relevant to the result clause
than other clauses. The above inferences are consistent with
the ground truth, which illustrates CNCM can effectively
locate cause regions and focus on clauses that are more rele-
vant to emotion result clauses. Similarly, there is another
instance shown in Fig. 13 B to illustrate the working details
of CNCM.
In addition, we also provide two error cases to illustrate
the existing problems of CNCM. As shown in the third col-
umn of Fig. 13 C, although our model could identify the
accurate causal region by the learned causal narrative repre-
sentation, the emotional causal association learned from the
ﬁnal emotional attention is biased. Therefore, CNCM misi-
dentiﬁes the adjacent non-cause clause of the real cause as a
cause clause. Notably, the incorrect prediction sentence is
located very close to the actual cause clause. It indicates that
CNCM tends to be misled by the adjacent clauses of the
true cause clause. As a further exploration, we aim to
address this issue in the future.Fig. 12. The statistics of emotion category in the Chinese dataset.TABLE 8
Performances of CNCM under Different Scales of Training
Dataset of the Chinese Dataset
Training data size Precision Recall F1
20% of Original Dataset 92.63% 76.12% 83.57%
40% of Original Dataset 92.48% 77.62% 84.40%
60% of Original Dataset 93.11% 79.29% 85.64%
80% of Original Dataset 93.97% 78.85% 85.75%1754 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:22 UTC from IEEE Xplore.  Restrictions apply. 6C ONCLUSION AND FUTURE WORK
In this work, we focused on emotion cause extraction and
argued that causal narrative comprehension is very impor-
tant to this issue. To this end, we proposed a novel Causal
Narrative Comprehension Model (CNCM) based on Causal
Narrative Comprehension to address this task. Different
from the previous works focusing on semantic understand-
ing of clauses and emotion phrases, our proposed model
focused on modeling and utilizing the causal narratives of
documents to learn emotional causal association among
clauses for emotion cause extraction. Speciﬁcally, CNCM
utilized causal narrative to deﬁne cause regions and
obtained causal narrative representations based on narra-
tive coherence of the causes and the results. Guided by the
representation of causal narrative, we developed a result-
aware emotional attention unit to understand the emotional
causal association multiple times, so as to realize the task of
emotion cause detection. Extensive experimental results on
the benchmark datasets demonstrated the effectiveness of
CNCM for the ECE task.In the future, we will strive to develop a multilingual cor-
pus of the ECE task to reﬁne our studies. Based on this, we
also hope to conduct further research on much more general
narrative material and attempt to make utilization of the
narrative information to promote some appropriate tasks
about text semantic understanding.
REFERENCES
[1] S. Ruan et al., “Context-aware generation-based net for multi-label
visual emotion recognition,” in Proc. IEEE Int. Conf. Multimedia
Expo , 2020, pp. 1–6.
[2] Z. Ding, H. He, M. Zhang, and R. Xia, “From independent predic-
tion to reordered prediction: Integrating relative position and
global label information to emotion cause identiﬁcation,” in Proc.
AAAI Conf. Artif. Intell. , 2019, pp. 6343–6350.
[3] Q. Gao et al., “Overview of ntcir-13 ECA task,” in Proc. 13th NII
Testbeds Community Informat. Access Res. , 2017, pp. 165–192.
[4] G. Hu, G. Lu, and Y. Zhao, “FSS-GCN: A graph convolutional net-
works with fusion of semantic and structure for emotion cause
analysis,” Knowl.-Based Syst. , vol. 212, 2021, Art. no. 106584.
[5] S. Ruan, K. Zhang, L. Wu, T. Xu, Q. Liu, and E. Chen, “Color
enhanced cross correlation net for image sentiment analysis,” IEEE
Trans. Multimedia ,t ob ep u b l i s h e d ,d o i : 10.1109/TMM.2021.3118208 .
Fig. 13. Some examples of emotion cause extraction by CNCM. Figure (A) and (B) show correct cases, while ﬁgure (C) illustrates an error case.CAO ET AL.: CAUSAL NARRATIVE COMPREHENSION: A NEW PERSPECTIVE FOR EMOTION CAUSE EXTRACTION 1755
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:22 UTC from IEEE Xplore.  Restrictions apply. [6] K. Zhang, L. Wu, G. Lv, M. Wang, E. Chen, and S. Ruan, “Making
the relation matters: Relation of relation learning network for sen-
tence semantic matching,” in Proc. AAAI Conf. Artif. Intell. , 2021,
pp. 14 411–14 419.
[ 7 ] J .H u ,Y .L i u ,J .Z h a o ,a n dQ .J i n ,“ M M G C N :M u l t i m o d a l
fusion via deep graph convolution network for emotion recog-
nition in conversation,” in Proc. 59th Annu. Meeting Assoc.
Comput. Linguistics 11th Int. Joint Conf. Natural Lang. Process. ,
2021, pp. 5666–5675.
[8] D. Li et al., “Enhancing emotion inference in conversations with
commonsense knowledge,” Knowl.-Based Syst. , vol. 232, 2021,
Art. no. 107449.
[9] Y. Chen, S. Y. M. Lee, S. Li, and C.-R. Huang, “Emotion cause
detection with linguistic constructions,” in Proc. 23rd Int. Conf.
Comput. Linguistics , 2010, pp. 179–187.
[10] L. Gui, L. Yuan, R. Xu, B. Liu, Q. Lu, and Y. Zhou, “Emotion cause
detection with linguistic construction in chinese weibo text,” in
Proc. CCF Int. Conf. Natural Lang. Process. Chin. Comput. , 2014,
pp. 457–464.
[11] S. Y. M. Lee, Y. Chen, and C.-R. Huang, “A text-driven rule-
based system for emotion cause detection,” in Proc. NAACL
HLT Workshop Comput. Approaches Anal. Gener. Emotion Text ,
2010, pp. 45–53.
[12] D. Ghazi, D. Inkpen, and S. Szpakowicz, “Detecting emotion stim-
uli in emotion-bearing sentences,” in Proc. Int. Conf. Intell. Text
Process. Comput. Linguistics , 2015, pp. 152–165.
[13] X. Yu, W. Rong, Z. Zhang, Y. Ouyang, and Z. Xiong, “Multiple
level hierarchical network-based clause selection for emotion
cause extraction,” IEEE Access , vol. 7, pp. 9071–9079, 2019.
[14] R. Xia, M. Zhang, and Z. Ding, “RTHN: A RNN-transformer hier-
archical network for emotion cause extraction,” in Proc. 28th Int.
Joint Conf. Artif. Intell. , 2019, pp. 5285–5291.
[15] X. Li, S. Feng, D. Wang, and Y. Zhang, “Context-aware emotion
cause analysis with multi-attention-based neural network,”
Knowl.-Based Syst. , vol. 174, pp. 205–218, 2019.
[16] S. Hochreiter and J. Schmidhuber, “Long short-term memory,”
Neural Comput. , vol. 9, no. 8, pp. 1735–1780, 1997.
[17] Y. LeCun et al., “Backpropagation applied to handwritten zip
code recognition,” Neural Comput. , vol. 1, no. 4, pp. 541–551, 1989.
[18] X. Li, W. Gao, S. Feng, Y. Zhang, and D. Wang, “Boundary detec-
tion with bert for span-level emotion cause analysis,” in Proc.
Findings Assoc. Comput. Linguistics , 2021, pp. 1–6.
[19] C. Fan et al., “A knowledge regularized hierarchical approach for
emotion cause analysis,” in Proc. Conf. Empir. Methods Natural
Lang. Process. 9th Int. Joint Conf. Natural Lang. Process. , 2019,
pp. 5614–5624.
[20] J. M. Keenan, S. D. Baillet, and P. Brown, “The effects of causal
cohesion on comprehension and memory,” J. Verbal Learn. Verbal
Behav. , vol. 23, no. 2, pp. 115–126, 1984.
[21] S. E. Pickren, M. Stacy, S. N. Del Tufo, M. Spencer, and L. E. Cut-
ting, “The contribution of text characteristics to reading compre-
hension: Investigating the inﬂuence of text emotionality,” Reading
Res. Quart. , vol. 57, pp. 649–667, 2021.
[22] E. Kaiser, “Order of mention in causal sequences: Talking about
cause and effect in narratives and warning signs,” Discourse Pro-
cesses , vol. 56, no. 8, pp. 599–618, 2019.
[23] T. Sanders, “Coherence, causality and cognitive complexity in dis-
course,” in Proc. 1st Int. Symp. Exploration Modelling Meaning , 2005,
pp. 105–114.
[24] T. Sanders and W. Spooren, “The cognition of discourse coherence,”
Discourse Course . Amsterdam, The Netherlands: John Benjamins,
2009, pp. 197–212.
[25] M. B. Wolfe, J. P. Magliano, and B. Larsen, “Causal and semantic
relatedness in discourse understanding and representation,” Dis-
course Processes , vol. 39, no. 2/3, pp. 165–187, 2005.
[26] P. Van den Broek, “The causal inference maker: Towards a pro-
cess model of inference generation in text comprehension,” Com-
prehension Processes in Reading . Mahwah, NJ, USA: L. Erlbaum,
1990, pp. 423–445.
[27] C. R. Fletcher and C. P. Bloom, “Causal reasoning in the compre-
hension of simple narrative texts,” J. Memory Lang. , vol. 27, no. 3,
pp. 235–244, 1988.
[28] T. Hoffmann, “Construction grammar as cognitive structuralism:
The interaction of constructional networks and processing in the
diachronic evolution of english comparative correlatives,” English
Lang. Linguistics , vol. 21, no. 2, pp. 349–373, 2017.[29] T. Trabasso and P. Van Den Broek, “Causal thinking and the
representation of narrative events,” J. Memory Lang. , vol. 24, no. 5,
pp. 612–630, 1985.
[30] B. J. Grosz, A. Joshi, and S. Weinstein, “Centering: A framework
for modeling the local coherence of discourse,” Comput. Linguis-
tics, vol. 21, no. 2, pp. 203–225, 1995.
[31] B. Wellner, J. Pustejovsky, C. Havasi, A. Rumshisky, and R. Sauri,
“Classiﬁcation of discourse coherence relations: An exploratory
study using multiple knowledge sources,” in Proc. 7th SIGDIAL
Workshop Discourse Dialogue , 2006, pp. 117–125.
[32] G. Mulder and T. J. Sanders, “Causal coherence relations and lev-
els of discourse representation,” Discourse Processes , vol. 49, no. 6,
pp. 501–522, 2012.
[33] J. R. Anderson, Cognitive Psychology and its Implications . New York,
NY, USA: Macmillan, 2005.
[34] M. Dragoni, I. Donadello, and E. Cambria, “OntoSenticNet 2:
Enhancing reasoning within sentiment analysis,” IEEE Intell.
Syst., vol. 37, no. 2, pp. 103–110, Mar./Apr. 2022.
[35] A. Esuli, A. Moreo, and F. Sebastiani, “Cross-lingual sentiment
quantiﬁcation,” IEEE Intell. Syst. , vol. 35, no. 3, pp. 106–114, May/
Jun. 2020.
[36] M. Thelwall, “This! identifying new sentiment slang through
orthographic pleonasm online: Yasss slay gorg queen ilysm,”
IEEE Intell. Syst. , vol. 36, no. 4, pp. 114–120, Jul./Aug. 2021.
[37] W. Li, W. Shao, S. Ji, and E. Cambria, “Bieru: Bidirectional emo-
tional recurrent unit for conversational sentiment analysis,” Neu-
rocomputing , vol. 467, pp. 73–82, 2022.
[38] S. Modi and M. H. Bohara, “Facial emotion recognition using con-
volution neural network,” in Proc. 5th Int. Conf. Intell. Comput.
Control Syst. , 2021, pp. 1339–1344.
[39] S. Zhao et al., “A two-stage 3D CNN based learning method for
spontaneous micro-expression recognition,” Neurocomputing ,
vol. 448, pp. 276–289, 2021.
[40] W. Peng, X. Hong, and G. Zhao, “Adaptive modality distillation
for separable multimodal sentiment analysis,” IEEE Intell. Syst. ,
vol. 36, no. 3, pp. 82–89, May/Jun. 2021.
[41] N. Xu, W. Mao, P. Wei, and D. Zeng, “MDA: Multimodal data
augmentation framework for boosting performance on senti-
ment/emotion classiﬁcation tasks,” IEEE Intell. Syst. , vol. 36,
no. 6, pp. 3–12, Nov./Dec. 2021.
[42] A. Zadeh, R. Zellers, E. Pincus, and L.-P. Morency, “Multimodal
sentiment intensity analysis in videos: Facial gestures and verbal
messages,” IEEE Intell. Syst. , vol. 31, no. 6, pp. 82–88, Nov./Dec.
2016.
[43] K. Zhang, Y. Li, J. Wang, E. Cambria, and X. Li, “Real-time video
emotion recognition based on reinforcement learning and domain
knowledge,” IEEE Trans. Circuits Syst. Video Technol. , vol. 32,
no. 3, pp. 1034–1047, Mar. 2022.
[44] Y. M. Lee, Y. Chen, S. Li, and C.-R. Huang, “Emotion cause events:
Corpus construction and analysis,” in Proc. 7th Int. Conf. Lang.
Resour. Eval. , 2010, pp. 1121–1128.
[45] A. Neviarouskaya and M. Aono, “Extracting causes of emotions
from text,” in Proc. 6th Int. Joint Conf. Natural Lang. Process. , 2013,
pp. 932–936.
[46] I. Russo, T. Caselli, F. Rubino, E. Boldrini, and P. Mart /C19ınez-Barco,
“Emocause: An easy-adaptable approach to extract emotion cause
contexts,” in Proc. 2nd Workshop Comput. Approaches Subjectivity
Sentiment Anal. , 2011, pp. 153–160.
[47] K. Gao, H. Xu, and J. Wang, “Emotion cause detection for chinese
micro-blogs based on ecocc model,” in Proc. Paciﬁc-Asia Conf.
Knowl. Discov. Data Mining , 2015, pp. 3–14.
[48] L. Gui, D. Wu, R. Xu, Q. Lu, and Y. Zhou, “Event-driven emotion
cause extraction with corpus construction,” in Proc. Conf. Empir.
Methods Natural Lang. Process. , 2016, pp. 1639–1649.
[49] M. Schuster and K. K. Paliwal, “Bidirectional recurrent neural
networks,” IEEE Trans. Signal Process. , vol. 45, no. 11, pp. 2673–2681,
Nov. 1997.
[50] X. Li, K. Song, S. Feng, D. Wang, and Y. Zhang, “A co-attention
neural network model for emotion cause analysis with emotional
context awareness,” in Proc. Conf. Empir. Methods Natural Lang.
Process. , 2018, pp. 4752–4757.
[51] D. Bahdanau, K. H. Cho, and Y. Bengio, “Neural machine transla-
tion by jointly learning to align and translate,” 2014, arXiv:1409.0473 .
[52] L. Gui, J. Hu, Y. He, R. Xu, Q. Lu, and J. Du, “A question answer-
ing approach for emotion cause extraction,” in Proc. 2017 Conf.
Empir. Methods Natural Lang. Process. , 2017, pp. 1593–1602.1756 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:22 UTC from IEEE Xplore.  Restrictions apply. [53] B. Xu, H. Lin, Y. Lin, Y. Diao, L. Yang, and K. Xu, “Extracting
emotion causes using learning to rank methods from an informa-
tion retrieval perspective,” IEEE Access , vol. 7, pp. 15 573–15 583,
2019.
[54] G. Hu, G. Lu, and Y. Zhao, “Bidirectional hierarchical attention
networks based on document-level context for emotion cause
extraction,” in Proc. Findings Assoc. Comput. Linguistics , 2021,
pp. 558–568.
[55] R. Xia and Z. Ding, “Emotion-cause pair extraction: A new task to
emotion analysis in texts,” in Proc. 57th Annu. Meeting Assoc. Com-
put. Linguistics , 2019, pp. 1003–1012.
[56] Z. Ding, R. Xia, and J. Yu, “Ecpe-2D: Emotion-cause pair extrac-
tion based on joint two-dimensional representation, interaction
and prediction,” in Proc. 58th Annu. Meeting Assoc. Comput. Lin-
guistics , 2020, pp. 3161–3170.
[57] X. Chen, Q. Li, and J. Wang, “A uniﬁed sequence labeling model
for emotion cause pair extraction,” in Proc. 28th Int. Conf. Comput.
Linguistics , 2020, pp. 208–218.
[58] C. Fan, C. Yuan, J. Du, L. Gui, M. Yang, and R. Xu, “Transition-
based directed graph construction for emotion-cause pair extrac-
tion,” in Proc. 58th Annu. Meeting Assoc. Comput. Linguistics , 2020,
pp. 3707–3717.
[59] P. Wei, J. Zhao, and W. Mao, “Effective inter-clause modeling for
end-to-end emotion-cause pair extraction,” in Proc. 58th Annu.
Meeting Assoc. Comput. Linguistics , 2020, pp. 3171–3181.
[60] Y. Chen, W. Hou, X. Cheng, and S. Li, “Joint learning for emotion
classiﬁcation and emotion cause detection,” in Proc. Conf. Empir.
Methods Natural Lang. Process. , 2018, pp. 646–651.
[61] J. W. Orr, “Towards narrative understanding with deep neural
networks and hidden Markov models,” Ph.D. dissertation, Ore-
gon State University, Corvallis, OR, USA, 2019.
[62] S. Benini, M. Savardi, K. B /C19alint, A. B. Kov /C19acs, and A. Signoroni,
“On the inﬂuence of shot scale on ﬁlm mood and narrative
engagement in ﬁlm viewers,” IEEE Trans. Affect. Comput. , vol. 13,
no. 2, pp. 592–603, Apr./Jun. 2022.
[63] J. Tarvainen, J. Laaksonen, and T. Takala, “Film mood and its
quantitative determinants in different types of scenes,” IEEE
Trans. Affect. Comput. , vol. 11, no. 2, pp. 313–326, Apr./Jun. 2020.
[64] J. Tang et al., “From discourse to narrative: Knowledge projection
for event relation extraction,” 2021, arXiv:2106.08629 .
[65] J. Tourille, O. Ferret, A. Neveol, and X. Tannier, “Neural architec-
ture for temporal relation extraction: A Bi-LSTM approach for
detecting narrative containers,” in Proc. 55th Annu. Meeting Assoc.
Comput. Linguistics , 2017, pp. 224–230.
[66] A. Gupta, H. Abi-Akl, and H. De Mazancourt, “Not all titles are
created equal: Financial document structure extraction shared
task,” in Proc. 3rd Financial Narrative Process. Workshop , 2021,
pp. 86–88.
[67] T. Ko /C20cisky`et al., “The narrativeqa reading comprehension
challenge,” Trans. Assoc. Comput. Linguistics , vol. 6, pp. 317–328,
2018.
[68] J. Fitzgerald and D. L. Spiegel, “Enhancing children’s reading com-
prehension through instruction in narrative structure,” J. Reading
Behav. , vol. 15, no. 2, pp. 1–17, 1983.
[69] S. Babayi /C21git, S. Roulstone, and Y. Wren, “Linguistic comprehen-
sion and narrative skills predict reading ability: A 9-year longitu-
dinal study,” Brit. J. Educ. Psychol. , vol. 91, no. 1, pp. 148–168, 2021.
[70] R. C. Schank and R. P. Abelson, Scripts, Plans, Goals, and Under-
standing: An Inquiry Into Human Knowledge Structures , London,
U.K.: Psychology Press, 2013.
[71] X. Zhou, S. Luo, and Y. Wu, “Co-attention hierarchical network:
Generating coherent long distractors for reading comprehension,”
inProc. AAAI Conf. Artif. Intell. , 2020, pp. 9725–9732.
[72] A. Bolte, T. Goschke, and J. Kuhl, “Emotion and intuition: Effects
of positive and negative mood on implicit judgments of semantic
coherence,” Psychol. Sci. , vol. 14, no. 5, pp. 416–421, 2003.
[73] J. Chen, J. Chen, and Z. Yu, “Incorporating structured common-
sense knowledge in story completion,” in Proc. AAAI Conf. Artif.
Intell. , 2019, pp. 6244–6251.
[74] D. Herremans and E. Chew, “Morpheus: Generating structured
music with constrained patterns and tension,” IEEE Trans. Affect.
Comput. , vol. 10, no. 4, pp. 510–523, Oct./Dec. 2019.
[75] S. Cumming, G. Greenberg, and R. Kelly, “Conventions of view-
point coherence in ﬁlm,” Philosophers , vol. 17, no. 1, pp. 1–29, 2017.
[76] Z. Hu and M. Walker, “Inferring narrative causality between
event pairs in ﬁlms,” in Proc. 18th Annu. SIGDIAL Meeting Dis-
course Dialogue , 2017, pp. 342–351.[77] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-
training of deep bidirectional transformers for language under-
standing,” in Proc. Conf. North Amer. Chapter Assoc. Comput. Lin-
guistics Hum. Lang. Technol. , 2019, pp. 4171–4186.
[78] Y. Cui et al., “Pre-training with whole word masking for chinese
bert,” 2019, arXiv:1906.08101 .
[79] J. Langer, “The reading process,” Secondary School Reading: What
Research Reveals for Classroom Practice , Berlin, Germany: Springer,
1982, pp. 39–52.
[80] J. L. Myers, M. Shinjo, and S. A. Duffy, “Degree of causal related-
ness and memory,” J. Memory Lang. , vol. 26, no. 4, pp. 453–465, 1987.
[81] T. Trabasso and M. Langston, “Modeling causal integration and
availability of information during comprehension of narrative
texts,” Construction Ment. Representations During Reading , vol. 25,
pp. 25–59, 1998.
[82] P. van Den Broek, B. Linzie, C. Fletcher, and C. J. Marsolek, “The
role of causal discourse structure in narrative writing,” Memory
Cogn. , vol. 28, no. 5, pp. 711–721, 2000.
[83] R. Xu, J. Hu, Q. Lu, D. Wu, and L. Gui, “An ensemble approach for
emotion cause detection with event extraction and multi-kernel
SVMs,” Tsinghua Sci. Technol. , vol. 22, no. 6, pp. 646–659, 2017.
[84] G. Montavon, G. Orr, and K.-R. M €uller, Neural Networks: Tricks of
the Trade , Berlin, Germany: Springer, 2012, vol. 7700.
[85] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean,
“Distributed representations of words and phrases and their
compositionality,” in Proc. Adv. Neural Informat. Process. Syst. ,
2013, pp. 3111–3119.
[86] T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Efﬁcient estimation
of word representations in vector space,” 2013, arXiv:1301.3781 .
[87] M. A. Hearst, S. T. Dumais, E. Osuna, J. Platt, and B. Scholkopf,
“Support vector machines,” IEEE Intell. Syst. Their Appl. , vol. 13,
no. 4, pp. 18–28, Jul./Aug. 1998.
[88] K. Bardovi-Harlig, “Reverse-order reports and the acquisition of
tense: Beyond the principle of chronological order,” Lang. Learn. ,
vol. 44, no. 2, pp. 243–282, 1994.
[89] L. Xu and H. Lin, “Ontology-driven affective chinese text analysis
and evaluation method,” in Proc. Int. Conf. Affect. Comput. Intell.
Interaction , 2007, pp. 723–724.
Wei Cao is currently working toward the PhD
degree in data mining with the School of Com-
puter Science and Technology, University of Sci-
ence and Technology of China, Hefei, China. Her
current research interests include affective com-
puting, computer vision, natural language proc-
essing and text mining.
Kun Zhang received the PhD degree in com-
puter science and technology from the University
of Science and Technology of China, Hefei,
China, in 2019. He is is currently a faculty mem-
ber with the Hefei University of Technology
(HFUT), China. His research interests include
natural language processing, Recommendation
System, and text mining. He has published sev-
eral papers in refereed conference proceedings
such as AAAI, KDD, ICDM. He received the KDD
2018 Best Student Paper Award.
Shulan Ruan received the BS degree from
Hunan University, Changsha, China, in 2018. He
is currently working toward the PhD degree with
the School of Computer Science and Technology,
University of Science and Technology of China,
Hefei, China. His research interests include senti-
ment analysis, computer vision and natural lan-
guage processing. He has published several
papers in IEEE Transactions on Multimedia ,
ICCV , AAAI, ICME, BIBM, etc.CAO ET AL.: CAUSAL NARRATIVE COMPREHENSION: A NEW PERSPECTIVE FOR EMOTION CAUSE EXTRACTION 1757
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:22 UTC from IEEE Xplore.  Restrictions apply. Hanqing Tao is currently working toward the
PhD degree with the Department of Computer
Science and Technology from University of Sci-
ence and Technology of China (USTC). His
research interests include data mining, natural
language processing, Chinese language analysis
and interpretable artiﬁcial intelligence. He has
published several papers in referred conference
proceedings, such as AAAI, ICDM, ICME, CCL
etc.
Sirui Zhao is currently working toward the PhD
degree with the Department of Computer Science
and Technology from University of Science and
Technology of China (USTC). His research interests
include automatic micro-expressions analysis,
human-computer interaction (HCI) and affect com-
puting. He has published several papers in refereed
conferences and journals, including ACM Multime-
dia Conference , Neural Networks, Neurocomput-
ing,e t c .
Hao Wang received the PhD degree in computer
science from USTC. He is currently an associate
researcher with the School of Computer Science
and Technology, USTC. His main research inter-
ests include data mining, representation learning,
network embedding and recommender systems.
He has published several papers in referred con-
ference proceedings, such as TKDE, TOIS, Neu-
riPS, and AAAI.
Qi Liu (Member, IEEE) received the PhD degree
in computer science from USTC. He is a profes-
sor with USTC. His general area of research is
datamining and knowledge discovery . He has
published proliﬁcally in refereed journals and con-
ference proceedings, e.g., IEEE Transactions on
Knowledge and Data Engineering ,ACM Transac-
tions on Information Systems ,ACM Transactions
on Knowledge Discovery from Data ,ACM Trans-
actions on Intelligent Systems and Technology ,
KDD, IJCAI, AAAI, ICDM, SDM, and CIKM. He is
a member of the ACM. He received the ICDM 2011 Best Research
Paper Award and the Best of SDM 2015 Award.
Enhong Chen (Senior Member, IEEE) received the
PhD degree from USTC. He is a professor and vice
dean with the School of Computer Science, USTC.
His general area of research includes data mining
and machine learning, social network analysis, and
recommender systems. He has published more
than 100 papers in refereed conferences and jour-
nals, including IEEE Transactions on Knowledge
and Data Engineering ,IEEE Transactions on Mobile
Computing , KDD, ICDM, NeurIPS, and CIKM. He
was on program committees of numerous conferen-
ces including KDD, ICDM, and SDM. His research is supported by the
National Science Foundation for Distinguished Young Scholars of China.
"For more information on this or any other computing topic,
please visit our Digital Library at www.computer .org/csdl.1758 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:22 UTC from IEEE Xplore.  Restrictions apply. Aspect-Opinion Correlation Aware and
Knowledge-Expansion Few Shot Cross-Domain
Sentiment Classiﬁcation
Haopeng Ren, Yi Cai ,Member, IEEE , Yushi Zeng, Jinghui Ye,
Ho-fung Leung ,Senior Member, IEEE , and Qing Li ,Senior Member, IEEE
Abstract— Cross-domain sentiment analysis has recently attracted signiﬁcant attention, which can effectively alleviate the problem
of lacking large-scale labeled data for deep neural network based methods. However, most of the existing cross-domain sentiment
classiﬁcation models neglect the domain-speciﬁc features, which limits their performance especially when the domain discrepancy
becomes larger . Meanwhile, the relations between the aspect and opinion terms cannot be effectively modeled and thus the sentiment
transfer error problem is suffered in the existing unsupervised domain-adaptation methods. To address these two issues, we propose
an aspect-opinion correlation aware and knowledge-expansion few shot cross-domain sentiment classiﬁcation model. Sentiment
classiﬁcation can be effectively conducted with only a few support instances of the target domain. Extensive experiments are
conducted and the experimental results show the effectiveness of our proposed model.
Index Terms— Cross domain sentiment analysis, knowledge graph, few-shot learning
Ç
1I NTRODUCTION
SENTIMENT analysis (SA) is a fundamental task in natural lan-
guage processing (NLP), aiming to automatically assign the
sentiment polarities (i.e., posit ive, neutral, or negative) to the
user-generated text data like the restaurant reviews. For exam-
ple, the sentence review ” The food in this restaurant is delicious ”
expresses the positive sentiment. Currently, the deep neural
network based SA models [1], [2] are widely-used and achieve
remarkable performance, nevert heless suffer from the problem
of lacking large-scale labeled data. It is usually time-consumingand human-intensive to make annotations in many applica-
tions. To alleviate this problem, the task of cross-domain senti-
ment classiﬁcation [3], [4], [5] recently attracts considerable
attention, which transfers the knowledge learned from the
label-rich source domain to the label-scarce target domain.
The main challenge in cross-domain sentiment classiﬁca-
tion is the discrepancy between the source and target
domain (e.g., the different expressions of users’ emotions
across domains). Facing this challenge, one group of recent
domain-adaptation methods (e.g., the adversarial learning
[7], [8], [9], [10]) focus on learning the domain-invariant
(domain-shared) features (e.g., the opinion terms “ terrible ”,
“great ” and “ fast” which are shared in both the source and
target domains, as shown in Fig. 1). They are often based on
a key assumption that the domain-invariant features also
share the same sentiment polarities in both the source and
target domains. Nevertheless, it is often violated in many
realistic scenarios and causes the sentiment transfer error
problem. For example shown in Fig. 1, the opinion term
“fast” expresses the negative sentiment when describing the
aspect “ battery ” in the Electronic domain, while expresses
the positive sentiment for the aspect term “ pan” in the
Kitchen domain. The sentiment of the domain-invariant fea-
ture “ fast” from the source domain (i.e., Electronic domain)
is wrongly transferred as negative polarity into the target
domain (i.e., the Kitchen domain). Therefore, the sentiment
polarities of the domain-invariant features not only rely on
the domains they are in but also depend on the aspects they
describe. Inspired by the success of applying syntactic infor-
mation in the aspect-opinion pairs extraction task [11], [12],
we introduce the syntactic knowledge structure to capture
the relational features between the aspect and opinion terms
for the cross-domain learning, aiming to solve the sentiment
transfer error problem. Speciﬁcally, the syntactic knowledge/C15Haopeng Ren, Yushi Zeng, and Jinghui Ye are with the School of Software
Engineering, South China University of Technology, Guangzhou 510650,
China, and also with the Key Laboratory of Big Data and Intelligent Robot,
SCUT, Guangzhou 510335, China. E-mail: se_renhp@mail.scut.edu.cn,
yushi_znn@foxmail.com, jhuiye@qq.com.
/C15Yi Cai is with the School of Software Engineering, South China University
of Technology, Guangzhou 510650, China, and also with the Key Labora-
tory of Big Data and Intelligent Robot, SCUT and the Pazhou Lab,
Guangzhou 510335, China. E-mail: ycai@scut.edu.cn.
/C15Ho-fung Leung is with the Department of Computer Science and Engineer-
ing, The Chinese University of Hong Kong, Hong Kong.
E-mail: lhf@cuhk.edu.hk.
/C15Qing Li is with the Department of Computing, The Hong Kong Polytech-
nic University, Hong Kong. E-mail: csqli@comp.polyu.edu.hk.
Manuscript received 10 January 2022; revised 20 July 2022; accepted 4 Sep-
tember 2022. Date of publication 9 September 2022; date of current version 15
November 2022.
This work was supported in part by the National Natural Science Foundation
of China under Grant 62076100, in part by the Fundamental Research Funds
for the Central Universities, SCUT under Grant x2rjD2220050, in part by the
Science and Technology Planning Project of Guangdong Province under
Grant 2020B0101100002, in part by the Hong Kong Research Council under
Grants PolyU 11204919 and C1031-18G, and in part by the Internal Research
from the Hong Kong Polytechnic University under Grant 1.9B0V.
(Corresponding author: Yi Cai.)
Recommended for acceptance by E. Cambria.
Digital Object Identiﬁer no. 10.1109/TAFFC.2022.3205358IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022 1691
1949-3045 ©2022 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See ht _tps://www.ieee.org/publications/rights/index.html for more information.
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:22 UTC from IEEE Xplore.  Restrictions apply. structure can provide key clues for supporting the underly-
ing reasoning. As shown in Fig. 2, the ones with syntactic
structures “ obj - advmod ” can bring the evident characteristic
clues for facilitating the inference of the aspect-opinion
pairs.
In addition, another group of methods [13], [14] not only
focuses on capturing the domain-invariant features but also
the domain-speciﬁc features which are the strong indicators
for the sentiment analysis in the target domain (e.g., the opin-
ion terms ‘ delicious ’ and ‘ tasty’ in the target (Kitchen) domain,
as shown in Fig. 1). To learn the domain-speciﬁc features of
target domain, two kinds of solutions, i.e., ﬁne-tuning [13] and
semi-supervised learning [14] are designed by giving a small
amount (e.g., 50) of target-domain training labeled data.
However, these methods are still based on the deep neural
networks with large-scale parameters and suffer from the
lack of large-scale labeled data for the target domain, which
are prone to overﬁtting [15]. In contrast, it is intuitive that
humans can learn new knowledge after being taught just a
few labeled instances [16]. Based on this intuition, the few-
shot learning technique has shown effectiveness in various
tasks (e.g., relation classiﬁcation [17], opinion summarization
[18] and image classiﬁcation [16]). It encourages the model to
learn the fast-learning ability from previous experience and
quickly generalize to the new scenarios with a few support
instances. The transferable knowledge can be extracted and
propagated from a collection of meta-tasks, which enables
the model to prevent the overﬁtting problem [19]. Motivated
by this, our work in this paper explores the task of few-shot
cross-domain sentiment classiﬁcation, in which the cross-
domain SA system can not only extract the domain-invariant
features but also obtain the domain-speciﬁc features by giv-
ing only a few (e.g., 1 or 5) support instances meanwhile
without encountering the overﬁtting problem.
Though many studies on few-shot learning obtain promis-
ing results, they still suffer from one major challenge when
directly adapted to the cross-domain sentiment classiﬁcation
task i.e. the scarce domain-speciﬁc features contained in the
few support instances from the target domain. According to
our observation, the relational knowledge graph (e.g.,
ConceptNet [9], [20]) has the rich domain commonsense
knowledge which beneﬁts the domain-speciﬁc semantic
understanding and becomes a potential solution to solve
the problem of scarce domain-speciﬁc features. Speciﬁcally,
as shown in Fig. 3, the few-shot SA model conducts thecross-domain sentiment classiﬁcation based on only a few
(i.e., 2) support instances which are respectively provided for
the positive and negative classes in the target domain. First,
the relationships between the aspect and opinion terms are
built based on the dependency relations. For instance, the
aspect-opinion pair “ soup$delicious ”a r ec o n n e c t e dw i t ht h e
dependency relation “ nsubj ”, as shown in Fig. 3. Then,
through the relational knowledge graph, the rich domain-spe-
ciﬁc background knowledge can be linked based on the given
few support aspect-opinion pairs, which beneﬁts the semantic
understanding of aspect and opinion terms in the target
domain. As we can observe, the terms with similar semantics
usually share the relational knowledge structures. The senti-
ment features can be transferred based on the shared rela-
tional knowledge structures. For example shown in Fig. 3, the
aspect terms “ soup”a n d“ pizza ” share most of the neighbor-
hood nodes (e.g., the terms “ meat”, “restaurant ” and so on) in
the relational knowledge graph. Based on the bridge of the
shared relational knowledge structure, the sentiment features
can be transferred from the few support instances to the query
instances (e.g., both the aspect-opinion pair “ soup$delicious ”
in sample ð1Þand “ pizza$delicious ”i nq u e r y ð1Þshare the posi-
tive sentiment polarity). In this way, with the help of the exter-
nal relational knowledge graph, the domain-speciﬁc
sentiment features can be enriched based on only a few sup-
port instances.
In this paper, we propose an aspect-opinion correlation
aware and knowledge-expansion few-shot cross-domain
sentiment classiﬁcation model (AKFSM). As shown in
Fig. 4, the framework of our proposed model consists of
two phases. For the ﬁrst phase named Aspect-Opinion Corre-
lation Aware Graph Feature Learning , two self-supervised
tasks (i.e., the relation classiﬁcation task and the sentiment
alignment task) are designed to pre-train the graph convo-
lution network (GCN) encoder, aiming to capture the rela-
tional knowledge features. Then, the second phase, named
feature-fusion based few-shot learning , conducts the sentiment
classiﬁcation with a few (e.g., 1 or 5) support instances by
infusing the relational knowledge features and the semantic
features from the domain-adapted BERT.
Our contributions are summarized as follows:
/C15We explore a problem of cross-domain sentiment
classiﬁcation in the few-shot scenario. For this prob-
lem, we propose an aspect-opinion correlation aware
and knowledge-expansion few-shot cross-domain
sentiment classiﬁcation model. To the best of our
knowledge, our work is the ﬁrst study focusing on
few-shot cross-domain sentiment classiﬁcation .Fig. 1. An example of the sentiment transfer error in current cross-
domain sentiment classiﬁcation methods which focus on the domain-
invariant features learning.
Fig. 2. Illustration of the aspect-opinion relationships in two sentences
with (1) syntactic dependency structures and (2) the corresponding part-
of-speech tags.1692 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:22 UTC from IEEE Xplore.  Restrictions apply. /C15We design an aspect-opinion correction aware graph
feature learning method with two self-supervised
pre-trained tasks to solve the sentiment transfer error
problem suffered in existing unsupervised domain-
adaptation methods.
/C15We propose a knowledge-expansion few-shot cross-
domain sentiment classiﬁcation model. It can effec-
tively expand the domain-speciﬁc knowledge with
only a few support instances meanwhile do not suf-
fer from the overﬁtting problem.
/C15Extensive experiments and visualization analysis
are conducted to evaluate the effectiveness of our
proposed model in the few-shot cross-domain senti-
ment classiﬁcation scenario.
2R ELATED WORK
Sentiment classiﬁcation aims to classify the sentiment polari-
ties (e.g., positive, negative and neutral sentiment) of the
given text, which is a basic task in NLP. Deep neural network
based methods [1], [21], [22], [23], [24], [25], [26], [27], [28]
highly rely on the large-scale labeled training data in a speciﬁc
domain, but the data labeling process is often labor-intensive
and time-consuming. To solve this problem, the cross-domain
sentiment classiﬁcation task is proposed and attracts much
researchers’ attention. It aims to transfer the knowledge
from the label-rich source domain to the label-scarce target
domain. Existing transfer-based methods for the cross-
domain sentiment classiﬁcation task can be summarized into
three categories: pivot extraction based methods, non-pivot
extraction based methods and deep transfer learning based
methods.
First, the pivot extraction based method aims to capture
the pivot terms and treat them as transferable features across
domains. The pivot-selection strategies can be classiﬁed into
two groups: statistics-based and label-based [29]. Speciﬁcally,
the statistics-based methods extract the domain-shared and
sentiment-indicative features based on the statistics informa-
tion (e.g., the term frequencies [30] and pointwise mutual
information (PMI) [31]) between the source and target
domains. These methods are mostly the heuristic methodswhich lack the semantic understanding of the text and
require the manual selection. Then, the label-based methods
aim to select the pivots from the sentiment features by a
supervised classiﬁer that is trained with the labeled data of
the source domain. For instance, the instance weighting
method [32] is proposed to obtain the pivot feature represen-
tation by bridging the distribution of the source and target
domains. The deep network based methods, such as the mar-
ginalized stacked denoising autoencoder (MSDA) [33], the
HATN [34], PBLM [35] and TPT [36] are designed to extract
the domain-shared sentiment features.
Second, the non-pivot extraction based method aims to
capture the emotion terms which are usually the indicators
for the sentiment classiﬁcation in the target domain. Speciﬁ-
cally, the HATN [34] is proposed to introduce the non-piv-
ots by treating the pivots as the bridge. Moreover, the prior
knowledge (e.g., sentiment dictionary) is added into neural
networks [37], [38], [39] to capture the non-pivots. Li et al.
2020 [36] propose a Transferable Pivot Transformer (TPT)
which detects both the pivot words and non-pivot words by
modeling the relationships between the pivot and non-pivot
words. Nevertheless, the accuracy of detecting the non-
pivot words relies on the performance of pivot word extrac-
tion. Thus, these methods suffer from the error propagation
problem. Moreover, as the discrepancy across domains
increases, the pivot words become scarce and then the non-
pivot words are difﬁcult to be extracted.
The third category method focusing on the cross-domain
sentiment classiﬁcation task is based on the deep transfer
learning technique. Recently, the adversarial training based
methods [7], [40], [41], [42], [43] aim to automatically obtain
the domain-invariant (domain-shared) features by applying
the attention mechanism and adversarial training strategy.
With the success of the pre-training language model (e.g.,
BERT [44]), the domain-aware BERT and adversarial training
strategy are combined to automatically learn the domain-
invariant features across domains. Moreover, the domain-
adversarial framework KinGDOM [9] is proposed to learn
the domain-invariant features by introducing the relational
knowledge graph (i.e., ConceptNet [20]). Based on the suc-
cess of graph convolutional network techniques [26], [27],
[28], [45], [46], [47], [48], the graph-structure domain
Fig. 3. A 2-shot setting example for few-shot cross-domain sentiment classiﬁcation with the external commonsense knowledge graph. The “ nsubj ”
denotes the dependency relation with the Standford CoreNLP libraries [6].REN ET AL.: ASPECT-OPINION CORRELATION AWARE AND KNOWLEDGE-EXPANSION FEW SHOT CROSS-DOMAIN SENTIMENT ... 1693
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:22 UTC from IEEE Xplore.  Restrictions apply. knowledge features can be captured, which beneﬁt the cross-
domain learning. The semantics of the review documents can
be enriched[20] by providing both the domain-invariant and
domain-speciﬁc background concepts. Then, the domain-
invariant features are captured by utilizing the adversarial
training strategy. However, KinGDOM mainly learns the
domain-invariant features and ignores the domain-speciﬁc
features. In addition, the relationships between the aspect
and opinion concepts are not built in the utilized external
knowledge graph, which also causes the sentiment transfer
error problem in the cross-domain sentiment classiﬁcation,
as shown in Fig. 1. In this paper, our work also utilizes the
external knowledge graph and focuses on solving the above
two problems (i.e., the domain-speciﬁc features ignoring
problem and the sentiment transfer error problem).
In summary, though the existing methods based on deep
transfer learning recently achieve better performance, they
mainly focus on extracting the domain-invariant features
for the target domain while neglecting the domain-speciﬁc
features. Current two solutions for introducing the domain-
speciﬁc features (i.e., the ﬁne-tuning [13] and the semi-
supervised method [14]) are prone to suffer from the overﬁt-
ting problem when training with only a few labeled data of
the target domain. Currently, the few-shot learning tech-
nique achieves success in many NLP tasks [15], [17], [18],
[49], [50], [51], [52] and is a effective solution to avoid the
overﬁtting problem. Inspired by the success of the adversar-
ial training strategy [53], several cross-domain few-shot
learning baselines [17] are designed. Moreover, two related
works [54], [55] focusing on the cross-domain few-shot text
classiﬁcation task are proposed. They are also adapted to
the cross-domain few-shot sentiment classiﬁcation task and
conduct the comparative experiments with our proposed
model.
In our paper, we propose a few-shot learning based
cross-domain sentiment classiﬁcation model to effectively
address the problem of ignoring domain-speciﬁc features.
Utilizing the external knowledge graph, the rich domain-
speciﬁc features can be expanded with only a few support
instances. Furthermore, an aspect-opinion correlation aware
graph learning method is designed to solve the sentiment
transfer error problem suffered in the existing methods
based on the deep transfer learning.
3M ODEL
To solve the sentiment transfer error problem and the
domain-speciﬁc features ignoring problem suffered in exist-
ing methods, we propose a knowledge-expansion few-shot
learning model for the cross-domain sentiment classiﬁcation
task. As shown in Fig. 4, two phases are contained in our
proposed model and they are sequential relationships. First,
the graph feature encoder (i.e., GCN Autoencoder) is
designed and pretrained with the unlabeled data from both
the source and target domains in Phase 1, aiming to capture
the graph structure features of the expanded commonsense
knowledge graph. In Phase 2, both the graph structure fea-
tures (extracted by the GCN encoder in Phase 1) and the
text semantic features (obtained by the domain-adapted
encoder in Phase 2) are fused to conduct the sentiment clas-
siﬁcation with a few (1 or 5) support instances.In the ﬁrst phase, named aspect-opinion correlation aware
graph feature learning , an aspect-opinion correlation aware
knowledge graph is constructed based on the ConceptNet
[20]. Then, two self-supervised tasks (i.e., relation classiﬁca-
tion task and sentiment alignment task) are conducted with
the knowledge graph to pre-train the GCN auto-encoder,
aiming to learn the relational knowledge structure of the
aspect-opinion pairs. Then, the second phase, named
knowledge-expansion based few-shot learning ,a i m st oe x p a n d
the domain-speciﬁc features by the commonsense knowl-
edge graph based on the GCN encoder (which is obtained
in Phase 1) given only a few support instances. Moreover,
not all relational knowledg e is beneﬁcial for sentiment
classiﬁcation of the query instances. Little noise in the sup-
port set may cause a huge deviation of the feature repre-
s e n t a t i o ni nt h ef e w - s h o tl e a r ning scenario [51]. Motivated
by this, the shared-knowledge aware attention is designed
to select the transferable knowledge triplets in cross-
domain learning. Finally, both the graph features from the
GCN autoencoder and the text semantic features from the
domain-adapted BERT are fused for the support and
query instances. Then, sent iment classiﬁcation is con-
ducted based on the prototyp ical network with only a few
support instances.
3.1 Problem Deﬁnitions
The task of few-shot cross-domain sentiment classiﬁcation
can be deﬁned as follows. Given a support set Swith two
sentiment polarity categories C2fPositive; Negative g,a
model classiﬁes the query instance qinto the most possible
sentiment polarity ci2C.Sis deﬁned as follows:
S¼fðx1
1;c1Þ;ðx2
1;c1Þ;...;ðxn1
1;c1Þg;
fðx1
2;c2Þ;ðx2
2;c2Þ;...;ðxn2
2;c2Þg/C26/C27
(1)
where c1;c22C; and ðxj
i;ciÞdenotes the support instance xj
i
belongs to the cisentiment polarity; xj
iis denoted as a word
sequence fw1;w2;...;wLgandLis the length of the word
sequence; niis the number of support instances belonging
to the sentiment polarity ciand is generally quite small in
few-shot scenario. The few-shot cross-domain sentiment
classiﬁcation model is trained in the source domain and is
tested in the target domain. The transferable information
can be extracted and propagated from the source domain to
the target domain by conducting a collection of meta-tasks
[19]. Speciﬁcally, both the support set Sand the query set Q
are constructed in each meta task. The sentiment classiﬁca-
tion of the query instance q2Qis conducted based on the
support set.
3.2 Phase 1: Aspect-Opinion Correlation Aware
Graph Feature Learning
As shown in Fig. 1, the sentiment polarities of the domain-
invariant features depend on both the domains they are in
and the aspects they describe. Based on this observation, we
design an aspect-opinion correlation aware graph learning
method to capture the relational features between the aspect
and opinion terms, aiming to solve the sentiment transfer
error problem. Then, an aspect-opinion correlation aware
sentiment knowledge graph is constructed with the large-
scale unlabeled data in both source and target domain1694 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:22 UTC from IEEE Xplore.  Restrictions apply. utilizing the ConceptNet . The relationship between the
aspect and opinion terms is built based on the syntactic
information. Then, two self-supervised tasks are designed
to pre-train the GCN-encoder, which aims to learn the rela-
tional knowledge features for the aspect and opinion terms.
3.2.1 Knowledge Graph Construction
The relational knowledge graph is constructed based on the
large-scale of unlabeled documents respectively for the
source and target domain. Two steps are contained to con-
struct the knowledge graph. The ﬁrst step is to extract the
aspect-opinion relational triplets based on the syntactic
knowledge structures of each unlabeled document. Speciﬁ-
cally, for each unlabeled document, we use the Standard
CoreNLP libraries [6] to recognize the part-of-speech of
each token and the dependency relations among the tokens.
The tokens which belong to nouns ,adjectives oradverbs are
treated as the seed nodes and are connected together by the
dependency relations (i.e., the “ nsubj ”, “advmod ”, “amod ”,
“obj” and “ conj” dependency relations). Thus, the relation-
ships between the aspect and opinion terms can be
extracted based on the syntactic knowledge structure. For
example shown in Fig. 3, the aspect-opinion relational trip-
lets (e.g., “ soup /C0!nsubjdelicious ” in Sample ð1Þ) are obtained,
where the “ soup” and “ delicious ” can be respectively seen as
the aspect and opinion terms.
Second, the ConceptNet knowledge graph [20] is utilized to
link the commonsense knowledge triplets based on the
aspect-opinion relational triplets. Motivated by the success
in the node embedding techniques [56], the nodes with simi-
lar neighborhoods will have the close feature embeddings.
For example shown in Fig. 3, the aspect terms “ soup” and
“pizza ” share most of the neighborhoods, which indicatesthey have similar semantic features. Utilizing the bridge of
the shared relational knowledge structure, the sentiment fea-
tures can be transferred from the few support instances to
the query instances in Phase 2. Speciﬁcally, based on the
seed triple “ soup /C0!nsubjdelicious ”, one hop of the commonsense
knowledge triplets (e.g., “ soup”/C0!relatedTo“meat”, “soup”/C0!isA
“food” and so on) can be extracted from ConceptNet . In this
way, with the unlabeled documents for the source and target
domain, the knowledge graph G¼ðV;C;RÞwith nodes vi2
Vand triplets ðvi;ri;j;vjÞ2Ccan be constructed, where ri;j2
Rdenotes the relation between the node viandvj.
3.2.2 Knowledge Graph Pre-Training
As described in Section 3.2.1, the relational knowledge
graph is respectively constructed from the source and target
domains. Currently, the Relational Graph Convolutional
Network (GCN) encoder [24], [25], [57], [58] is proven to
have the ability of accumulating relational evidence in mul-
tiple inference steps from the local neighborhood around a
given node. Following Ghosal et al. 2020 [9], for each node
(i.e., term) viinG, we utilize a two-layer GCN encoder
(which are stacked one another) to learn its feature repre-
sentation gggggggi, as follows,
gggggggiiiiiii¼hhhhhhhð2Þ
i¼fðhhhhhhhð1Þ
i;2Þ;hhhhhhhð1Þ
i¼fðvvvvvvvi;1Þ (2)
fðxxxxxxxi;lÞ¼sX
r2RX
j2Nr
i1
jNr
ijWðlÞ
rxxxxxxxjþWðlÞ
0xxxxxxxi0
@1
A (3)
where Nr
idenotes the neighbouring nodes of node viunder
the relation r2R;sdenotes the activation function such as
ReLU; vvvvvvviis the randomly initialized representation vector;
Wð1=2Þ
randWð1=2Þ
0denotes the learnable parameters.
Fig. 4. The framework of our proposed knowledge-expansion few-shot cross-domain sentiment classiﬁcation model.REN ET AL.: ASPECT-OPINION CORRELATION AWARE AND KNOWLEDGE-EXPANSION FEW SHOT CROSS-DOMAIN SENTIMENT ... 1695
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:22 UTC from IEEE Xplore.  Restrictions apply. To learn the relational knowledge features of the aspect
or opinion terms, two self-supervised learning tasks (i.e.,
relation classiﬁcation and sentiment alignment) are con-
ducted to pre-train the GCN encoder model [57] in our
constructed knowledge graph. Both of these two self-super-
vised learning tasks respectively adopt different strategies
to constrain the GCN encoder to learn the graph feature
representation, which beneﬁts the model to transfer and
expand the domain-speciﬁc sentiment features with a
few support instances in Phase 2. Speciﬁcally, following
Schlichtkrull et al. 2018 [57] and Ghosal et al. 2021 [9], the
self-supervised relation classiﬁcation task is adopted to
force or push the GCN encoder to learn the graph structure
features by accumulating the relational evidence. The nodes
with similar neighborhoods (i.e., the relational knowledge
structure) will have the close feature embeddings [56],
which indicates they have similar semantics. Utilizing the
bridge of the shared relational knowledge structure, the sen-
timent features can be transferred from the few support
instances to the query instances in Phase 2 (e.g., both the
aspect terms “ soup” and “ pizza ” described by the same opin-
ion term “ delicious ” can be referred to have the same senti-
ment polarity (i.e., Positive) as shown in Fig. 3) .
Moreover, the sentiment alignment task is designed to
help the GCN encoder to capture the sentiment alignment
features among aspect-opinion pairs by exploiting their co-
occurrence features within documents. For example, the
aspect-opinion pairs “ terrible $product ” and “ battery $fast”
are often co-occurring in the same document and share the
same sentiment polarity. The sentiments of the aspect-opin-
ion pairs can be derived through the sentiments of their con-
textual aspect-opinion pairs, which facilitates the transfer
and expansion of sentiment features.
Speciﬁcally, for the self-supervised relation classiﬁcation
task, the model takes as input the triplets C0(named posi-
tive triplets) from CinGand the equal number of the nega-
tive triplets. Note that the negative triplets are created by
randomly modifying either one of the nodes or the relations
in the positive triplets. Both the positive and negative sam-
ples are merged into a set Tand their labels are respectively
denoted as 0 and 1 (i.e., y2f0;1g). Therefore, given the trip-
letsðvi;ri;j;vjÞ2T, a binary classiﬁcation task is conducted
to train the GCN encoder with the cross-entropy loss:
£G¼/C01
jTjX
ðvi;ri;j;vj;yÞ2Tðylogsðvi;ri;j;vjÞþ
ð1/C0yÞlogð1/C0sðvi;ri;j;vjÞÞÞ (4)
sðvi;ri;j;vjÞ¼sðgggggggT
iRrgggggggjÞ (5)
where sðvi;ri;j;vjÞdenotes the DistMult factorization [59]
score function; Each relation r2Ris associated with a diag-
onal matrix Rr2Rd/C2d.
For the self-supervised sentiment alignment learning
task, the model takes as input the aspect-opinion relational
triplets (named positive triplets) in one review and the
equal number of negative triplets which are created by ran-
domly selecting the aspect-opinion relational triplets from
other reviews. Then both the positive and the negative trip-
lets are merged into a set Pand their labels are respectively
denoted as 0 and 1 (i.e., y20;1). Thus, given the tripletsðvi;ri;j;vjÞ2P, the model conducts a binary classiﬁcation
task to train the GCN encoder with the cross-entropy loss:
£align¼/C0XN
k¼1ð1
jPkjX
ðvi;ri;j;vj;yÞ2Pkðylogsðvi;ri;j;vjÞ
þð1/C0yÞlogð1/C0sðvi;ri;j;vjÞÞÞÞ
where Ndenotes the number of the unlabeled reviews in
source or target domain; Pkdenotes the aspect-opinion rela-
tional triplets set of the kth unlabeled reviews.
Finally, the GCN encoder can be optimized with the
cross-entropy loss (i.e., £ Gand £ align) by simultaneously con-
ducting the relation classiﬁcation task and sentiment align-
ment task for each unlabeled document review, aiming to
guide or force the GCN encoder to capture the graph struc-
ture features.
3.3 Phase 2: Knowledge-Expansion based
Few-Shot Learning
3.3.1 Sentence Encoder
Given a review instance, two kinds of encoders (i.e., Graph
Feature Encoder and Domain-Adapted BERT Encoder ) are
designed to obtain the instance representation. Speciﬁcally,
the graph feature encoder (i.e., GCN Encoder) can be
obtained from Phase 1, which aims to capture the graph
structure features. Moreover, following Zhou et al. 2020
[10], the domain-adapted BERT encoder is adopted to cap-
ture the domain-invariant and sentiment-aware text seman-
tic features. Finally, the feature representation of the given
instance can be obtained by fusing the graph structure fea-
tures (which are encoded by the GCN encoder) and text
semantic features (which are encoded by the domain-
adapted BERT encoder) with a reconstruction loss.
Graph Feature Encoder. The three modules (i.e., Aspect-
Opinion Relational Triplets Extraction ,Relational Knowledge
Expansion and GCN Autoencoder ) are shared in both phases,
which aims to obtain the graph feature representation of the
given instances. Similar to the Phase 1, given an instance x,
the aspect-opinion relational triplets can be obtained based
on the syntactic knowledge structures by the Standard Cor-
eNLP libraries [6]. With the aspect-opinion relational trip-
lets, an expanded commonsense knowledge sub-graph Gx
for the instance xcan be obtained by linking the external
knowledge graph ConceptNet [20]. Then, each node in graph
Gxcan be encoded as a d-dimensional vector vvvvvvvnodeby the
pre-trained GCN encoder. The instance xcan be repre-
sented as xxxxxxxgby averaging all the nodes’ representations, as
follows:
xxxxxxxg¼1
MXM
i¼1vi
Gx; (7)
where Mdenotes the number of nodes in the graph Gxand
vi
Gx2Rdgdenotes the representation of the ith node in the
graph Gxfor the instance x.
Domain-Adapted BERT Encoder. The pre-trained language
model BERT has shown to be effective in many NLP tasks,
but is task-agnostic and little understanding of opinion text
[8], [10]. To adapt the BERT into the speciﬁc domains
(including both source and target domain), we conduct1696 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:22 UTC from IEEE Xplore.  Restrictions apply. several pre-training tasks at both token level and sentence
level to obtain the domain-invariant sentiment knowledge
by masking and prediction. Following Zhou et al. 2020 [10],
three token-level (i.e., sentiment-aware word prediction ,word
sentiment prediction and emotion pretiction ) and one sentence-
level prediction tasks (i.e., emoticon prediction ) are utilized to
ﬁne-tune the BERT encoder. Therefore, based on the pre-
trained BERT encoder1, we can obtain the semantic feature
representation xxxxxxxw2Rdwfor the instance x, where dwdenotes
the dimension of the vector.
Feature Fusion. Each instance xcan be encoded as the rela-
tional knowledge feature representation xxxxxxxgand the semantic
feature representation xxxxxxxwrespectively by the graph feature
encoder and domain-adapted BERT encoder. According to
our observation, the feature representations respectively
encoded by the graph feature encoder and the domain-
adapted BERT encoder are in different embedding spaces.
The feature fusion using a simple concatenation or averaging
operation will lead to bias in the distance metric of the proto-
typical network [9], [16]. To reduce the feature space discrep-
ancy from the GCN encoder and the domain-adapted
encoder, a feature mapping layer with a reconstruction loss
(mean-squared error) is adopted as follows:
xxxxxxxm
g¼Wgxxxxxxxgþbg (8)
xxxxxxxrecon¼Wreconxxxxxxxm
gþbrecon; (9)
where Wg2Rd/C2d,Wrecon2Rd/C2d,bgandbrecon are the train-
able parameters. The reconstruction loss is obtained by
using the cosine similarity function, aiming to keep the fea-
tures before and after the mapping operation unchanged.
£recon¼xxxxxxxg/C1xxxxxxxrecon
xxxxxxxg/C13/C13/C13/C13
2/C1xxxxxxxreconkk2(10)
Therefore, the feature representation xxxxxxxof the given
instance xcan be obtained as follows:
xxxxxxx¼½xxxxxxxm
g;xxxxxxxw/C138 (11)
3.3.2 Shared-Knowledge Aware Attention
As shown in Fig. 3, not all the external knowledge nodes are
equally important to the query instance. To capture the
shared relational knowledge structures between the support
and query instances, the shared-knowledge aware attention
is designed. Speciﬁcally, given the support instance xand
query instances q, the corresponding two sub-graphs Gx
andGqcan be obtained by linking the ConceptNet (i.e., the
two steps: aspect-opinion relational triplets extraction and
relational knowledge expansion in Fig. 4). Then, these two
sub-graphs are respectively encoded by the pre-trained
GCN encoder as VVVVVVVGx2RNx/C2dgandVVVVVVVGq2RNq/C2dg, where Nx
andNqare respectively the number of nodes in the sub-
graph GxandGq;dgdenotes the dimensional size of the
graph feature representation. The graph feature representa-
tionxxxxxxxgin Eq. (8) is replaced by Eq. (12) as follows:xxxxxxxg¼XNx
i¼1aiðWattvvvvvvvi
GxþbattÞ; (12)
where Watt2Rdg/C2dgandbatt2Rdgare the learnable parame-
ters; airepresents the important degree of the ith node in
the sub-graph Gxand is calculated as follows:
ai¼expðejÞPNx
k¼1expðekÞ(13)
ei¼sumfsððvvvvvvvi
GxÞT/C2ðWattVVVVVVVT
GqþbattÞÞg; (14)
where sð/C1Þis an activation function tanh and sumf/C1g
denotes the sum of all elements of the vector.
3.3.3 Prototypical Network
The prototypical network [16] is utilized to conduct the few-
shot sentiment classiﬁcation. With the sentence encoder
described in Section 3.3.1, both the support instances in sup-
port set Sand query instance qare respectively encoded
into low-dimensional vectors xxxxxxxj
iandxxxxxxxq. For each sentiment
polarity categories ci2Cwith ksupport instances (i.e., 2-
way k-shot setting), we can obtain the prototype pppppppiof cate-
gory cias follows:
pppppppi¼1
kXk
j¼1xxxxxxxj
i (15)
Finally, the probability of query instance qbelonging to sen-
timent polarity category ci2Ccan be measured as follows:
pfðcijqÞ¼expð/C0dðpppppppi;xxxxxxxqÞÞPC
j¼1expð/C0dðpppppppj;xxxxxxxqÞÞ(16)
where fdenotes all the trainable parameters in sentence
encoder; dð:; :Þis the Euclidean distance function for the two
given vectors.
3.4 Loss Layer
Finally, the loss function of the whole architecture can be
deﬁned as follows:
£¼£reconþ£softmax (17)
where £ softmax denotes the cross-entropy loss of prototypical
network in phase 2.
4E XPERIMENT
4.1 Dataset and Experiment Setting
We conduct experiments on the Amazon-reviews bench-
mark dataset for cross-domain sentiment classiﬁcation [60]
with a few support instances of the target domain. The data-
set ranges across four domains: Books (B), DVDs (D), Elec-
tronics (E), and Kitchen appliances (K). The reviews in all
domains are associated with a rating denoting their senti-
ment polarity. Following Du et al. 2020[8], reviews with rat-
ing up to 3 stars are considered as negative sentiment and 4
or 5 stars as positive sentiment. Each domain has 2000
labeled reviews and approximately 4000 unlabeled reviews.1.The pre-trained BERT checkpoint ﬁles can be downloaded in
https://github.com/12190143/SentiX .REN ET AL.: ASPECT-OPINION CORRELATION AWARE AND KNOWLEDGE-EXPANSION FEW SHOT CROSS-DOMAIN SENTIMENT ... 1697
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:22 UTC from IEEE Xplore.  Restrictions apply. In the training stage, our model is trained by conducting
the meta-task in each episode to optimize the models’ param-
eters [19]. For the N-way K-shot scenario, the support set S
and query set Qare constructed for each meta-task. Note that
Nequals 2 (i.e., the positive and negative sentiment catego-
ries) in the cross-domain sentiment classiﬁcation scenario.
Speciﬁcally, we randomly select Kinstances for each senti-
ment category ci2Cto construct the support set S.M e a n -
while, the query set Qis constructed by randomly selecting
jQj¼5instances respectively from the positive and negative
sentiment category, where S\Q¼;. Our proposed models
are optimized by conducting iter¼20000 meta-tasks. Similar
to the task of cross-domain sentiment classiﬁcation, the model
is trained on the source domain and tested on the target
domain. All the hyperparameters are shown in Table 1.
As shown in Tables 2, 3, and 4, we compare the perfor-
mance of our proposed model with that of two categories of
related works: 1) the few-shot learning baselines (i.e., GNN
[49], MetaNet [50], SNAIL [61], Proto-CNN [16], Proto-CNN
with adversarial training ( Proto-CNNy) [17], Proto_HATT
[51], Proto-BERT [17], Proto-BERT with adversarial training
(Proto-BERTy) [17], BERT-PAIR [17], MLADA [54], PtNet
[55]) and 2) current cross-domain sentiment classiﬁcation
models (i.e., DANN [40], PBLM [35], HATN [34], ACAN [43],
IATN [42], HATN-BERT [34], CoCMD [14], KinGDOM [9],
BERT-DAAT [8]) and SENTIX [10]).
4.2 Result Analysis
In our experiments, we compare the performance of our pro-
posed model with 11 few-shot learning based baselines (i.e.,
GNN ,MetaNet ,SNAIL ,Proto-CNN ,Proto_HATT [51], Proto-
BERT ,Proto-CNN with adversarial training ( Proto-CNNy),
Proto-BERT with adversarial training ( Proto-BERTy),BERT-
PAIR [17]), MLADA [54]), PtNet [55] and 10 current cross-
domain sentiment classiﬁcation models (i.e., DANN [40],
PBLM [35], HATN [34], ACAN [43], IATN [42], HATN-BERT
[34], CoCMD [14], KinGDOM [9],BERT-DAAT [8]) and SEN-
TIX[10] and 3 supervised learning based baselines (i.e., CNN ,
LSTM [1] and BERT [62]) which are trained with 1000 target
domain labeled data, validated with 200 labeled data and
tested with 800 labeled data. Furthermore, the ablation experi-
ments and visualization analysis are conducted to evaluate
the effectiveness of different modules in our proposed model.
We analyze the experimental results from four perspectives
as follows.
4.2.1 Comparison With Few-Shot Learning Baselines
Currently, the few-shot learning technique obtains success
on several NLP tasks (e.g., text classiﬁcation and relationclassiﬁcation). To evaluate the effectiveness of our proposed
model, eight popular few-shot learning baselines designed
for relation classiﬁcation or text classiﬁcation are adapted
into the few-shot cross-domain sentiment classiﬁcation sce-
nario. As shown in Tables 2 and 3, two experimental set-
tings (i.e., 1-shot and 5-shot) are considered in our
experiments. As we can observe, our proposed model
achieves higher accuracies with a large margin in all of the
cross-domain experimental settings. Speciﬁcally, a few (e.g.,
1 or 5) support instances are given for the few-shot senti-
ment classiﬁcation task and only cover a small amount of
domain-speciﬁc features. Different from the few-shot learn-
ing baselines, our proposed model can effectively enhance
the support information with the given few support sam-
ples from the target domain, which effectively improves the
performance in cross-domain learning.
As shown in Table 2 and Table 3, the performance of the
three baselines GNN ,MetaNet and SNAIL is worse than our
models with a large margin. According to our observation,
these three baselines even perform worse when the number
of support instances increases in some cross-domain tasks
(e.g., the Kitchen !Book cross-domain task for the model
SNAIL shown in Tables 2 and 3). We analyze that not all the
provided support features (or instances) can beneﬁt identi-
fying the sentiment polarities of the query instances. The
support instances may contain irrelevant features and even
noises when identifying the sentiment polarity of the query
instances [51]. Motivated by this, the shared-knowledge
aware attention module is designed to weight the expanded
commonsense knowledge nodes, aiming to alleviate the
effect of the irrelevant and noisy knowledge. In the experi-
ments shown in Table 2 and 3, our proposed model achieves
better performance as the number of support instances
increases and can make full use of the few provided support
instances.
Moreover, compared with the prototypical network based
baselines (i.e., the model Proto-CNN ,Proto_HATT ,Proto-
BERT ), our proposed model (which is also based on the pro-
totypical network) performs better with a large margin. It can
further evaluate the effectiveness of the external common-
sense knowledge learning module (i.e., Phase 1: Aspect-Opin-
ion Correlation Aware Graph Feature Learning ) designed in our
model. With the help of the constructed external knowledge
graph, rich support information can be effectively expanded
and further improve the performance of few-shot learning
methods. Compared with the BERT-based few-shot learning
baselines (i.e., Proto-BERT ,Proto-BERTy,BERT-PAIR ), our
proposed model with domain-adapted BERT encoder
achieves around 2%-4% higher accuracies. The pre-trained
language model BERT has a powerful ability in language
understanding and is enabled to improve many NLP tasks.
However, the BERT is task-agnostic and has no domain
awareness. Motivated by this, the language model BERT is
ﬁne-tuning in the speciﬁc domain texts, aiming to force the
model to obtain the domain-awareness ability.
To the best of our knowledge, we are the ﬁrst to focus on
the cross-domain few-shot sentiment classiﬁcation task.
Currently, many efforts have been devoted to the unsuper-
vised domain adaptation on sentiment classiﬁcation task
[8], [9], [40], [43]. Among them, adversarial training [53] has
been proved to be efﬁcient in ﬁnding domain-invariantTABLE 1
Hyperparameters Settings
Parameter Name Value
Maximum Instance Length L 200
Hidden Layer Dimension d 768
Batch Size 4
dg 100
Initial Learning Rate 0.02
Query Set Size jQj 5
keep dropout rate 0.01698 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:22 UTC from IEEE Xplore.  Restrictions apply. features and achieved remarkable performance in the cross-
domain setting. Motivated by this, current few-shot learn-
ing baselines (e.g., Proto-CNNyand Proto-BERTy[17])
adopt the adversarial training strategy to solve the cross-
domain few-shot classiﬁcation problem. Compared with the
adapted few-shot learning methods, our proposed modelsachieve higher accuracies with a large margin in all experi-
mental settings, as shown in Tables 2 and 3. The few-shot
learning methods with adversarial training strategy only
focus on the domain-invariant features but neglect the
domain-speciﬁc features which are the strong indicators for
sentiment classiﬁcation. Note that the adversarial trainingTABLE 2
Average Accuracies (%) Comparison in 1-Shot Scenario With Current Few-Shot Learning Based
Adapted Baselines are Conducted in the Amazon-Reviews Benchmark Dataset
Models D !BE !BK !BB !DE !DK !DB !ED !EK !EB !KD !KE !K Avg
GNN 68.3 67.0 63.2 65.7 64.2 67.5 64.5 69.4 70.7 66.5 65.7 76.4 67.4
MetaNet 70.5 65.9 67.8 61.6 69.1 72.8 68.1 67.7 77.3 69.5 70.5 79.7 70.0
SNAIL 69.5 64.0 66.9 70.0 62.3 61.2 61.1 63.4 73.1 64.7 65.1 72.1 66.1
Proto-CNN 58.4 56.8 57.0 60.4 59.7 57.7 56.4 57.7 65.5 57.3 56.6 65.0 59.41
Proto_HATT 59.2 55.7 56.3 62.1 57.4 58.2 57.8 58.5 66.6 57.2 58.1 65.8 57.2
Proto-BERT 79.6 76.8 72.8 80.3 75.6 76.7 71.8 72.9 84.0 75.1 80.2 85.4 77.6
BERT-PAIR 72.9 70.3 61.2 78.5 70.6 72.6 78.2 81.0 81.1 72.0 79.8 73.4 74.3
Proto-CNNy57.3 55.1 56.2 60.2 57.8 57.5 54.7 56.9 63.5 53.7 52.3 61.2 59.0
Proto-BERTy83.5 77.9 77.2 82.0 74.1 77.1 76.9 81.6 85.2 80.4 81.1 86.9 80.3
MLADA 54.6 52.3 51.5 55.3 53.1 52.4 54.2 54.4 55.9 52.5 53.1 56.5 53.8
PNet 64.2 61.7 61.8 65.2 61.4 62.5 60.4 61.0 67.1 62.7 62.6 69.5 63.4
AKFSM 88.8 89.1 89.3 88.9 88.5 87.7 90.7 90.3 91.9 92.6 94.2 94.7 90.6
The results are the average accuracies of 10000 meta tasks.
TABLE 3
Average Accuracies (%) Comparison in 5-Shot Scenario With Current Few-Shot Learning
Based Adapted Baselines are Conducted in the Amazon-Reviews Benchmark Dataset
Models D !BE !BK !BB !DE !DK !DB !ED !EK !EB !KD !KE !K Avg
GNN 70.5 66.5 66.0 71.5 64.5 68.2 65.6 69.5 74.6 67.2 68.6 75.8 69.0
MetaNet 72.0 64.8 68.2 74.1 67.9 71.1 68.2 69.6 78.1 66.6 70.7 79.3 70.9
SNAIL 70.1 64.6 61.7 68.4 65.7 64.5 62.2 63.1 72.0 65.3 63.1 74.1 66.2
Proto-CNN 68.2 62.7 65.2 69.2 65.5 64.9 63.2 65.9 76.5 64.4 64.1 76.6 67.2
Proto_HATT 68.1 63.5 62.8 69.6 64.9 64.3 64.8 64.8 75.2 66.6 64.4 77.3 65.0
Proto-BERT 86.7 83.4 84.4 87.0 82.7 83.2 82.8 85.7 89.6 91.2 87.3 91.6 86.3
BERT-PAIR 81.0 81.0 68.2 86.6 78.9 80.8 84.0 82.9 87.1 75.8 85.2 89.0 81.7
Proto-CNNy64.6 63.2 64.6 69.0 60.7 65.6 62.9 61.9 74.8 62.5 60.3 69.9 67.2
Proto-BERTy84.2 81.4 82.4 85.8 82.4 84.2 84.8 83.2 86.4 82.2 82.6 91.2 84.2
MLADA 63.6 61.4 60.1 64.6 62.6 60.7 61.3 62.6 64.5 63.1 63.5 67.7 63.0
PNet 75.9 70.9 71.7 76.7 70.9 72.5 70.4 71.7 78.8 73.9 72.6 80.2 73.9
AKFSM 92.5 91.4 91.2 92.3 91.4 91.5 93.7 93.9 94.7 96.7 96.2 96.5 93.5
The results are the average accuracies of 10000 meta tasks.
TABLE 4
Average Accuracies (%) Comparison With Current Cross-Domain Sentiment Classiﬁcation
Models on the Amazon-Reviews Benchmark Dataset
S!TD !BE !BK !BB !DE !DK !DB !ED !EK !EB !KD !KE !K Avg
DANN 81.7 78.6 79.3 82.3 79.7 80.5 77.6 79.7 86.7 76.1 77.4 84.0 80.3
PBLM 82.5 71.4 74.2 84.2 75.0 79.8 77.6 79.6 87.1 82.5 83.2 87.8 80.4
HATN 86.3 81.0 83.3 86.1 84.0 84.5 85.7 85.6 87.0 85.2 86.2 87.9 85.2
ACAN 82.4 79.8 80.8 83.5 81.8 82.1 81.2 82.8 86.6 83.1 78.6 83.4 82.2
IATN 87.0 81.8 84.7 86.8 84.1 84.1 86.5 86.9 87.6 85.9 85.8 88.7 85.8
HATN-BERT 89.8 87.1 87.9 89.4 88.8 87.8 87.2 87.0 90.3 89.4 87.6 92.0 88.7
CoCMD 81.8 76.9 77.2 83.1 78.3 79.6 83.0 83.4 87.2 85.3 85.5 87.3 82.4
KinGDOM 82.7 78.4 80.0 85.0 80.3 82.3 83.9 83.9 88.6 86.6 87.1 89.4 84.0
BERT-DAAT 90.9 88.9 88.0 89.7 90.1 88.8 89.6 89.3 91.7 90.8 90.5 93.2 90.1
SENTIX 91.2 90.4 89.6 91.3 91.2 89.9 93.3 93.6 93.6 96.2 96.0 96.2 92.7
AKFSM 92.5 91.4 91.2 92.3 91.4 91.5 93.7 93.9 94.7 96.7 96.2 96.5 93.5
Five support instances of target domain are given in our proposed model.REN ET AL.: ASPECT-OPINION CORRELATION AWARE AND KNOWLEDGE-EXPANSION FEW SHOT CROSS-DOMAIN SENTIMENT ... 1699
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:22 UTC from IEEE Xplore.  Restrictions apply. strategy even degrades the performance in some cross-
domain tasks (e.g., the cross-domain task DVD !BOOK in
the 5-shot setting by comparing the model Proto-BERT and
Proto-BERTy). We analyze that the domain-invariant fea-
tures are scarce in the given few support instances, which
limits the performance of sentiment analysis in the target
domain.
Recently, two related works focusing on the cross-
domain few-shot text classiﬁcation (i.e., MLADA [54] and
PtNet [55] in Tables 2 and 3) are proposed. They are also
adapted into the cross-domain sentiment classiﬁcation task.
As shown in Tables 2 and 3, comparing with the perfor-
mance of MLADA and PtNet, our proposed model achieves
higher accuracies with a large margin. We observe that they
mainly focus on capturing the domain-invariant features
but ignore the domain-speciﬁc features. Moreover, with a
few (e.g., 1 or 5) support instances of the target domain, few
domain-speciﬁc features are contained, which limits the
performance of the sentiment classiﬁcation in the target
domain. Instead, with the commonsense knowledge graph,
rich domain-speciﬁc sentiment features can be expanded in
our proposed model and improve the performance of senti-
ment classiﬁcation in the target domain.
4.2.2 Comparison With Related Cross-Domain
Sentiment Classiﬁcation Models
Meanwhile, we compare the performance of our proposed
model with current cross-domain sentiment classiﬁcation
models. As shown in Table 4, our proposed model achieves
higher accuracies in all cross-domain sentiment classiﬁca-
tion tasks. As we can observe, most of the cross-domain sen-
timent classiﬁcation models (e.g., DANN [40] , PBLM [35],
ACAN [43] IATN [42] and so on) mainly focus on extracting
the domain-invariant features by the way of unsupervised
learning, but ignoring the domain-speciﬁc features. As the
discrepancy between the source and target domains
increases, the performance of these models will decrease
substantially. They have a large gap in the performance
among different cross-domain tasks. For example, the
model KinGDOM almost have around 11% accuracy gap
among the cross-domain tasks (e.g., the two cross-domain
tasks E !B and E !K, as shown in Table 4). In contrast,
our proposed model only has 5:5%accuracy gap among all
the cross-domain tasks. To some extent, it can evaluate that
our proposed model can effectively capture the domain-
speciﬁc features of the target domain and narrow the dis-
crepancy between the source and target domains. Cur-
rently, to capture the domain-speciﬁc features, several
works (e.g., the CoCMD [14]) learn the domain-speciﬁc fea-
tures by providing a few (i.e., 50) number of labeled sam-
ples of the target domain. However, they are also based on
the deep neural networks and are prone to suffer from the
overﬁtting problem. As shown in Table 4, our proposed
model (with 5 support instances) even achieves better per-
formance with a large margin than the CoCMD (with 50
support instances). It can evaluate that our proposed model
can effectively solve the overﬁtting problem and learn the
rich domain-speciﬁc features with only a few support
instances. Moreover, similar to our proposed model, the
model KinGDOM [9] also utilizes the external commonsenseknowledge graph in the cross-domain adaptation. It adopts
the adversarial training strategy to capture the domain-
invariant features. Nevertheless, KinGDOM also ignores the
domain-speciﬁc features which are also the strong indica-
tors for the sentiment analysis for the target domain. More-
over, the relations between the aspect and opinion can not
be modeled, which leads to the sentiment transfer error
problem. As we can observe, our proposed model obtains
higher accuracies with a large margin than the model KinG-
DOM in all cross-domain tasks, which can prove that our
model can effectively capture the domain-speciﬁc features
and the aspect-opinion correlation features in the external
commonsense knowledge learning.
Meanwhile, several supervised learning based baselines
are compared with our proposed model. As shown in
Table 5, both the unsupervised learning based methods
(e.g., BERT-DAAT [8] and SENTIX [10]) and our proposed
model even perform better than the supervised learning
based baselines with many shots (i.e., 1000). We analyze
that the performance of the supervised learning based base-
lines highly depends on large-scale labeled data of target
domain. The provided 1000 labeled samples even can not
satisfy the optimization of the deep neural network based
methods with the thousands of parameters.
4.2.3 Ablation Study
Several ablation experiments are conducted, as shown in
Table 6. Speciﬁcally, comparing with the performance of
AKFSM|, our proposed model AKFSM achieves higher
accuracies with a large margin in all experimental settings.
Though the pre-trained language models have achieved
remarkable performance in cross-domain NLP tasks [10],
they ignore the domain-invariant sentiment-speciﬁc knowl-
edge (e.g., the opinion words “ bad” and the emoticon). The
domain-adapted BERT encoder is enabled to learn and
understand the semantics of the domain-invariant sentiment
features, which beneﬁts the cross-domain few-shot learning.
In addition, to evaluate the effectiveness of the expanded rela-
tional knowledge in our proposed model, we conduct the
ablation experiment for the module Graph Feature Enocder .
Comparing with the performance of AKFSM/C5and AKFSM ,
our model with the Graph Feature Encoder achieves higher
performance with a large margin. We analyze that the rela-
tional knowledge can effectively enrich the domain-speciﬁc
information based on the provided few support instances and
beneﬁt the performance of cross-domain sentiment analysis
tasks.
Moreover, the related work KinGDOM [9] also adopts the
external commonsense knowledge graph to enrich domainTABLE 5
Average Accuracies (%) Comparison With
Supervised Learning Baselines
Target Domain B D K E
CNN 63.1 69.2 73.5 70.9
LSTM 79.6 76.2 81.5 77.7
BERT 87.0 88.3 91.0 89.9
AKFSM(AVG) 91.7 91.7 94.1 96.5
The results of our model are the average accuracies of the three cross-domain
tasks.1700 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:22 UTC from IEEE Xplore.  Restrictions apply. features for the cross-domain sentiment analysis task. Dif-
ferent from our proposed model, the relations between
aspect and opinion terms cannot be modeled, which leads
to the sentiment transfer error problem, as shown in Fig. 1.
KinGDOM adopts the adversarial learning strategy to cap-
ture the domain-invariant features but ﬁlters out the
domain-speciﬁc features which are also the strong indica-
tors for the sentiment classiﬁcation in the target domain.
Speciﬁcally, comparing with the model AKFSMtu, our pro-
posed model AKFSM with the Aspect-Opinion Correlation
aware Graph Feature Learning module performs better with a
large margin, which can evaluate that our model can effec-
tively solve the sentiment transfer error problem and cap-
ture the domain-speciﬁc features. Furthermore, we also
conduct the ablation experiments for the representation
fusion strategy (i.e., the feature mapping layer with a recon-
struction loss £ recon). Compared with the performance of the
model AKFSMyin Table 6, our model with the reconstruc-
tion loss achieves better performance in all cross-domain
experimental settings. The performance of the prototypical
network highly depends on the spacial distribution of
instance embeddings [51]. We consider that the discrepancy
of the embedding spaces from the graph feature encoder
and domain-adapted BERT encoder leads to bias in the dis-
tance metric, thereby degrading the performance of proto-
typical networks. The comparable experimental results can
evaluate the effectiveness of the designed feature fusion
strategy with reconstruction loss.In addition, the self-supervised sentiment alignment task
is designed in our model to force the GCN encoder to capture
the sentiment alignment features among aspect-opinion pairs
by exploiting their co-occurrence features within documents.
To evaluate its effectiveness, an ablation experiment is con-
ducted (i.e., the model AKFSMzand AKFSM in Table 6). The
self-supervised sentiment alignment task in Phase 1 can effec-
tively improve the performance of cross-domain sentiment
analysis. We analyze that the sentiments of the aspect-opin-
ion pairs can be derived through the sentiments of their con-
textual aspect-opinion pairs. The sentiment information
(especially for the domain-speciﬁc features) can be captured
by aligning the sentiment among the contextual aspect-opin-
ion pairs.
Finally, we conduct the ablation experiments for the
shared-knowledge aware attention. According to our obser-
vation, not all the support external knowledge beneﬁts the
sentiment analysis of the query instances. Motivated by this,
the shared-knowledge aware attention is designed in ourTABLE 6
Ablation Experiments for the Modules of our Proposed Model in the 5-Shot Setting
S!T AKFSM|AKFSM/C5AKFSMtuAKFSMyAKFSMzAKFSM€AKFSM
D!B 86.0 89.2 89.6 91.0 91.3 91.9 92.5
E!B 81.8 89.0 88.4 90.1 91.0 90.5 91.4
K!B 83.8 89.1 86.4 90.7 90.4 90.8 91.2
B!D 85.0 89.4 89.0 92.0 89.5 90.9 92.3
E!D 82.2 88.8 90.4 91.0 89.5 90.6 91.4
K!D 83.8 85.8 88.4 89.9 89.3 90.2 91.5
B!E 83.8 89.4 89.6 92.5 92.5 92.8 93.7
D!E 82.6 89.2 90.2 92.1 92.5 92.1 93.9
K!E 89.8 90.3 92.0 93.5 93.8 93.9 94.7
B!K 85.6 89.8 91.2 94.8 95.2 95.5 96.7
D!K 83.4 90.8 91.4 95.8 95.7 95.3 96.2
E!K 87.6 85.0 92.4 95.5 95.8 94.9 96.5
Avg 85.4 88.8 89.9 92.4 92.2 92.5 93.5
AKFSM|denotes our proposed model which replaces the Domain-adapted BERT Encoder with BERT Encoder without post-training; AKFSM/C5denotes our
proposed model without the Graph Feature Encoder ;AKFSMtudenotes our proposed model which replaces the aspect-opinion correlation aware sentiment
knowledge graph with the knowledge graph in KinGDOM [9]; AKFSMydenotes our proposed model without graph feature reconstruction strategy. AKFSMz
denotes our proposed model without self-supervised sentiment alignment learning task for the GCN encoder pretraining in Phase 1. AKFSM€denotes our pro-
posed model without the shared-knowledge aware attention.
TABLE 7
Average Accuracies (%) Comparison of our Proposed Model With 1-hop Knowledge Linking
Strategy and 2-hop Knowledge Linking Strategy in Phase 1
S!TD !BE !BK !BB !DE !DK !DB !ED !EK !EB !KD !KE !K Avg
1-shot1-hop 88.8 89.1 89.3 88.9 88.5 87.7 90.7 90.3 91.9 92.6 94.2 94.7 90.6
2-hop 90.5 89.3 91.5 90.5 89.0 88.4 91.3 91.4 93.2 93.9 94.6 94.9 91.5
5-shot1-hop 92.5 91.4 91.2 92.3 91.4 91.5 93.7 93.9 94.7 96.7 96.2 96.5 93.5
2-hop 93.8 92.0 92.0 93.0 92.0 91.8 93.9 94.0 94.8 96.8 96.5 96.7 93.9
TABLE 8
The Comparison of Resource Cost Between the Model
With 1-hop Knowledge Triplets Strategy and 2-hop
Knowledge Triplets Strategy
Settings Num. of Triplets Training Time Memory Cost
1-hop 1 448 735 188.89 hour 5046Mib
2-hop 2 941 072 387.12 hour 15219MibREN ET AL.: ASPECT-OPINION CORRELATION AWARE AND KNOWLEDGE-EXPANSION FEW SHOT CROSS-DOMAIN SENTIMENT ... 1701
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:22 UTC from IEEE Xplore.  Restrictions apply. paper to select the important and relevant knowledge nodes
in the graph. As shown in Table 6, comparing with AKFSM€
and AKFSM, our model with shared-knowledge aware
attention obtains better performance, which evaluates the
effectiveness of the shared-knowledge aware attention.
4.2.4 Analysis for N-Hop Knowledge Linking Strategy
We conduct the comparative experiments between our pro-
posed model with 1-hop knowledge linking strategy and
that with 2-hop knowledge linking strategy adopted in the
Knowledge Graph Construction of Phase 1. As shown in
Table 7, we can observe that the model with 2-hop knowl-
edge linking strategy outperforms the model with 1-hop
linking knowledge strategy by only less than 1%. Further-
more, we also conduct the resource cost comparison, as
shown in Table 8. Under the same experimental settings
(e.g., the batch size is set as 10 in the pre-training stage of
Phase 1), the model with 2-hop knowledge linking strategy
costs twice as much pre-training times as the model with 1-
hop knowledge linking strategy. What’s more, the model
with 2-hop knowledge linking strategy needs nearly three
times as much memory usage as the model with 1-hop
knowledge linking strategy. Compared with the model with
1-hop knowledge linking strategy, the model with the 2-hop
knowledge linking strategy has only a slight improvement in
performance but requires more than twice or three times the
resource cost. The selection of the number of hops in linking
knowledge triplets is a trade-off between the performance
and resource cost. The experimental results evaluate that the
1-hop linked knowledge triplets from the ConceptNet can
already effectively describe and understand the terms of the
reviews.
4.2.5 Viusalization
To better understand the effectiveness of our proposed
model, we randomly select 100 support instances from posi-
tive and negative categories and encode them into the hid-
den embeddings in the task of cross-domain (i.e., from
Kitchen domain to Electronic domain) sentiment classiﬁca-
tion. Then, we map them into 2Dpoints using Principal
Component Analysis (PCA). As shown in Fig. 5, the instan-
ces expressing the same sentiment polarity are clustered
together in the same distribution space, which demonstrates
that the model performs better in domain adaptation task. Spe-
ciﬁcally, the effectiveness of domain-adapted BERT encoder
can be evaluated by comparing with Figs. 5a and 5d. We can
observe that the semantic understanding of the domain-spe-
ciﬁc (i.e., the target domain) features are signiﬁcant for thecross-domain learning. Moreover, Fig. 5b shows the instance
embedding distribution of our proposed model without the
Graph Feature Encoder .C o m p a r i n gw i t hF i g s .5 ba n d5 d ,i tc a n
evaluate that our model with the aspect-opinion correlation
aware graph feature learning module can effectively distin-
guish the positive and negative sentiment polarities in the
same feature space. The relations between the aspect and opin-
ion terms are beneﬁcial to the cross-domain learning and effec-
tively solve the sentiment transfer errors. Finally, we conduct
the visualization analysis for our proposed model with the
shared-knowledge aware attent ion. Comparing with Figs. 5c
and 5d, we can ﬁnd that the model with a shared-knowledge
attention module can better distinguish the positive and nega-
tive sentiment polarities in the feature space, which can evalu-
ate the effectiveness of the attention strategy in our model.
5C ONCLUSION
In this paper, we propose an aspect-opinion correlation
aware and knowledge-expansion cross-domain sentiment
classiﬁcation model in the few-shot scenario. To solve the
domain-speciﬁc features ignoring problem in current unsu-
pervised domain adaptation methods, a few-shot cross-
domain sentiment classiﬁcation model is designed and can
effectively capture the domain-speciﬁc features with only a
few support instances. Furthermore, to solve the sentiment
transfer error problem, we design an aspect-opinion correc-
tion aware graph learning module to capture the relational
features between the aspect and opinion terms. The exten-
sive experimental results and visualization analysis show
that our proposed model obtains better performance in the
cross-domain sentiment classiﬁcation with only a few sup-
port data.
REFERENCES
[1] L. Zhang, S. Wang, and B. Liu, “Deep learning for sentiment anal-
ysis: A survey,” Wiley Interdiscipl. Rev. Data Mining Knowl. Discov. ,
vol. 8, no. 4, 2018, Art. no. e1253.
[2] T. Shoryu, L. Wang, and R. Ma, “A deep neural network approach
using convolutional network and long short term memory for text
sentiment classiﬁcation,” in Proc. IEEE 24th Int. Conf. Comput. Sup-
ported Cooperative Work Des. , 2021, pp. 763–768.
[3] M. Yang, W. Yin, Q. Qu, W. Tu, Y. Shen, and X. Chen, “Neural
attentive network for cross-domain aspect-level sentiment classi-
ﬁcation,” IEEE Trans. Affective Comput. , vol. 12, no. 3, pp. 761–775,
Jul.–Sep. 2019.
[4] H. Tang, Y. Mi, F. Xue, and Y. Cao, “Graph domain adversarial
transfer network for cross-domain sentiment classiﬁcation,” IEEE
Access , vol. 9, pp. 33 051–33 060, 2021.
[5] S. Zhang, X. Bai, L. Jiang, and H. Peng, “Dual adversarial network
based on bert for cross-domain sentiment classiﬁcation,” in Proc.
CCF Int. Conf. Natural Lang. Process. Chin. Comput. , 2021, pp. 557–569.
Fig. 5. Visualization analysis in cross-domain sentiment classiﬁcation; KþandEþdenote the positive category respectively from Kitchen domain
andElectronic domain; K/C0andE/C0denote the negative category respectively from Kitchen domain and Electronic domain.1702 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:22 UTC from IEEE Xplore.  Restrictions apply. [6] C. D. Manning, M. Surdeanu, J. Bauer, J. R. Finkel, S. Bethard, and
D. McClosky, “The stanford corenlp natural language processing
toolkit,” in Proc. 52nd Annu. Meeting Assoc. Comput. Linguistics
Syst. Demonstrations , 2014, pp. 55–60.
[7] Z. Li, X. Li, Y. Wei, L. Bing, Y. Zhang, and Q. Yang, “Transferable
end-to-end aspect-based sentiment analysis with selective adver-
sarial learning,” in Proc. Conf. Empirical Methods Natural Lang. Pro-
cess. 9th Int. Joint Conf. Natural Lang. Process. , 2019, pp. 4590–4600.
[8] C. Du, H. Sun, J. Wang, Q. Qi, and J. Liao, “Adversarial and
domain-aware bert for cross-domain sentiment analysis,” in Proc.
58th Annu. Meeting Assoc. Comput. Linguistics , 2020, pp. 4019–4028.
[9] D. Ghosal, D. Hazarika, A. Roy, N. Majumder, R. Mihalcea, and S.
Poria, “Kingdom: Knowledge-guided domain adaptation for sen-
timent analysis,” in Proc. 58th Annu. Meeting Assoc. Comput. Lin-
guistics , 2020, pp. 3198–3210.
[10] J. Zhou, J. Tian, R. Wang, Y. Wu, W. Xiao, and L. He, “SentiX: A sen-
timent-aware pre-trained model for cross-domain sentiment analy-
sis,” in Proc. 28th Int. Conf. Comput. Linguistics , 2020, pp. 568–579.
[11] X. Zhang, J. Xu, Y. Cai, X. Tan, and C. Zhu, “Detecting depen-
dency-related sentiment features for aspect-level sentiment classi-
ﬁcation,” IEEE Trans. Affective Comput. , to be published,
doi:10.1109/TAFFC.2021.3063259 .
[12] S. Wu, H. Fei, Y. Ren, D. Ji, and J. Li, “Learn from syntax: Improv-
ing pair-wise aspect and opinion terms extractionwith rich syntac-
tic knowledge,” in Proc. 13th Int. Joint Conf. Artif. Intell. , 2021,
pp. 3957–3963.
[13] S. J. Pan and Q. Yang, “A survey on transfer learning,” IEEE Trans.
Knowl. Data Eng. , vol. 22, no. 10, pp. 1345–1359, Oct. 2010.
[14] M. Peng, Q. Zhang, Y.-G. Jiang, and X.-J. Huang, “Cross-domain
sentiment classiﬁcation with target domain speciﬁc information,”
inProc. 56th Annu. Meeting Assoc. Comput. Linguistics , 2018,
pp. 2505–2513.
[15] H. Ren, Y. Cai, X. Chen, G. Wang, and Q. Li, “A two-phase proto-
typical network model for incremental few-shot relation classi-
ﬁcation,” in Proc. 28th Int. Conf. Comput. Linguistics , 2020,
pp. 1618–1629.
[16] J. Snell, K. Swersky, and R. Zemel, “Prototypical networks for
few-shot learning,” in Proc. 31st Int. Conf. Neural Informat. Process.
Syst., 2017, pp. 4080–4090.
[17] T. Gao et al., “FewRel 2.0: Towards more challenging few-shot
relation classiﬁcation,” in Proc. Conf. Empirical Methods Natural
Lang. Process. 9th Int. Joint Conf. Natural Lang. Process. , 2019,
pp. 6250–6255.
[18] A. Bra /C20zinskas, M. Lapata, and I. Titov, “Few-shot learning for
opinion summarization,” in Proc. Conf. Empirical Methods Natural
Lang. Process. , 2020, pp. 4119–4135.
[19] W.-Y. Chen, Y.-C. Liu, Z. Kira, Y.-C. F. Wang, and J.-B. Huang, “A
closer look at few-shot classiﬁcation,” in Proc. Int. Conf. Learn. Rep-
resentations , 2018, pp. 1–17.
[20] R. Speer, J. Chin, and C. Havasi, “ConceptNet 5.5: An open multi-
lingual graph of general knowledge,” in Proc. AAAI Conf. Artif.
Intell. , 2017, pp. 4444–4451.
[21] J. Xu, D. Chen, X. Qiu, and X.-J. Huang, “Cached long short-term
memory neural networks for document-level sentiment classi-
ﬁcation,” in Proc. Conf. Empirical Methods Natural Lang. Process. ,
2016, pp. 1660–1669.
[22] Z. Yang, D. Yang, C. Dyer, X. He, A. Smola, and E. Hovy,
“Hierarchical attention networks for document classiﬁcation,” in
Proc. Conf. North Amer. Chapter Assoc. Comput. Linguistics Hum.
Lang. Technol. , 2016, pp. 1480–1489.
[23] Y. Yin, Y. Song, and M. Zhang, “Document-level multi-aspect sen-
timent classiﬁcation as machine comprehension,” in Proc. Conf.
Empirical Methods Natural Lang. Process. , 2017, pp. 2044–2054.
[24] H. Chen, Z. Zhai, F. Feng, R. Li, and X. Wang, “Enhanced multi-
channel graph convolutional network for aspect sentiment triplet
extraction,” in Proc. 60th Annu. Meeting Assoc. Comput. Linguistics ,
2022, pp. 2974–2985.
[25] M. Zhao, J. Yang, J. Zhang, and S. Wang, “Aggregated graph con-
volutional networks for aspect-based sentiment classiﬁcation,”
Inf. Sci. , vol. 600, pp. 73–93, 2022.
[26] A. Dai, X. Hu, J. Nie, and J. Chen, “Learning from word semantics
to sentence syntax by graph convolutional networks for aspect-
based sentiment analysis,” Int. J. Data Sci. Analytics , vol. 14, no. 1,
pp. 17–26, 2022.
[27] Z. Zhao, M. Tang, W. Tang, C. Wang, and X. Chen, “Graph convolu-
tional network with multiple weight mechanisms for aspect-based
sentiment analysis,” Neurocomputing , vol. 500, pp. 124–134, 2022.[28] W. Li, S. Yin, and T. Pu, “Lexical attention and aspect-oriented
graph convolutional networks for aspect-based sentiment analy-
sis,” J. Intell. Fuzzy Syst. , vol. 42, pp. 1–12, 2022.
[29] D. Anand and B. S. Mampilli, “A novel evolutionary approach for
learning syntactic features for cross domain opinion target extrac-
tion,” Appl. Soft Comput. , vol. 102, 2021, Art. no. 107086.
[30] J. Blitzer, R. McDonald, and F. Pereira, “Domain adaptation with
structural correspondence learning,” in Proc. Conf. Empirical Meth-
ods Natural Lang. Process. , 2006, pp. 120–128.
[31] D. Bollegala, D. Weir, and J. Carroll, “Cross-domain sentiment
classiﬁcation using a sentiment sensitive thesaurus,” IEEE Trans.
Knowl. Data Eng. , vol. 25, no. 8, pp. 1719–1731, Aug. 2012.
[32] J. Jiang and C. Zhai, “Instance weighting for domain adaptation in
nlp,” in Proc. 45th Annu. Meeting Assoc. Comput. Linguistics , 2007,
pp. 264–271.
[33] Y. Ganin and V. Lempitsky, “Unsupervised domain adaptation by
backpropagation,” in Proc. Int. Conf. Mach. Learn. , 2015, pp. 1180–1189.
[34] Z. Li, Y. Wei, Y. Zhang, and Q. Yang, “Hierarchical attention
transfer network for cross-domain sentiment classiﬁcation,” in
Proc. AAAI Conf. Artif. Intell. , 2018, pp. 5773–5780.
[35] Y. Ziser and R. Reichart, “Pivot based language modeling for
improved neural domain adaptation,” in Proc. Conf. North Amer.
Chapter Assoc. Comput. Linguistics Hum. Lang. Technol. , 2018,
pp. 1241–1251.
[36] L. Li, W. Ye, M. Long, Y. Tang, J. Xu, and J. Wang, “Simultaneous
learning of pivots and representations for cross-domain sentiment
classiﬁcation,” in Proc. AAAI Conf. Artif. Intell. , 2020, pp. 8220–8227.
[37] T. Manshu and W. Bing, “Adding prior knowledge in hierarchical
attention neural network for cross domain sentiment classi-
ﬁcation,” IEEE Access , vol. 7, pp. 32 578–32 588, 2019.
[38] Y. Fu and Y. Liu, “Cross-domain sentiment classiﬁcation based on
key pivot and non-pivot extraction,” Knowl.-Based Syst. , vol. 228,
2021, Art. no. 107280.
[39] A. Geethapriya and S. Valli, “An enhanced approach to map
domain-speciﬁc words in cross-domain sentiment analysis,” Infor-
mat. Syst. Front. , vol. 23, pp. 1–15, 2021.
[40] Y. Ganin et al., “Domain-adversarial training of neural networks,”
J. Mach. Learn. Res. , vol. 17, no. 1, pp. 2096–2030, 2016.
[41] Z. Li, Y. Zhang, Y. Wei, Y. Wu, and Q. Yang, “End-to-end
adversarial memory network for cross-domain sentiment clas-
siﬁcation,” in Proc. 26th Int. Joint Conf. Artif. Intell. , 2017,
pp. 2237–2243.
[42] K. Zhang, H. Zhang, Q. Liu, H. Zhao, H. Zhu, and E. Chen,
“Interactive attention transfer network for cross-domain senti-
ment classiﬁcation,” in Proc. AAAI Conf. Artif. Intell. , 2019,
pp. 5773–5780.
[43] X. Qu, Z. Zou, Y. Cheng, Y. Yang, and P. Zhou, “Adversarial cate-
gory alignment network for cross-domain sentiment classi-
ﬁcation,” in Proc. Conf. North Amer. Chapter Assoc. Comput.
Linguistics: Hum. Lang. Technol. , 2019, pp. 2496–2508.
[44] J. D. M.-W. C. Kenton and L. K. Toutanova, “BERT: Pre-training of
deep bidirectional transformers for language understanding,” in
Proc. NAACL-HLT , 2019, pp. 4171–4186.
[45] B. Liang, H. Su, L. Gui, E. Cambria, and R. Xu, “Aspect-based senti-
ment analysis via affective knowledge enhanced graph convolu-
tional networks,” Knowl. Based Syst. , vol. 235, 2022, Art. no. 107643.
[46] H. Wu, Z. Zhang, S. Shi, Q. Wu, and H. Song, “Phrase dependency
relational graph attention network for aspect-based sentiment
analysis,” Knowl. Based Syst. , vol. 236, 2022, Art. no. 107736.
[47] S. Liang, W. Wei, X.-L. Mao, F. Wang, and Z. He, “BiSyn-GAT+:
Bi-syntax aware graph attention network for aspect-based senti-
ment analysis,” in Proc. Findings Assoc. Comput. Linguistics , 2022,
pp. 1835–1848.
[48] Z. Zhang, Z. Zhou, and Y. Wang, “SSEGCN: Syntactic and seman-
tic enhanced graph convolutional network for aspect-based senti-
ment analysis,” in Proc. Conf. North Amer. Chapter Assoc. Comput.
Linguistics Hum. Lang. Technol. , 2022, pp. 4916–4925.
[49] V. G. Satorras and J. B. Estrach, “Few-shot learning with graph neural
networks,” in Proc. Int. Conf. Learn. Representations , 2018, pp. 1–13.
[50] T. Munkhdalai and H. Yu, “Meta networks,” Proc. Mach. Learn.
Res., vol. 70, pp. 2554–2563, 2017.
[51] T. Gao, X. Han, Z. Liu, and M. Sun, “Hybrid attention-based pro-
totypical networks for noisy few-shot relation classiﬁcation,” in
Proc. AAAI Conf. Artif. Intell. , 2019, pp. 6407–6414.
[52] R. Geng, B. Li, Y. Li, J. Sun, and X. Zhu, “Dynamic memory induc-
tion networks for few-shot text classiﬁcation,” in Proc. 58th Annu.
Meeting Assoc. Comput. Linguistics , 2020, pp. 1087–1094.REN ET AL.: ASPECT-OPINION CORRELATION AWARE AND KNOWLEDGE-EXPANSION FEW SHOT CROSS-DOMAIN SENTIMENT ... 1703
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:22 UTC from IEEE Xplore.  Restrictions apply. [53] I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harness-
ing adversarial examples,” Statistical , vol. 1050, 2015, Art. no. 20.
[54] C. Han, Z. Fan, D. Zhang, M. Qiu, M. Gao, and A. Zhou, “Meta-
learning adversarial domain adaptation network for few-shot text
classiﬁcation,” in Proc. Findings Assoc. Comput. Linguistics: ACL-
IJCNLP , 2021, pp. 1664–1673.
[55] C. Zhang and D. Song, “A simple baseline for cross-domain few-
shot text classiﬁcation,” in Proc. CCF Int. Conf. Natural Lang. Pro-
cess. Chin. Comput. , 2021, pp. 700–708.
[56] H. Cai, V. W. Zheng, and K. C.-C. Chang, “A comprehensive survey
of graph embedding: Problems, techniques, and applications,” IEEE
Trans. Knowl. Data Eng. , vol. 30, no. 9, pp. 1616–1637, Sep. 2018.
[57] M. Schlichtkrull, T. N. Kipf, P. Bloem, R. Van Den Berg, I. Titov, and
M. Welling, “Modeling relational data with graph convolutional
networks,” in Proc. Eur. Semantic Web Conf. , 2018, pp. 593–607.
[58] H. T. Phan, N. T. Nguyen, and D. Hwang, “Convolutional atten-
tion neural network over graph structures for improving the perfor-
mance of aspect-level sentiment analysis,” Informat. Sci. , vol. 589,
pp. 416–439, 2022.
[59] B. Yang, S. W.-T. Yih, X. He, J. Gao, and L. Deng, “Embedding
entities and relations for learning and inference in knowledge
bases,” in Proc. Int. Conf. Learn. Representations , 2015, pp. 1–12.
[60] J. Blitzer, M. Dredze, and F. Pereira, “Biographies, bollywood,
boom-boxes and blenders: Domain adaptation for sentiment clas-
siﬁcation,” in Proc. 45th Annu. Meeting Assoc. Comput. Linguistics ,
2007, pp. 440–447.
[61] N. Mishra, M. Rohaninejad, X. Chen, and P. Abbeel, “A simple
neural attentive meta-learner,” in Proc. Int. Conf. Learn. Representa-
tions, 2018, pp. 1–17.
[62] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-
training of deep bidirectional transformers for language under-
standing,” in Proc. Conf. North Amer. Chapter Assoc. Comput. Lin-
guistics: Hum. Lang. Technol. , 2019, pp. 4171–4186.
Haopeng Ren received the BS degree from the
School of Information and Computing Science,
Zhongkai University of Agriculture and Engineer-
ing, Guangdong, China. He is currently working
toward the PhD degree with the School of Soft-
ware Engineering, South China University of
Technology , Guangdong, China. His research
interest focuses on information extraction, knowl-
edge graph, and natural language processing.
Yi Cai (Member, IEEE) received the PhD degree
in computer science from the Chinese University
of Hong Kong. He is currently a professor in South
China University of Technology (SCUT). His
research interests include recommendation sys-
tem, personalized search, semantic web, and
data mining. His research works are published on
many conferences and journals, such as IEEE
Transactions on Knowledge and Data Engineer-
ing (TKDE) ,Neural Networks, Knowledge-based
Systems ,EAAI and Neurocomputing , as well as
AAAI, COLING, CIKM, AAMAS, DASFAA and other international confer-
ences about perspective mining, cognitive modeling, information retrieval
and semantic analysis.
Yushi Zeng received the BS degree from the
School of Software Engineering, Jishou University
of Agriculture and Engineering, Hunan, China. She
is currently working toward the master degree with
the School of Software Engineering, South China
University of Technology, Guangdong, China. Her
research interest focuses on information extrac-
tion, knowledge graph, sentiment analysis, and
natural language processing.
Jinghui Ye is currently working toward the MPhill
degree with the Hong Kong University of Science
and Technology (Guangzhou). Moreover, he is
taking an internship in Tencent AI Lab. His current
research interest is visual-language problems,
including summarization, VQA, and sign lan-
guage translation.
Ho-fung Leung (Senior Member, IEEE) received
the BSc and MPhil degrees in computer science
from the Chinese University of Hong Kong, and
the PhD degree from the University of London,
with DIC (Diploma of Imperial College) in Com-
puting from Imperial College London. He is a pro-
fessor in the Department of Computer Science
and Engineering with the Chinese University of
Hong Kong. He has authored more than 250 pub-
lications, including 5 research monographs, and
5 edited volumes. He was the chairperson of
ACM (Hong Kong Chapter) in 1998. He is a Chartered Fellow of the
BCS, a Fellow of the HKIE, a full member the HKCS. He is a Chartered
Engineer registered by the Engineering Council.
Qing Li (Senior Member, IEEE) received the
BEng degree in computer science from Hunan
University, Changsha, and the MSc and PhD
degrees in computer science, from the University
of Southern California, Los Angeles. He is cur-
rently a chair professor with the Department of
Computing, The Hong Kong Polytechnic Univer-
sity . His research interests include multi-modal
data management, conceptual data modeling,
social media, Web services, and e-learning sys-
tems. He has authored or coauthored more than
400 publications in these areas. He is actively involved in the research
community . He is a Fellow of IEE/IET , U.K., and a Distinguished Member
of CCF , China.
"For more information on this or any other computing topic,
please visit our Digital Library at www.computer .org/csdl.1704 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:22 UTC from IEEE Xplore.  Restrictions apply. Short and Long Range Relation Based
Spatio-Temporal Transformer
for Micro-Expression Recognition
Liangfei Zhang , Xiaopeng Hong ,Member, IEEE ,
Ognjen Arandjelovi /C19c,Member, IEEE , and Guoying Zhao ,Fellow, IEEE
Abstract— Being spontaneous, micro-expressions are useful in the inference of a person’s true emotions even if an attempt is made to
conceal them. Due to their short duration and low intensity , the recognition of micro-expressions is a difﬁcult task in affective computing. The
early work based on handcrafted spatio-temporal features which showed some promise, has recently been superseded by different deep
learning approaches which now compete for the state of the art performance. Nevertheless, the problem of capturing both local and global
spatio-temporal patterns remains challenging. To this end, herein we propose a novel spatio-temporal transformer architecture – to the best
of our knowledge, the ﬁrst purely transformer based approach (i.e., void of any convolutional network use) for micro-expression recognition.
The architecture comprises a spatial encoder which learns spatial patterns, a temporal aggregator for temporal dimension analysis, and a
classiﬁcation head. A comprehensive evaluation on three widely used spontaneous micro-expression data sets, namely SMIC-HS, CASME
II and SAMM, shows that the proposed approach consistently outperforms the state of the art, and is the ﬁrst framework in the published
literature on micro-expression recognition to achieve the unweighted F1-score greater than 0.9 on any of the aforementioned data sets. The
source code is available at https://github.com/Vision-Intelligence-and-Robots-Group/SLSTT .
Index Terms— Emotion recognition, long-term optical ﬂow, temporal aggregator, self-attention mechanism
Ç
1I NTRODUCTION
FACIAL expressions play an important role in interpersonal
communication and their recognition is one of the most
signiﬁcant tasks in affective computing. Though there some
disagreement on this remains, a notable number of psycholo-
gists believe that although due to different cultural environ-
ments individuals use different languages to communicate,
the expression of their emotions is rather universal [1]. Cor-
rectly recognizing facial expressions is important in general
communication and can help understanding people’s mental
state and emotions.
When colloquially used, the term ‘facial expressions’ refers
to what are more precisely technically termed facial macro-expressions (MaEs). While crucial for human interaction, provid-
ing a universal and non-verbal means of articulating emotion
[2], facial macro-expressions c an be effected voluntarily which
means that they can be used to deceive. In other words, a per-
son’s macro-expression may not accurately represent their truly
felt emotion. However, whatever the conscious effort, felt emo-
tions effect short-lasting contra ction of facial muscles which
are expressed involuntarily under psychological inhibition. The
resulting minute, sudden, and transient expressions are
referred to as micro-expressions (MEs). After being ﬁrst observed
and recognized as a phenomenon of interest by Haggard and
Isaacs [3], and then further elaborated on by a case study
reported by Ekman and Friesen [4], MEs began to be researched
more widely by psychologists, and in the last decade attracting
interest within the ﬁeld of computer vision [5]. In contrast to
MaEs, MEs are subtle. They are exhibited for 0.04s to 0.2s [1],
and with lesser facial movement. These characteristics make
MEs harder to be recognized than MaEs, whether manually
( i . e . ,b yh u m a n s )o ra u t o m a t i c a l l y( i . e . ,b yc o m p u t e r s ) .
The seminal work by Pﬁster, et al. and the release of the
database of micro-expression movie clips, namely SMIC-sub
(Spontaneous Micro-expression Corpus) [6], effected a
marked empowerment of computer scientists in the realm of
micro-expression recognition (MER). The ﬁrst generation of
solutions built upon the well-established computer vision
tradition and introduced a series of handcrafted features,
such as Local Binary Pattern-Three Orthogonal Planes (LBP-
TOP) [7], 3 Dimensional Histograms of Oriented Gradients
(3DHOG) [8], Histograms of Image Gradient Orientation
(HIGO) [9] and Histograms of Oriented Optical Flow
(HOOF) [10] and their variations. The next generation shifted/C15Liangfei Zhang and Ognjen Arandjelovi /C19c are with the School of Computer
Science, University of St Andrews, KY16 9AJ St Andrews, U.K.
E-mail: {lz36, oa7}@st-andrews.ac.uk.
/C15Xiaopeng Hong is with the Harbin Institute of Technology, Harbin,
Heilongjiang 150001, China. E-mail: hongxiaopeng@ieee.org.
/C15Guoying Zhao is with the University of Oulu, 90570 Oulu, Finland.
E-mail: guoying.zhao@oulu.ﬁ.
Manuscript received 30 March 2022; revised 25 June 2022; accepted 7 October
2022. Date of publication 10 October 2022; date of current version 15 Novem-
ber 2022.
This work was supported by the China Scholarship Council – University of St
Andrews Scholarships under Grant 201908060250 funds Liangfei Zhang for
her PhD, in part by the National Key Research and Development Project of
China under Grant 2019YFB1312000, in part by the National Natural Science
Foundation of China under Grant 62076195, and in part by the Fundamental
Research Funds for the Central Universities under Grant AUGA5710011522.
(Corresponding author: Xiaopeng Hong.)
Recommended for acceptance by S. Wang.
Digital Object Identiﬁer no. 10.1109/TAFFC.2022.3213509IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022 1973
1949-3045 ©2022 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See ht _tps://www.ieee.org/publications/rights/index.html for more information.
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:23 UTC from IEEE Xplore.  Restrictions apply. focus towards Convolutional Neural Network (CNN) based
deep learning methods [11], [12], [13], [14], [15]. Early work
by and large uses convolutional kernels to extract spatial
information from micro-expression video frames. This kind
of pixel level operators can be considered as capturing
“short-range ”, local spatial relationships. “ Long-range ”, global
relationships between different spatial regions have also
been proposed and studied, notably by means of Graph Con-
volutional Network (GCN) based architectures [16], [17],
[18], [19], [20]. The activations of Facial Action Units (AUs)
are generally used as nodes to build graphs. The relation-
ships between different AU engagements are combined with
image features to improve the discriminatory power in the
context of MER. However, though these approaches consider
global spatial relations so as to assist learning, they can only
learn these after local features are extracted, i.e., they are
unable to learn both kinds of relations jointly.
In order to capture automatically both short- and long-
range relations at the same time, we apply Multi-head Self-
attention Mechanism (MSM) instead of a Convolutional
Kernel as the cornerstone of our deep learning MER architec-
ture. As shown in Fig. 1, the relations between block 1 and N
will hardly ever be learnt by CNN but has been considered
at the beginning of MSM. MSM based networks are called
Transformer . Short-range and long-range relationships
between elements of a sequence can be learned in a parallel-
ized manner because transformers utilize sequences in their
entirety, as opposed to processing sequence elements
sequentially like recurrent networks. Most recently, trans-
former networks came to the attention of the CV community.
By dividing them into smaller constituent patches, two-
dimensional images can be converted into one-dimensional
sequences, translating the spatial relationships into the rela-
tionships between sequence elements (image patches). In
this way, transformer networks can be simply applied to
vision problems and on various tasks they have outper-
formed CNNs [21]. Examples include segmentation [22],
image super-resolution [23], image recognition [24], [25],
video understanding [26], [27] and object detection [28], [29].
Most MER research in the published literature is video
based, as Ben et al. elaborated [30], though there is a small
but notable body of work on single-frame analysis [31], [32],
[33]. This statistic reﬂects the consensus that for best perfor-
mance both spatial and temporal information need be con-
sidered. In particular, absolute and relative facial motions
are extracted and analysed through spatial and temporal
features respectively. Most handcrafted methods in exis-
tence use the same kind of operator to detect spatial and
temporal information from different dimensions by consid-
ering the frames as 3D data. The resulting spatio-temporalfeatures with uniform format are used together to imple-
ment video based MER. In deep learning based methods,
spatial features are mainly extracted by means of a convolu-
tional neural network. Some concatenate spatial features
extracted from each frame and others use recurrent neural
networks to derive temporal information. To integrate vari-
ous spatio-temporal relations, our design makes use of
long-term temporal information in spatial data (i.e., each
frame of video sample) prior to the spatial encoder, and a
temporal aggregation block to fuse both short- and long-
term temporal relationships afterwards.
In this work we show how a transformer based deep
learning architecture can be applied to MER in a manner
which outperforms the current state of the art. The main
contributions of the present work are as follows:
1) We propose a novel spatio-temporal deep learning
transformer framework for video based micro-
expression recognition, which we name Short and
Long range relation based Spatio-Temporal Transformer
(SLSTT) , the structure whereof is summarized in
Fig. 2. To the best of our knowledge, ours is the ﬁrst
deep learning MER work of this kind, in that it does
not employ a CNN at any stage, but is rather entirely
centred on a transformer architecture.
2) We use matrices of long-term optical ﬂow, computed
in a novel way particularly suited for MER, instead
of the original colour images as the input to our net-
work. The feature ultimately arrived at combines
long-term temporal information and short- and
long-range spatial relations, and is derived by a
transformer encoder block.
3) We design a temporal aggregation block to connect
spatio-temporal features of spatial relations extracted
from each frame by multiple transformer encoder
layers and achieve video based MER. The empirical
performance and analysis of mean and LSTM (long
short-term memory) aggregators is presented too.
We evaluate our approach on the three well known
and popular ME databases, Spontaneous Micro-Expression
Corpus (SMIC) [34], Chinese Academy of Sciences Micro-
Expression II (CASME II) [35] and Spontaneous Actions and
Fig. 1. Comparison of the different spatial feature extraction methods of
CNN and transformer .
Fig. 2. The framework of proposed Short and Long range relation based
Spatio-Temporal Transformer (SLSTT).1974 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:23 UTC from IEEE Xplore.  Restrictions apply. Micro-Movements (SAMM) [36], in both Sole Database Evalu-
ation (SDE) and Composite Database Evaluation (CDE) set-
tings and achieve state of the art results.
2R ELATED WORK
2.1 Micro-Expression Recognition
Since the publication of the SMIC data set in 2013, the volume
of research on automatic micro-expression recognition has
been increasing steadily over the years. From the handcrafted
computer vision methods in the early years to the deep learn-
ing approaches more recently, the main ideas of micro-expres-
sion feature extraction could be categorized as primarily
pursuing either a spatial strategy or a temporal one.
2.1.1 Spatial Features
The fundamental challenge of computer vision is that of
extracting semantic information from images or videos.
Whatever the approach, the extraction of some kind of spa-
tial features is central to addressing this challenge. Micro-
expression recognition is no exception. In a manner similar
to many gradient based features applied previously on
generic computer vision tasks, Polikovsky et al. [8] pro-
posed the use of a gradient feature adapted to MER to
describe local dynamics of the face. The magnitudes of local
gradient projections in the XYplane is used to construct
histograms across different regions, which are used as spa-
tial features. LBP quickly became the most popular operator
for micro-expression analysis after Pﬁster et al. [6] ﬁrst
applied it to MER. This operator describes local appearance
in an image. The key idea behind it is that the relative
brightness of neighbouring pixels can be used to describe
local appearance in a geometrically and photometrically
robust manner. Its widespread use and favourable perfor-
mance often make it the default baseline method when new
data sets are published, or a new ME related task proposed.
As for deep learning approaches, CNN model can be
thought as a combination of two components: a feature
extraction part and a classiﬁcation part. The convolution
and pooling layers perform spatial feature extraction.
Further to local appearance based features, numerous
other strategies have been described for spatial feature
extraction in micro-expression analysis. One of the simplest
and commonest of these employs facial Region Of Interest
(ROI) segmentation. Polikovsky et al. [8] segmented each
face sample into 12 regions according to the Facial Action
Coding System (FACS) [37], each region corresponding to an
independent facial muscle complex, and applied appearance
normalization to individual regions. Others have modiﬁed or
extended this strategy, e.g., employing different methods for
segmentation or different salient regions – 11 [38], 16 [39], 36
[10] instead of 12 of Polikovsky et al. Spatial feature operators
are applied with each ROI rather the whole image, thus pro-
viding a more nuanced description of the face. In recent
years, a more principled equivalent of this strategy (in that it
is learnt, rather than predetermined by a human), can be
found in the form of attention blocks applied within neural
networks to improve their ability to learn spatial features.
These blocks can generate weight masks for feature maps,
helping a network pay greater attention to signiﬁcant
regions. Most recently, GCNs have also been used withindeep learning frameworks as a means of capturing spatial
information, often using AUs as correponding to graph
nodes. For example, Lei et al. [20] segment node patches
based on facial landmarks and fuse them with an AU GCN.
Xie et al. [18] infer AU node features from the backbone fea-
tures by global average pooling and use them to build an AU
relation graph for GCN layers. These optimization measures
use a priori knowledge (AUs in FACS) to enhance the
extracted spatial features. Long-range spatial relationships
are not directly learnt by such networks.
2.1.2 Temporal Features
Since one of the most characteristic aspects of micro-expres-
sions is their sudden occurrence, temporal features cannot
be ignored. While some methods in the literature do use
only the single, apex frame instead of all frames in each ME
sample [31], [32], [33], [40], most employ all in the range
between the onset frame and the offset, thus treating all
temporal changes within this time period on the same foot-
ing. Some go further and employ temporal frame interpola-
tion (as indeed we do herein) so as to increase the frame
count [6], [9], [10], [12], [39].
A vast number of handcrafted feature based approaches
treat raw video data as a 3D spatio-temporal volume, treat-
ing the temporal dimension as no different than the spatial
ones. In other words, they apply the same kind of operator
used to extract spatial features on pseudo-images formed
by a cut through the 3D volume comprising one spatial
dimension and the temporal dimension. For example, in
LBP-TOP, LBP operators are applied on XTandYTplanes
to extract temporal features, and their histogram across the
three dimensions forms the ﬁnal representation. 3DHOG
similarly treats videos as spatio-temporal cuboids with no
distinction made between the three dimensions, but argu-
ably with even greater uniformity than LBP-TOP in that the
descriptor itself is inherently 3D based. Similar in this
regard are optical ﬂow based features, which too inherently
combine local spatial and temporal elements – the use of
optical strain [41], ﬂow orientation [10] or its magnitude
[31] are all variations on this theme.
As an alternative to the use of raw appearance imagery
as input to a deep learning network, the use of pre-proc-
essed data in the form of optic ﬂow matrices has been pro-
posed by some authors [15], [19], [42]. In this manner,
proximal temporal information is exploited directly. On the
other hand, the learning of longer range temporal patterns
has been approached in a variety of ways by different
authors. Some extract temporal patterns simply by treating
video sequences as 3-dimensional matrices [16], [41], [43],
rather than 2-dimensional ones which naturally capture sin-
gle images. Others employ structures such as the recurrent
neural network (RNN) or the LSTM [12], [44]. In addition to
the use of off-the-shelf recurrent deep learning strategies,
recently there has been an emergence of methods which
apply domain speciﬁc knowledge so as to make the learning
particularly effective for micro-expression analysis [15].
2.2 Transformers in Computer Vision
For approximately a decade now, convolutional neural net-
works have established themselves as the backbone of mostZHANG ET AL.: SHORT AND LONG RANGE RELATION BASED SPATIO-TEMPORAL TRANSFORMER FOR MICRO-EXPRESSION... 1975
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:23 UTC from IEEE Xplore.  Restrictions apply. deep learning algorithms in computer vision. However,
convolution always operates on ﬁxed size windows and is
thus unable to extract distal relations. The idea of a trans-
former was ﬁrst introduced in the context of NLP. It relies
on a self-attention mechanism, learning the relationships
between elements of a sequence. Transformers are able to
capture ‘long-term’ dependence between sequence elements
which is challenging for conventional recurrent models to
encode. By dividing an image into sub-images and impos-
ing a consistent ordering on them, a planar image can be
converted into a sequence, so spatial dependencies can be
learned in the same way as temporal features. For this rea-
son, transformer based deep learning architectures have
recently gained signiﬁcant attention from the computer
vision community and are starting to play an increasing
role in a number of computer vision tasks.
A representative example in the context of object detec-
tion is the DEtection TRansformer (DETR) [28] framework
which uses transformer blocks ﬁrst, for regression and
classiﬁcation, but the visual features are still extracted by a
CNN based backbone. The Image Generative Pre-Training
(iGPT) approach of Chen et al. [45] attempts to exploit the
strengths of transformers somewhat differently, pre-train-
ing BERT (Bidirectional Encoder Representations from
Transformers) [46], originally proposed for language
understanding, and thereafter ﬁne tuning the network
with a small classiﬁcation head. iGPT uses pixels instead
language tokens within BERT, but suffers from signiﬁcant
information loss effected by a necessary image resolution
reduction. In the context of classiﬁcation, the Vision Trans-
former (ViT) approach of Dosovitskiy et al. [24] applies
transformer encoding of image patches as a means of
extracting visual features directly. It is the ﬁrst pure vision
transformer, and in its spirit and design, follows the origi-
nal transformer [47] architec ture faithfully. As such, it
facilitates the application of scalable transformer architec-
tures used in NLP effortlessly.
Following these successes, transformers have been
applied to a variety of computer vision tasks, including
those in the realm of affective computing [48], [49]. Nota-
ble examples include facial action unit detection [50] and
facial image-based macro-e xpression recognition [51].
However, none of the existing approaches to micro-
expression recognition adequately make use of both the
spatial and temporal information due to the design difﬁ-
culties posed by the challenges we discussed in the pre-
vious sections.3P ROPOSED METHOD
In the present work we propose a method that takes advan-
tage both of the physiological understanding of micro-expres-
sions and their characteristics, as well as of the transformer
framework. The approach overcomes many of the weak-
nesses of the existing MER methods in the literature as
discussed in the previous section. Importantly, our method is
able to extract and thus beneﬁt both from proximal (i.e., short-
range) and distal (i.e., long-range) spatio-temporal features.
Each element of the proposed framework is laid out in detail
next, corresponds to each sub-section.
3.1 Long-Term Optical Flow
Optical ﬂow describes the apparent motion of brightness
patterns between frames, caused by the relative movement
of the content of a scene and the camera used to image it
[52]. If the camera is static, optical ﬂow can be used to infer
both the direction and the magnitude of an imaged object’s
movement from the change in the appearance of pixels
between frames [53].
Optical ﬂow is inherently temporally local, i.e., save for
practical considerations (numerical, efﬁciency, etc.) it is
computed between consecutive frames of sequence. This
introduces a problem when micro-expression videos are
considered, created by the already noted limited motion
exhibited during the expressions. Therefore, herein we pro-
pose to calculate optical ﬂow between each sample frame
and the onset frame instead of consecutive frames, see
Fig. 3. To see the reasons behind this choice, consider Fig. 4
which shows optical ﬂow ﬁelds of consecutive frames start-
ing with the micro-expression onset frame. It can be readily
observed that the ﬁelds are rather similar up to the apex
frame, which can be attributed to the aforementioned brev-
ity of the expression, with a similar trend thereafter but in
the opposite direction. In contrast, our, temporally non-local
modiﬁed optical ﬂow – long-term optical ﬂow in a manner
of speaking – exhibits a much more structured pattern,
always being in the same direction, increasing in magnitude
up to the apex frame and declining in magnitude thereafter.
Fig. 3. Different computing mechanism between short- and long-term
optical ﬂow.
Fig. 4. Illustration of optic ﬂow computed between the onset and the apex
frame, corresponding to the motion effected by the activation unit Brow
Lowerer (AU4). Compare with the one computed between consecutive
frames.1976 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:23 UTC from IEEE Xplore.  Restrictions apply. This results in much more stable and discriminative fea-
tures associated with each micro-expression.
3.2 Spatial Feature Extraction
The key idea underlying the proposed method lies in the
extraction of long-range spatial relations from each frame
using a transformer encoder, with images as before being
treated as sequences of constituent patches. More speciﬁ-
cally, input frames are ﬁrst represented as vector sequences
with local spatial features of each image patch. The resulting
sequences are then fed into the transformer encoder for
long-term spatial feature extraction.
3.2.1 Input Embedding and Short-Range Spatial
Relation Learning
The standard transformer receives a 1D sequence as input.
To handle 2D images, we represent each image as a
sequence of rasterized 2D patches. Herein we do not use
appearance images, that is the original video sequence
frames, as input but rather the corresponding optical ﬂow
ﬁelds. An input embedding block is proposed as a means of
representing input images as vector sequences for input to
the transformer encoder.
The general input embedding mechanism considers the
image X2RH/C2W/C2Cas a sequence of non-overlapping P/C2
Ppixel patches, where H,W, and Care respectively the
height, the width, and the channel count of the input. Differ-
ent from the “separate and ﬂat” linear patch embedding
proposed by Dosovitskiy et al. [24], we ﬁrst extract local
spatial features in patch regions with a patch-wise fully con-
nected layer. Patches of image Xare represented as Xp2
RN/C2ðP2;CÞ. As shown in Fig. 5, we extract the short-range
spatial features from image Xto feature map X2RH
P/C2W
P/C2D,
ﬂatten and transpose them to ND -dimensional vectors,
where N¼HW
P2the resulting number of patches in each
image. D-dimensional vectors are passed through all trans-
former encoder layers. The speciﬁc values of parameters
used in our experiments are stated in Section 4.
After that, a learnable D-dimensional vector is concatenated
with the sequence, as the class token ( Z0½0/C138¼xclass), whose
state as the output of the transformer encoder ( ZLT½0/C138). The
effective input sequence length for the transformer encoder is
thusNþ1. Then a position embedding is added to each vectorin the sequence. The whole input embedding procedure can be
described as follows:
Z0¼½Xclass;X1
pE;X2
pE;... ;XN
pE/C138þEpos;
E2RðP2;CÞ/C2D;Epos2RðNþ1Þ/C2D; (1)
where Z02RðN/C2DÞis the input of the transformer encoder.
3.2.2 Long-Range Spatial Relation Learning by
Transformer Encoder
After short-range spatial relation are extracted from the
input long-term optical ﬂow ﬁelds of each frame and embed-
ded as vectors, they are passed to a transformer encoder for
further long-range spatial feature extraction. Our encoder
contains LTtransformer layers; herein we use LT¼12,
adopting this value from the ViT-Base model of Dosovitskiy
et al. [24] (the pre-trained encoder we use in experiments).
Each layer involves two blocks, a Multi-head Self-attention
Mechanism (MSM) and a Position-Wise fully connected
Feed-Forward network (PWFF), as shown in Fig. 6. Layer
Normalisation (LN) is applied before each block and residual
connections after each block [54], [55]. The output of the
transformer layer can be written as follows:
Z0
l¼MSM ðLNðZl/C01ÞÞ þZl/C01;l¼1. . .LT; (2)
Zl¼PWFF ðLNðZ0
lÞÞ þZ0
l;l¼1. . .LT; (3)
where Zlis the output of layer l. The PWFF block contains
two layers with the Gaussian Error Linear Unit (GELU)
non-linear activation function. The feature embedding
dimension thereby ﬁrst increases from Dto4Dand then
reduces back to D, which equals 768 in our experiments.
Multi-head attention allows the model to focus simulta-
neously on information content from different parts of the
sequences, so both long-range and short-range spatial rela-
tions can be learnt. An attention function is mapping a
query and a set of key-value pairs to the output, a weighted
sum of the values. The weights are computed using a com-
patibility function of the queries with the corresponding
keys, and they are all vectors. The self-attention function is
computed on a set of queries simultaneously. The queries,
keys and values can be grouped together and represented
as matrices Q,KandV, so the computation of the matrix of
Fig. 5. Long-term optical ﬂow ﬁelds are as inputs of the Input Embedding blocks. After short-range spatial feature extraction, patch and position
embedding, the resulting sequence of vectors are fed to standard transformer encoder layers.ZHANG ET AL.: SHORT AND LONG RANGE RELATION BASED SPATIO-TEMPORAL TRANSFORMER FOR MICRO-EXPRESSION... 1977
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:23 UTC from IEEE Xplore.  Restrictions apply. outputs can be written as:
Q¼Zl/C01WQ; (4)
K¼Zl/C01WK; (5)
V¼Zl/C01WV; (6)
SAðZlÞ¼softmaxQKT
ﬃﬃﬃﬃ
Dp/C18/C19
V; (7)
where WQ;WK;WV2RD/C2Dmare learnable matrices and SA
is the self-attention module. MSM can be seen as a type of
self-attention with Mheads in parallel operation and a pro-
jection of their concatenated outputs:
MSM ðZlÞ¼Concat ðSAhðZlÞ;8h21::M½/C138 fg Þ WO; (8)
where WO2RM/C1Dm/C2Dis a re-projection matrix. Dmis typi-
cally set toD
M, so as to keep the number of parameters con-
stant with changing M.
3.3 Temporal Aggregation
After extracting both local and global spatial features associ-
ated with each frame using a transformer encoder, we intro-
duce an aggregation block to extract temporal features
before performing the ultimate classiﬁcation. The aggrega-
tion function ensures that our transformer model can be
trained and applied to the spatial feature sets of each frame,
subsequently processing the temporal relations between
frames in each sample. Since facial movement during
micro-expressions is almost imperceptible, all frames from
a single video sample are rather similar one to another. Nev-
ertheless, it is still possible to identify reliably a number of
salient frames, such as the apex frame, that play a particu-
larly important role in the analysis of a micro-expression.
Therefore, we propose an LSTM architecture for temporal
aggregation.Long Short-Term Memory (LSTM) [56] is a type of recur-
rent neural network with feedback connections, which over-
comes two well-known problems associated with RNNs: the
vanishing gradient problem, and the sensitivity to the varia-
tion of the temporal gap length between salient events in a
processed sequence. The elements of the input are the sets
of outputs from the transformer encoder for each frame.
The inputs are not concatenated, and the input sequence
length is thus dependent on the number of frames in each
ME video sample.
We used three LSTM layers in the aggregation block. The
computation details of each layer are:
t¼1. . .F; l¼LTþ1. . .LA;
ft¼sðWf/C1½Zt/C01
l;Zt
l/C01/C138þbfÞ; (9)
it¼sðWi/C1½Zt/C01
l;Zt
l/C01/C138þbiÞ; (10)
ot¼sðWo/C1½Zt/C01
l;Zt
l/C01/C138þboÞ; (11)
C0
t¼tanh ðWC/C1½Zt/C01
l;Zt
l/C01/C138þbCÞ; (12)
Ct¼ft/C2Ct/C01þit/C2C0
t; (13)
Zt
l¼ot/C2tanh ðCtÞ; (14)
where Fis the number of chosen frames in each video sam-
ple,LAis the total number of layers in both the transformer
encoder and the LSTM aggregator. Zt
ldenotes the outputs
of the layer lafter tframes have been processed. After all
frames are processed in this manner, the result is a single
feature set describing the entire micro-expression video
sample. Finally, these features are fed into an MLP which is
used for the ultimate MER classiﬁcation. The details of how
previous output join the latter training are presented in
Fig. 7. We also design a comparative experiment to demon-
strate the effectiveness of the LSTM aggregator, the details
of which are described in the Section 4.3.2.
3.4 Network Optimization
Following the aggregation block, our network contains two
fully connected layers which facilitate the ﬁnal classiﬁcation
achieved using the SoftMax activation function. Cross
Entropy loss is used as the objective function for training:
L¼1
NX
iLi¼/C01
NX
iXC
c¼1yiclogðpicÞ; (15)
where Nis the number of the ME video samples and Cthe
number of emotion classes. The value of yicis 1 when the true
class of sample iis equal to cand 0 otherwise. Similarly, picis
the predicted probability that sample ibelongs to class c.
When using gradient descent to optimize the objective
function during network training, as the parameter set gets
closer to its optimum, the learning rate should be reduced.
Herein we achieve this using cosine annealing [57], i.e., using
the the cosine function to modulate the learning rate which
initially decreases slowly, and then rather rapidly before sta-
bilizing again. This learning rate adjustment is particularly
important in the context of the problem at hand, considering
that the number of available micro-expression video samples
is not large even in the largest corpora, readily learning to
overﬁtting if due care is not taken.
Fig. 6. Detailed structure of a transformer encoder layer . The output of
frame tprocessed by spatial encoder is Zt
LT.1978 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:23 UTC from IEEE Xplore.  Restrictions apply. 4E XPERIMENTS AND EVALUATION
In this section we describe the empirical experiments used
to evaluate the proposed method. We begin with a descrip-
tion of the data sets used, follow up with details on the data
pre-processing performed, relevant implementation details,
and evaluation metrics, and conclude with a report of the
results and a discussion of the ﬁndings.
4.1 Databases
Following the best practices in the ﬁeld, for our evaluation
we adopt the use of three large data sets, namely the Spon-
taneous Micro-Expression Corpus (SMIC) [34], the Chinese
Academy of Sciences Micro-Expression II data set (CASME
II) [35], and the Spontaneous Actions and Micro-Movement
database (SAMM) [36], thus ensuring sufﬁcient diversity of
data, evaluation scale, and ready and fair comparison with
other methods in the literature. All video samples in these
databases capture spontaneously exhibited, rather than
acted micro-expressions (see Zhang and Arandjelovi /C19c [5]
for discussion), which is important for establishing the real-
world applicability of ﬁndings.
4.1.1 SMIC
The Spontaneous Micro-Expression Corpus (SMIC) is the
earliest published spontaneous micro-expression database
[34]. It comprises three distinct parts captured by cameras
of different types, namely a conventional visual camera
(VIS), a near-infrared camera (NIR) and a high-speed cam-
era (HS). These subsets are designed to study micro-expres-
sion analysis tasks in various application scenarios. To
achieve uniformity with the other two corpora, namely
CASME-II and SAMM which are described next, which
only contain high-speed camera videos, it is the HS subset
from SMIC that we make use of herein. The SMIC-HS con-
tains 164 video sequences (samples) from 16 subjects of 3
ethnicities. Using two human labellers, these videos are cat-
egorized as corresponding to either negative (70), positive
(51), or surprised (43) expression, and both raw and
cropped frames are provided.
4.1.2 CASME II
The Chinese Academy of Sciences Micro-Expression II
(CASME II) data set contains 247 micro-expression video
samples from 26 Chinese participants. The full videos havethe resolution of 640/C2480pixels. Cropped facial frames in
280/C2340pixel resolution (higher than both CASME and
SMIC-HS), extracted using the same face registration and
alignment method as for SMIC, are also provided. The
micro-expression samples in CASME II are labelled by 2
coders to 5 classes, namely Happiness (33), Disgust (60),
Surprise (25), Repression (27), and Others (102).
4.1.3 SAMM
The Spontaneous Actions and Micro-Movement (SAMM)
database is the newest MER corpus. The 159 micro-expres-
sion short videos in the corpus were collected using 32 par-
ticipants of 13 ethnicities, with an even gender distribution
(16 male and 16 female), at 200 fps and the resolution of
2040 /C21088 pixels, with the face region size being approxi-
mately 400/C2400pixels. The samples are assigned to one of
8 emotion classes, namely Anger (57), Happiness (26), Other
(26), Surprise (15), Contempt (12), Disgust (9), Fear (8) and
Sadness (6).
4.2 Data Pre-Processing
4.2.1 Face Cropping
As noted in the previous section, cropped face images are
explicitly provided in both SMIC-HS and CASME II data
sets, with the same registration method used in both; no
cropped faces are provided as part of SAMM. In order to
maintain data consistency across different databases, in our
experiments we employ a different face extraction approach.
In particular, we utilize the Ensemble of Regression Trees
(ERT) [58] algorithm implemented in DLib [59] to localize
salient facial loci (68 of them) in a uniform manner regardless
of which data set a speciﬁc video sample came from.
In the case of SMIC-HS and CASME II videos, the origi-
nal authors’ face extraction process consists of facial land-
marks detection in the ﬁrst frame of a micro-expression clip
and then the detected face being registered to the model
face using a LWM transformation. Motivated by the short
duration of MEs, the faces in all remaining frames of the
video sample are registered using the same matrix.
However, in this paper we employ an alternative strat-
egy. The primary reason lies in the need for sufﬁcient and
representative data diversity, which is particularly impor-
tant in deep learning. In particular, the original face extrac-
tion method just described, often results in the close
resemblance of samples which increases the risk of model
Fig. 7. The repeating module in an LSTM aggregator layer .ZHANG ET AL.: SHORT AND LONG RANGE RELATION BASED SPATIO-TEMPORAL TRANSFORMER FOR MICRO-EXPRESSION... 1979
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:23 UTC from IEEE Xplore.  Restrictions apply. overﬁtting. Therefore, herein we instead simply use a non-
reﬂective 2D euclidean transformation, i.e., one comprising
only rotation and translation. By doing so, at the same time
we ensure the correct alignment of salient facial points and
maintain information containing facial contour variability.
Furthermore, unlike the authors of SMIC-HS and
CASME II, we do not perform facial landmark detection in
the ﬁrst frame of a micro-expression sample, but rather in
the apex, thereby increasing the registration accuracy of the
most informative parts of the video. As shown in Fig. 8,
points 27–30 can be used to determine the centre line of the
nose that can be considered as the vertical symmetry line of
the entire face area. Point 30 is set as the centre point, and
the square size s(in pixels) is computed by adding the verti-
cal distance from the centre point of the eyebrows (19) to the
lowest point of the chin (8), yapex ½8/C138/C0yapex ½19/C138, to the height of
chin, yapex ½8/C138/C0yapex ½57/C138, so that nearly the entire face is
included in the cropped image:
s¼ðyapex ½8/C138/C0yapex ½19/C138Þþðyapex ½8/C138/C0yapex ½57/C138Þ: (16)
4.2.2 Temporal Interpolation
Considering the short duration of micro-expressions, even
when samples are acquired using high-speed cameras, in
some instances only a small number (cc. 10) of frames is avail-
able. In an attempt to extract accurate temporal information,
we also apply frame interpolation from raw videos, effec-
tively synthetically augmenting data. In previous work, the
Temporal Interpolation Model (TIM) relies on a path graph
to characterize the structure of a sequence of frames, popu-
larly used in several handcrafted feature based methods [9],
[13], [60], whereas Liu et al. [10] use simple linear interpola-
tion. Herein we propose a novel approach to interpolation so
that its result is smoother in terms of optical ﬂow, it being the
nexus of our entire MER methodology. Most existing optical
ﬂow based methods produce artifacts on motion boundaries
by estimating bidirectional optical ﬂows, scaling and revers-
ing them to approximate intermediate ﬂows. We adopt the
Real-time Intermediate Flow Estimation (RIFE) method [61],
which uses an end-to-end trainable neural network, IFNet,
which speedily and directly estimates the intermediate ﬂows.
Original RIFE interpolates one frame between two given
consecutive frames, so we apply it recursively to interpolate
multiple intermediate frames. Speciﬁcally, given any twoconsecutive input frames I0;I1, we apply RIFE once to get
intermediate frame ^I0:5att¼0:5. We then apply RIFE to
interpolate between I0and ^I0:5to get ^I0:25,a n ds oo n .I no u r
experiment, we prioritize interpolation in the temporal vicin-
ity of the apex frame. The interpolated queue can be
expressed as f^Ia/C00:5;^Iaþ0:5;^Ia/C01:5;^Iaþ1:5;...;^Ioþ0:5or^If/C00:5g,
where a,oandfare frame indices of the apex, onset, and off-
set frames respectively. Recall that the apex frames are speci-
ﬁed explicitly in CASME II and SAMM, and for SMIC-HS we
choose the middle frame of each sample video as the apex. If
the number of interpolation frames is lower than the reference
count (the average number of frames in this period across the
database), we use the same method on the updated frame
sequence iteratively to generate further intermediate frames.
4.3 Experimental Settings
4.3.1 Implementation Details
In the spatial feature extraction procedure, we employed base
ViT blocks, with 12 Encoder layers, hidden size of 768, MLP
size of 3072, and 12 heads. For initialization, we use the ofﬁcial
ViT-B/16 model [24] pre-trained on ImageNet [62]. We resize
our input images to 384/C2384pixels and split each image into
patches with 16/C216pixels, so that the number of patches is
24/C224. 768-dimensional vectors are passed though all trans-
former encoder layers. For temporal aggregation, we select 11
frames (apex, and ﬁve preceding and succeeding it) per sam-
ple as inputs for the mean aggregator and LSTM aggregator.
We have tried other options with different number of frame,
but it didn’t work any better. We only use long-term optical
ﬂow in experiments, as motivated by the arguments dis-
cussed in Section 3.1. For learning parameters, the initial
learning rate and weight decay are set to be 1e-3 and 1e-4,
respectively. The momentum for Stochastic Gradient Decent
(SGD) is set to 0.9, with the batch size 4 for all experiments.
All the experiments were conducted with PyTorch.
4.3.2 Mean Versus LSTM Aggregator
We compare our LSTM aggregator with an alternative
which uses the simple mean operator for temporal aggrega-
tion. After each frame is processed by spatial encoder, the
corresponding output is used in the computation by the
mean aggregation layer (layer LTþ1):
Zt
LTþ1¼t/C01
tZt/C01
LTþ1þ1
tZt
LT;t¼1. . .F; (17)
In a manner similar to that described previously in the con-
text of the LSTM Aggregator, outputs of each frame from
our transformer encoder are taken as inputs to the temporal
feature extraction module. Compared to the mean operator,
LSTM has the advantage of larger expressive capability,
resulting in different extracted relationships between differ-
ent frames. Within the speciﬁc context of our work, this
means that its ability to distinguish between emotions is
also different, with LSTM expected to perform better.
4.3.3 Evaluation Metrics
Following previous work and the Micro-Expressions Grand
Challenges (MEGCs), we conducted experiments on SMIC-
HS, CASME II, and SAMM, evaluating the classiﬁcation
Fig. 8. The 68 facial landmarks used by our method, shown for the onset
(green) and the apex frame (red).1980 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:23 UTC from IEEE Xplore.  Restrictions apply. performance using the corresponding original emotion clas-
ses, as well as the composite corpus formed using all three
data sets and relabelled using three classes as proposed in
MEGC 2019 [63]. All results are reported using LOSO cross-
validation. Evaluation is repeated multiple times by holding
out test samples of each subject group while the remaining
samples are used for training. In this way we best mimic
real-world situations and in particular assess the robustness
to variability in ethnicity, gender, emotional sensitivity, etc.
4.3.3.1 Sole Database Evaluation (SDE). In the ﬁrst
part of our empirical evaluation, experiments are conducted
on three databases individually, using the corresponding origi-
nal emotion labels, excepting the very rare (and thus underrep-
resented) classes in CASME II and SAMM. SMIC-HS uses 3
class labels whereas the other two sets both use 5. We use accu-
racyandmacro F1-score to assess the recognition performance.
4.3.3.2 Composite Database Evaluation (CDE). In
the second part of our empirical evaluation, experiments
are conducted on the composite database with 3 emotion
classes (negative, positive, and surprise). The composite
database, that is the database obtained by merging SMIC,
CASME II, and SAMM contains the total of 68 subjects, 16
from SMIC, 24 from CASME II and 28 from SAMM. LOSO
cross-validation is applied on each database separately and
together on the composite database. Unweighted F1-score
(UF1) , also known as the macro F1-score and Unweighted
Average Recall (UAR) are used to assess performance:
UF1¼macro F 1/C0score; (18)
UAR ¼PC
c¼1PS
i¼1TPi;c
Nc
C; (19)where Ncis the total number of samples of class cacross all
subjects.
4.4 Results and Discussion
We compare the performance of the proposed approach
with baseline handcrafted feature extraction methods and
the most prominent recent deep learning based methods on
the widely used micro-expression databases, SMIC-HS,
CASME II, and SAMM, described in the previous section,
both in the SDE and the CDE settings. To ensure uniformity
and fairness of the comparison, the SDE results for all meth-
ods were obtained in identical conditions, i.e., for the identi-
cal number of samples, the number of labels (classes), and
using the same cross-validation approach. The details of the
performance of our SLSTT on different emotion categories
are shown in Fig. 9.
As can be readily seen in Table 1 which presents a com-
prehensive overview of our experimental results in the SDE
setting, the method proposed in the present paper performs
best (n.b. shown in bold) in all but one testing scenario, in
which it is second best (n.b. second best performance is
denoted by square brackets), trailing marginally behind the
method introduced by Sun et al. [69]. What is more, in most
cases our method outperforms rivals by a signiﬁcant margin.
Moving next to the results of our experiments in the CDE
setting, these are summarized in Table 2. It can be readily
seen that our method’s performance is again shown to be
excellent. In particular, in most cases our method again
comes out either at the top or second best (as before the for-
mer being shown in bold and the latter denoted by square
brackets enclosure). The only existing method in the litera-
ture which remains competitive against ours is that of Lei
et al. [20]. To elaborate in further detail, our approach
achieved the best results both in terms of UF1 and UAR on
Fig. 9. Confusion matrices corresponding to each of our experiments. Only one is shown for SMIC-HS because the SDE and the CDE are identical
when this database is used alone.ZHANG ET AL.: SHORT AND LONG RANGE RELATION BASED SPATIO-TEMPORAL TRANSFORMER FOR MICRO-EXPRESSION... 1981
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:23 UTC from IEEE Xplore.  Restrictions apply. CASME II, and on UF1 on the full composite database, and
second best on UAR on the composite database and on UF1
on SMIC-HS. The performance of all methods on CASME II
is consistently higher than when applied on other data sets,
which suggests that the challenge of MER is increased with
ethnic diversity of participants – this should be born in
mind in future research and any comparative analysis. It is
insightful to observe that in contrast with the results in the
SDE setting already discussed (see Table 1), our method
does not come out as dominant in the context of CDE. This
suggests an important conclusion, namely that our method
is particularly capable of nuanced learning over ﬁner
grained classes and that its superiority is less able to comethrough in a simpler setting when only 3 emotional classes
as used.
Taking into account the results from both the sole and the
composite database experiments, it is useful to observe that
when only short-range patterns are utilized, convolutional
neural network approaches do not outperform methods
based on handcrafted feature. It is the inclusion of long-
range spatial learning that is key, as shown by the marked
improvement in performance of the corresponding meth-
ods. Yet, the proposed method’s exceeds even their perfor-
mance, owing to its use of a multi-head self-attention
mechanism, thus demonstrating its importance in MER.
The superiority of our short- and long-range relation basedTABLE 1
SDE Results Comparison with LOSO on SMIC-HS (3 Classes), CASME II (5 Classes) and SAMM (5 Classes)
SMIC-HS CASME II SAMM
Acc(%) F1 Acc(%) F1 Acc(%) F1
Handcrafted
LBP-TOP* 53.66 0.538 46.46 0.424 – –
LBP-SIP* 44.51 0.449 46.56 0.448 – –
STLBP-IP [66] (2015) 57.93 – 59.51 – – –
STCLQP [64] (2015) 64.02 0.638 58.39 0.584 – –
Hierarchical STLBP-IP [67] (2018) 60.37 0.613 – – – –
HIGO+Mag [9] (2018) 68.29 – 67.21 – – –
Deep Learning
AlexNet** 59.76 0.601 62.96 0.668 52.94 0.426
DSSN [65] (2019) 63.41 0.646 70.78 0.730 57.35 0.464
AU-GACN [18] (2020) – – 49.20 0.273 48.90 0.310
MER-GCN [16] (2020) – – 42.71 – – –
Micro-attention [68] (2020) 49.40 0.496 65.90 0.539 48.50 0.402
Dynamic [69] (2020) 76.06 0.710 72.61 0.670 – –
GEME [70] (2021) 64.63 0.616 [75.20] [0.735] 55.88 0.454
SLSTT-Mean (Ours) 73.17 [0.719] 73.79 0.723 [66.42] [0.547]
SLSTT-LSTM (Ours) [75.00] 0.740 75.81 0.753 72.39 0.640
Best performances are shown in bold, second best by square brackets enclosure. (* Reported by Huang et al. [64], ** Reported by Khor et al. [65]).
TABLE 2
CDE Results Comparison with LOSO on SMIC-HS, CASME II, SAMM and Composite Database (3 Classes)
Composite SMIC-HS CASME II SAMM
UF1 UAR UF1 UAR UF1 UAR UF1 UAR
Handcrafted
LBP-TOP* 0.588 0.579 0.200 0.528 0.703 0.743 0.395 0.410
Bi-WOOF* 0.630 0.623 0.573 0.583 0.781 0.803 0.521 0.514
Deep learning
ResNet18** 0.589 0.563 0.461 0.433 0.625 0.614 0.476 0.436
DenseNet121** 0.425 0.341 0.460 0.333 0.291 0.352 0.565 0.337
Inception V3** 0.516 0.504 0.411 0.401 0.589 0.562 0.414 0.404
WideResNet28-2** 0.505 0.513 0.410 0.401 0.559 0.569 0.410 0.404
OFF-ApexNet* [32] (2019) 0.720 0.710 0.682 0.670 0.876 0.868 0.541 0.539
CapsuleNet [72] (2019) 0.652 0.651 0.582 0.588 0.707 0.701 0.621 0.599
Dual-Inception [73] (2019) 0.732 0.728 0.665 0.673 0.862 0.856 0.587 0.566
STSTNet [41] (2019) 0.735 0.761 0.680 0.701 0.838 0.869 0.659 0.681
EMR [42] (2019) 0.789 0.782 0.746 0.753 0.829 0.821 0.775 [0.715]
ATNet [40] (2019) 0.631 0.613 0.553 0.543 0.798 0.775 0.496 0.482
RCN [71] (2020) 0.705 0.716 0.598 0.599 0.809 0.856 0.677 0.698
AUGCN+AUFsuion [20] (2021) [0.791] 0.793 0.719 [0.722] [0.880] [0.871] [0.775] 0.789
SLSTT-Mean (Ours) 0.788 0.767 0.719 0.699 0.844 0.830 0.625 0.566
SLSTT-LSTM (Ours) 0.816 [0.790] [0.740] 0.720 0.901 0.885 0.715 0.643
Best performances are shown in bold, second best by square brackets enclosure. (*Reported by See et al. [63], **Reported by Xia et al. [71]).1982 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:23 UTC from IEEE Xplore.  Restrictions apply. spatiotemporal transformer is further corroborated by the
results shown in the latest two rows in both Tables 1 and 2
which summarize our comparison of the proposed LSTM
aggregator with the simpler mean operator aggregator.
In CASME II, distinguishing whether a micro-expression
is Disgust or Others is inherently difﬁcult because the data-
base contains multiple inconsistently labelled samples with
only AU4 activated – some of them are labelled as Others,
some as Disgust. It is also worth noting that in SAMM,
some AU labels (‘AU12 or 14’) for the Contempt class were
not manually veriﬁed, which also causes confusion with
the Happiness class (mostly with AU12 labelled). In part,
these labelling issues emerge from the fact that the mapping
between facial action unit activations and emotions (as
understood by psychologists) is not a bijection. It is also the
case that imperfect information is made use of because only
visual data is used. Hence, it should be understood that the
theoretical highest accuracy of automated micro-expression
recognition on the MER corpora currently used for research
purposes is not 100%. The micro-expression databases con-
taining multi-modal signals [74], [75], which have begun
emerging recently, seem promising in overcoming some of
the limitations of the existing corpora, and we intend to
make use of them in our future work.
5C ONCLUSION
In this paper, we proposed a novel transformer based spa-
tio-temporal deep learning framework for micro-expression
recognition, which is the ﬁrst deep learning work in the
ﬁeld entirely void of convolutional neural network use. In
our framework both short- and long-term relations between
pixels in spatial and temporal directions of the sample vid-
eos can be learned. We use transformer encoder layers with
multi-head self-attention mechanism to learn spatial rela-
tions from visualized long-term optical ﬂow frames and
design a temporal aggregation block for temporal relations.
Extensive experimental results using three large MER data-
bases, both in the context of sole database evaluation and
composite database evaluation settings and the Leave One
Subject Out cross validation protocol, consistently demon-
strate that our approach is effective and outperforms the
current state of the art. These ﬁndings strongly motivate fur-
ther research on the use of transformer based architectures
rather than convolutional neural networks in micro-expres-
sion analysis, and we hope that our theoretical contributions
will help direct such future efforts.
REFERENCES
[1] P. Ekman and W. V. Friesen, “Constants across cultures in the face
and emotion,” J. Pers. Social Psychol. , vol. 17, 1971, Art. no. 124.
[2] L. Zhang, O. Arandjelovi /C19c, S. Dewar, A. Astell, G. Doherty, and M.
Ellis, “Quantiﬁcation of advanced dementia patients’ engagement
in therapeutic sessions: An automatic video based approach using
computer vision and machine learning,” in Proc. Int. Conf. IEEE
Eng. Med. Biol. Soc. , 2020, pp. 5785–5788.
[3] L. A. Gottschalk, A. H. Auerbach, E. A. Haggard, and K. S. Isaacs,
“Micromomentary facial expressions as indicators of ego mecha-
nisms in psychotherapy,” in Proc. Methods of Research in Psycho-
therapy , Berlin, Germany: Springer, 1966, pp. 154–165.
[4] P. Ekman and W. V. Friesen, “Nonverbal Leakage and Clues to
Deception,” Psychiatry , vol. 32, no. 1, pp. 88–106, 1969.[5] L. Zhang and O. Arandjelovi /C19c, “Review of Automatic Micro-
expression Recognition in the Past Decade,” Mach. Learn. Knowl.
Extraction , vol. 3, no. 2, pp. 414–434, 2021.
[6] T. Pﬁster, X. Li, G. Zhao, and M. Pietik €ainen, “Recognising sponta-
neous facial micro-expressions,” in Proc. IEEE Int. Conf. Comput.
Vis., 2011, pp. 1449–1456.
[7] G. Zhao and M. Pietik €ainen, “Dynamic texture recognition using
local binary patterns with an application to facial expressions,”
IEEE Trans. Pattern Anal. Mach. Intell. , vol. 29, no. 6, pp. 915–928,
Jun. 2007.
[8] S. Polikovsky, Y. Kameda, and Y. Ohta, “Facial micro-expressions
recognition using high speed camera and 3D-Gradient descriptor,”
inProc. IET Seminar Dig. , 2009, pp. 16–16.
[9] X. Li et al., “Towards reading hidden emotions: A comparative
study of spontaneous micro-expression spotting and recognition
methods,” IEEE Trans. Affect. Comput. , vol. 9, no. 4, pp. 563–577,
Oct.–Dec. 2018.
[10] Y. J. Liu, J. K. Zhang, W. J. Yan, S. J. Wang, G. Zhao, and X. Fu, “A
main directional mean optical ﬂow feature for spontaneous
micro-expression recognition,” IEEE Trans. Affect. Comput. , vol. 7,
no. 4, pp. 299–310, Oct.–Dec. 2016.
[11] D. Patel, X. Hong, and G. Zhao, “Selective deep features for micro-
expression recognition,” in Proc. 23rd Int. Conf. Pattern Recognit. ,
2016, pp. 2258–2263.
[12] H. Q. Khor, J. See, R. C. W. Phan, and W. Lin, “Enriched long-term
recurrent convolutional network for facial micro-expression rec-
ognition,” in Proc. 13th IEEE Int. Conf. Autom. Face Gesture Recog-
nit., 2018, pp. 667–674.
[13] J. Li, Y. Wang, J. See, and W. Liu, “Micro-expression recognition
based on 3D ﬂow convolutional neural network,” Pattern Anal.
Appl. , vol. 22, no. 4, pp. 1331–1339, 2019.
[14] Z. Xia, X. Feng, X. Hong, and G. Zhao, “Spontaneous facial micro-
expression recognition via deep convolutional network,” in Proc.
IEEE 8th Int. Conf. Image Process. Theory Tools Appl. , 2019, pp. 1–6.
[15] Z. Xia, X. Hong, X. Gao, X. Feng, and G. Zhao, “Spatiotemporal
recurrent convolutional networks for recognizing spontaneous
micro-expressions,” IEEE Trans. Multimedia , vol. 22, no. 3,
pp. 626–640, Mar. 2020.
[16] L. Lo, H. X. Xie, H. H. Shuai, and W. H. Cheng, “MER-GCN:
Micro-expression recognition based on relation modeling with
graph convolutional networks,” in Proc. 3rd Int. Conf. Multimedia
Inf. Process. Retrieval , 2020, pp. 79–84.
[17] A. M. Buhari, C.-P. Ooi, V. M. Baskaran, R. C. W. Phan, K.
Wong, and W.-H. Tan, “FACS-based graph features for real-
time micro-expression recognition,” J. Imag. ,v o l .6 ,n o .1 2 ,
2020, Art. no. 130.
[18] H.-X. Xie, L. Lo, H.-H. Shuai, and W.-H. Cheng, “Au-assisted graph
attention convolutional network for micro-expression recognition,”
inProc. 28th ACM Int. Conf. Multimedia , 2020, pp. 2871–2880.
[19] A. J. R. Kumar and B. Bhanu, “Micro-expression classiﬁcation
based on landmark relations with graph attention convolutional
network,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit.
Workshops , 2021, pp. 1511–1520.
[20] L. Lei, T. Chen, S. Li, and J. Li, “Micro-expression recognition
based on facial graph representation learning and facial action
unit fusion,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit.
Workshops , 2021, pp. 1571–1580.
[21] S. Khan, M. Naseer, M. Hayat, S. W. Zamir, F. S. Khan, and M.
Shah, “Transformers in vision: A survey,” 2021, arXiv:2101.01169 .
[22] L. Ye, M. Rochan, Z. Liu, and Y. Wang, “Cross-modal self-attention
network for referring image segmentation,” in Proc. IEEE Comput.
Soc. Conf. Comput. Vis. Pattern Recognit. , 2019, pp. 10 494–10 503.
[23] F. Yang, H. Yang, J. Fu, H. Lu, and B. Guo, “Learning texture trans-
former network for image super-resolution,” in Proc. IEEE/CVF
Conf. Comput. Vis. Pattern Recognit. , 2020, pp. 5791–5800.
[24] A. Dosovitskiy et al., “An image is worth 16x16 words: Trans-
formers for image recognition at scale,” 2020, arXiv:2010.11929 .
[25] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and H.
J/C19egou, “Training data-efﬁcient image transformers & distillation
through attention,” 2020, arXiv:2012.12877 .
[26] C. Sun, A. Myers, C. Vondrick, K. Murphy, and C. Schmid,
“VideoBERT: A joint model for video and language representa-
tion learning,” in Proc. IEEE Int. Conf. Comput. Vis. , 2019,
pp. 7464–7473.
[27] R. Girdhar, J. JoaoCarreira, C. Doersch, and A. Zisserman, “Video
action transformer network,” in Proc. IEEE Comput. Soc. Conf.
Comput. Vis. Pattern Recognit. , 2019, pp. 244–253.ZHANG ET AL.: SHORT AND LONG RANGE RELATION BASED SPATIO-TEMPORAL TRANSFORMER FOR MICRO-EXPRESSION... 1983
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:23 UTC from IEEE Xplore.  Restrictions apply. [28] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S.
Zagoruyko, “End-to-end object detection with transformers,” in
Proc. Eur. Conf. Comput. Vis. , 2020, pp. 213–229.
[29] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai, “Deformable
DETR: Deformable transformers for end-to-end object detection,”
2020, arXiv:2010.04159 .
[30] X. Ben et al., “Video-based facial micro-expression analysis: A sur-
vey of datasets, features and algorithms,” IEEE Trans. Pattern
Anal. Mach. Intell. , vol. 44, no. 9, pp. 5826–5846, Sep. 2022.
[31] S. T. Liong, J. See, K. S. Wong, and R. C. Phan, “Less is more:
Micro-expression recognition from video using apex frame,” Sig-
nal Process. Image Commun. , vol. 62, pp. 82–92, 2018.
[32] Y. S. Gan, S. T. Liong, W. C. Yau, Y. C. Huang, and L. K. Tan,
“OFF-ApexNet on micro-expression recognition system,” Signal
Process. Image Commun. , vol. 74, pp. 129–139, 2019.
[33] Y. Li, X. Huang, and G. Zhao, “Joint local and global information
learning with single apex frame detection for micro-expression
recognition,” IEEE Trans. Image Process. , vol. 30, pp. 249–263, 2021.
[34] X. Li, T. Pﬁster, X. Huang, G. Zhao, and M. Pietikainen, “A spon-
taneous micro-expression database: Inducement, collection and
baseline,” in Proc. 10th IEEE Int. Conf. Workshops Autom. Face Ges-
ture Recognit. , 2013, pp. 1–6.
[35] W. J. Yan et al., “CASME II: An improved spontaneous micro-
expression database and the baseline evaluation,” PLoS One ,
vol. 9, 2014, Art. no. e86041.
[36] A. K. Davison, C. Lansley, N. Costen, K. Tan, and M. H. Yap,
“SAMM: A spontaneous micro-facial movement dataset,” IEEE
Trans. Affect. Comput. , vol. 9, no. 1, pp. 116–129, First Quarter,
2018.
[37] P. Ekman, W. V. Friesen, and J. C. Hager, Facial Action Coding Sys-
tem - Investigator’s Guide . Consulting Psychologists Press, Wash-
ington, DC, USA, 2002.
[38] L. Zhang, O. Arandjelovi /C19c, and X. Hong, “Facial action unit detec-
tion with local key facial sub-region based multi-label classiﬁca-
tion for micro-expression analysis,” in Proc. ACM Int. Conf.
Multimedia Workshops , 2021, pp. 11–18.
[39] S.-J. Wang, W.-J. Yan, G. Zhao, X. Fu, and C.-G. Zhou, “Micro-
expression recognition using robust principal component analysis
and local spatiotemporal directional features,” in Proc. Eur. Conf.
Comput. Vis. , vol. 8925, 2014, pp. 325–338.
[40] M. Peng, C. Wang, T. Bi, Y. Shi, X. Zhou, and T. Chen, “A novel apex-
time network for cross-dataset micro-expression recognition,” in
Proc. 8th Int. Conf. Affect. Comput. Intell. Interact. , 2019, pp. 1–6.
[41] S. T. Liong, Y. S. Gan, J. See, H. Q. Khor, and Y. C. Huang,
“Shallow triple stream three-dimensional CNN (STSTNet) for
micro-expression recognition,” in Proc. IEEE Int. Conf. Autom. Face
Gesture Recognit. , 2019, pp. 1–5.
[42] Y. Liu, H. Du, L. Zheng, and T. Gedeon, “A neural micro-expres-
sion recognizer,” in Proc. 14th IEEE Int. Conf. Autom. Face Gesture
Recognit. , 2019, pp. 1–4.
[43] S. P. T. Reddy, S. T. Karri, S. R. Dubey, and S. Mukherjee,
“Spontaneous facial micro-expression recognition using 3D spa-
tiotemporal convolutional neural networks,” in Proc. Int. Joint
Conf. Neural Netw. , 2019, pp. 1–8.
[44] D. H. Kim, W. J. Baddar, and Y. M. Ro, “Micro-expression recogni-
tion with expression-state constrained spatio-temporal feature
representations,” Proc. ACM Multimedia Conf. , 2016, pp. 382–386.
[45] H. Chen et al., “Pre-trained image processing transformer,” in Proc.
IEEE Conf. Comput. Vis. Pattern Recognit. , 2021, pp. 12 299–12 310.
[46] J. Devlin, M.-W. Chang, K. Lee, and K. T. Google, “BERT: Pre-
training of deep bidirectional transformers for language under-
standing,” 2018, arXiv:1810.04805 .
[47] A. Vaswani et al., “Attention is all you need,” in Proc. Adv. Neural
Inf. Process. Syst. , 2017, pp. 5998–6008.
[48] H. Chen, D. Jiang, and H. Sahli, “Transformer encoder with multi-
modal multi-head attention for continuous affect recognition,”
IEEE Trans. Multimedia , vol. 23, pp. 4171–4183, 2020.
[49] Y. Wang, W.-B. Jiang, R. Li, and B.-L. Lu, “Emotion transformer
fusion: Complementary representation properties of eeg and eye
movements on recognizing anger and surprise,” in Proc. IEEE Int.
Conf. Bioinf. Biomed. , 2021, pp. 1575–1578.
[50] G. M. Jacob and B. Stenger, “Facial action unit detection with
transformers,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recog-
nit., 2021, pp. 7680–7689.
[51] F. Ma, B. Sun, and S. Li, “Facial expression recognition with visual
transformers and attentional selective fusion,” IEEE Trans. Affec-
tive Comput. , to be published, doi: 10.1109/TAFFC.2021.3122146 .[52] D.-S. Pham, O. Arandjelovi /C19c ,a n dS .V e n k a t e s h ,“ D e t e c t i o no fd y n a m i c
background due to swaying move ments from motion features,” IEEE
Trans. Image Process. , vol. 24, no. 1, pp. 332–344, Jan. 2015.
[53] O. Arandjelovi /C19c, D.-S. Pham, and S. Venkatesh, “CCTV scene per-
spective distortion estimation from low-level motion features,”
IEEE Trans. Circuits Syst. Video Technol. , vol. 26, no. 5, pp. 939–949,
May 2016.
[54] Q. Wang et al., “Learning deep transformer models for machine
translation,” in Proc. 57th Annu. Meeting Assoc. Comput. Linguistics ,
2019, pp. 1810–1822.
[55] A. Baevski and M. Auli, “Adaptive input representations for neu-
ral language modeling,” in Proc. Int. Conf. Learn. Representations ,
2019.
[56] S. Hochreiter and J. Schmidhuber, “Long short-term memory,”
Neural Comput. , vol. 9, pp. 1735–1780, 2019.
[57] I. Loshchilov and F. Hutter, “SGDR: Stochastic gradient
descent with warm restarts,” in Proc. Int. Conf. Learn. Represen-
tations ,2 0 1 7 .
[58] V. Kazemi and J. Sullivan, “One millisecond face alignment with
an ensemble of regression trees,” in Proc. IEEE Comput. Soc. Conf.
Comput. Vis. Pattern Recognit. , 2014, pp. 1867–1874.
[59] D. E. King, “Dlib-ML: A machine learning toolkit,” J. Mach. Learn.
Res., vol. 10, pp. 1755–1758, 2009.
[60] S. J. Wang et al., “Micro-expression recognition with small sample
size by transferring long-term convolutional neural network,”
Neurocomputing , vol. 312, pp. 251–262, 2018.
[61] H. Zhewei, Z. Tianyuan, H. Wen, S. Boxin, and Z. Shuchang,
“RIFE: Real-time intermediate ﬂow estimation for video frame
interpolation,” 2020, arXiv:2011.06294 .
[62] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei,
“ImageNet: A large-scale hierarchical image database,”
inProc. IEEE Conf. Comput. Vis. Pattern Recognit. , 2009,
pp. 248–255.
[63] J. See, M. H. Yap, J. Li, X. Hong, and S.-J. Wang, “MEGC 2019–the
second facial micro-expressions grand challenge,” in Proc. 14th
IEEE Int. Conf. Autom. Face Gesture Recognit. , 2019, pp. 1–5.
[64] X. Huang, G. Zhao, X. Hong, W. Zheng, and M. Pietik €ainen,
“Spontaneous facial micro-expression analysis using spatiotem-
poral completed local quantized patterns,” Neurocomputing ,
vol. 175, pp. 564–578, 2016.
[65] H. Q. Khor, J. See, S. T. Liong, R. C. Phan, and W. Lin, “Dual-
stream shallow networks for facial micro-expression recognition,”
inProc. Int. Conf. Image Process. , 2019, pp. 36–40.
[66] X. Huang, S. J. Wang, G. Zhao, and M. Piteikainen, “Facial micro-
expression recognition using spatiotemporal local binary pattern
with integral projection,” in Proc. IEEE Int. Conf. Comput. Vis. ,
2015, pp. 1–9.
[67] Y. Zong, X. Huang, W. Zheng, Z. Cui, and G. Zhao, “Learning from
hierarchical spatiotemporal descriptors for micro-expression rec-
ognition,” IEEE Trans. Multimedia , vol. 20, no. 11, pp. 3160–3172,
Nov. 2018.
[68] C. Wang, M. Peng, T. Bi, and T. Chen, “Micro-attention for micro-
expression recognition,” Neurocomputing , vol. 410, pp. 354–362,
2020.
[69] B. Sun, S. Cao, D. Li, J. He, and L. Yu, “Dynamic micro-expression
recognition using knowledge distillation,” IEEE Trans. Affect.
Comput. , vol. 13, no. 2, pp. 1037–1043, Apr.–Jun. 2022.
[70] X. Nie, M. A. Takalkar, M. Duan, H. Zhang, and M. Xu, “Geme:
Dual-stream multi-task gender-based micro-expression recog-
nition,” Neurocomputing , vol. 427, pp. 13–28, 2021.
[71] Z. Xia, W. Peng, H. Q. Khor, X. Feng, and G. Zhao, “Revealing the
invisible with model and data shrinking for composite-database
micro-expression recognition,” IEEE Trans. Image Process. , vol. 29,
pp. 8590–8605, 2020.
[72] N. Van Quang, J. Chun, and T. Tokuyama, “CapsuleNet for
micro-expression recognition,” in Proc. IEEE 14th Int. Conf. Autom.
Face Gesture Recognit. , 2019, pp. 1–7.
[73] L. Zhou, Q. Mao, and L. Xue, “Dual-inception network for cross-
database micro-expression recognition,” in Proc. IEEE 14th Int.
Conf. Autom. Face Gesture Recognit. , 2019, pp. 1–5.
[74] J. Li et al., “CAS(ME)3: A third generation facial spontaneous
micro-expression database with depth information and high eco-
logical validity,” IEEE Trans. Pattern Anal. Mach. Intell. , to be pub-
lished, doi: 10.1109/TPAMI.2022.3174895 .
[75] X. Li et al., “4DME: A spontaneous 4D micro-expression dataset
with multimodalities,” IEEE Trans. Affect. Comput. , to be pub-
lished, doi: 10.1109/TAFFC.2022.3182342 .1984 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:23 UTC from IEEE Xplore.  Restrictions apply. Liangfei Zhang received the BEng degree from
the School of Computer Science and Technology,
Northwestern Polytechnical University, China, in
2018 and the MSc degree in artiﬁcial intelligence
from the School of Computer Science, University
of St Andrews in 2019. She is currently working
toward the PhD degree with the School of Com-
puter Science, University of St Andrews. Her
research interests are artiﬁcial intelligence, com-
puter vision and pattern recognition, and their
application such as affective computing, facial
behaviour analysis and expression recognition.
Xiaopeng Hong (Member, IEEE) received the
PhD degree in computer application and technol-
ogy from HIT , in 2010. He is a professor with the
Harbin Institute of Technology (HIT), PRC. He
had been a professor with Xi’an Jiaotong Univer-
sity, China, and an adjunct professor with the Uni-
versity of Oulu, Finland. He has authored more
than 50 articles in top-tier publications and con-
ferences such as IEEE Transactions on Pattern
Analysis and Machine Intelligence , CVPR, ICCV ,
and AAAI. He has served as an area chair/senior
program committee member for ACM MM, AAAI, IJCAI, and ICME, a
guest editor for peer-reviewed journals like Patter Recognition Letter
and Signal, Image and Video Processing , a co-organizer for six interna-
tional workshops in conjunction with IEEE CVPR, ACM MM, IEEE FG,
and a co-lecturer for two tutorials in conjunction with ACM MM21 and
IJCB21. His studies about subtle facial movement analysis have been
reported by International media like MIT Technology Review and been
awarded the 2020 IEEE Finland Section best student conference paper .
Ognjen Arandjelovi /C19c(Member , IEEE) received
the MEng degreetop of his class from the Depart-
ment of Engineering Science, the University of
Oxford. In 2007 he was awarded his PhD by the
University of Cambridge where he stayed thereaf-
ter as fellow of Trinity College Cambridge. Cur-
rently he is a reader with the School of Computer
Science at the University of St. Andrews in Scot-
land. Ognjen’s main research interests are com-
puter vision and pattern recognition, and their
application in various ﬁelds of science such as
bioinformatics, medicine, physiology, etc. He is a fellow of the Cambridge
Overseas Trust, winner of numerous awards, and area editor in chief of
Pattern Recognition and associate editor of Information, Cancers, and
Frontiers in AI.
Guoying Zhao (Fellow, IEEE) received the PhD
degree in computer science from the Chinese
Academy of Sciences. She is currently an Acad-
emy Professor with the Academy of Finland and
(tenured) full professor (from 2017) with the Cen-
ter for Machine Vision and Signal Analysis, Uni-
versity of Oulu, Finland. She joined University of
Oulu as a senior researcher . She was academy
fellow in 2011-2017 and an associate professor
from 2014 to 2017 with University of Oulu. She
has authored or coauthored more than 280
papers in journals and conferences. Her papers have currently more
than 18390 citations in Google Scholar (h-index 65). She has served as
associate editor for Pattern Recognition, IEEE Transactions on Circuits
and Systems for Video Technology ,IEEE Transaction on Multimedia
and Image and Vision Computing Journals . Her current research inter-
ests include image and video descriptors, facial-expression and micro-
expression recognition, emotional gesture analysis, affective computing,
and biometrics. Her research has been reported by Finnish national TV ,
newspapers and MIT Technology Review.
"For more information on this or any other computing topic,
please visit our Digital Library at www.computer .org/csdl.ZHANG ET AL.: SHORT AND LONG RANGE RELATION BASED SPATIO-TEMPORAL TRANSFORMER FOR MICRO-EXPRESSION... 1985
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:23 UTC from IEEE Xplore.  Restrictions apply. Self-Supervised Approach for Facial
Movement Based Optical Flow
Muhannad Alkaddour , Usman Tariq ,Member, IEEE , and Abhinav Dhall ,Member, IEEE
Abstract— Computing optical ﬂow is a fundamental problem in computer vision. However, deep learning-based optical ﬂow techniques
do not perform well for non-rigid movements such as those found in faces, primarily due to lack of the training data representing the ﬁne
facial motion. We hypothesize that learning optical ﬂow on face motion data will improve the quality of predicted ﬂow on faces. This work
aims to: (1) exploring self-supervised techniques to generate optical ﬂow ground truth for face images; (2) computing baseline results on
the effects of using face data to train Convolutional Neural Networks (CNN) for predicting optical ﬂow; and (3) using the learned optical
ﬂow in micro-expression recognition to demonstrate its effectiveness. We generate optical ﬂow ground truth using facial key-points in
the BP4D-Spontaneous dataset. This optical ﬂow is used to train the FlowNetS architecture to test its performance on the Extended
Cohn-Kanade dataset and a portion of the generated dataset. The performance of FlowNetS trained on face images surpassed that of other
optical ﬂow CNN architectures. Our optical ﬂow features are further compared with other methods using the STSTNet micro-expression
classiﬁer , and the results indicate that the optical ﬂow obtained using this work has promising applications in facial expression analysis.
Index Terms— Optical ﬂow, deep learning, micro-expression detection, facial expression analysis
Ç
1I NTRODUCTION
FACIAL expressions are generated due to non-rigid move-
ment in faces. From the perspective of automatic facial
expression recognition (FER), the motion information has
been well explored for the task of both micro and macro
expression analysis. Optical ﬂow is used to estimate the
motion of sets of pixels across images. This information on
faces can help characterize both micro and macro expres-
sions, which are useful in expression recognition. A major
motivation for using the motion information for FER is
based on what is known as the facial feedback hypothesis
[1], which, in summary, suggests that facial actions can both
encode current emotions as well as induce or amplify emo-
tions. An example of this would be that the furrowing of the
brow could increase anger [1]. It has also been demon-
strated that some facial muscle movements are linked to the
compound facial expression of negation [2]. Also, the rela-
tion between motion information extracted from the eyes
and mouth has been studied in its association with the facialexpressions of psychopaths [3]. Facial and head movements
are also important in social contexts, such as head motion
used to indicate particular social cues, or the famous twitch-
ing of the lip corners that may suggest lying [4].
Faces have a peculiar structure. Hence, in this work, we
focus on learning optical ﬂow specialized for faces, which we
will attempt to constrain the algorithm to learn only lifelike
expressions on faces. In doing so, we explore how well a deep
network can perform in this tas k. We demonstrate that the pro-
posed architecture will work w ell for faces compared and com-
pare it to traditional optical ﬂow algorithms. The results can
serve as a precursor to designing motion-based features for
supervised and unsupervised learning in tasks such as FER
and action unit (AU) recognition. of facial expressions by draw-
ing on existing research linking facial motion information to
facial expression and emotion re cognition. Several works docu-
ment the use of facial optical ﬂow features for facial expression
recognition and action unit recognition tasks.
We use the BP4D-Spontaneous dataset [5] consisting of
videos of 41 participants with different facial expressions to
generate the ground-truth optical ﬂow between every pair
of consecutive frames in the dataset. The ground-truth opti-
cal ﬂow is obtained using facial key-points and image warp-
ing with afﬁne transformations. We then use this facial
optical ﬂow ground truth to train a convolutional autoen-
coder based architecture, FlowNetS [6] (specialized for opti-
cal ﬂow estimation), to learn optical ﬂow specialized for
facial motions, meaning that the motion learned should
exhibit local coherency as would be expected on faces. We
also modify the architecture by adding a cyclic loss to help
the network reconstruct the latter image in a given image
pair using the optical ﬂow predicted by the network. We
argue that adding this reconstruction in the learning frame-
work improves the predicted optical ﬂow by guiding it
using the structure of the image pairs. We perform an abla-
tion study with different loss functions, and compare the/C15Muhannad Alkaddour and Usman Tariq are with the American University
of Sharjah, Sharjah 26666, UAE. E-mail: {b00059796, utariq}@aus.edu.
/C15Abhinav Dhall is with the Indian Institute of Technology Ropar, Rupnagar,
Punjab 140001, India, and also with the Monash University, Clayton, VIC
3800, Australia. E-mail: abhinav@iitrpr.ac.in.
Manuscript received 19 March 2021; revised 18 July 2022; accepted 24 July
2022. Date of publication 10 August 2022; date of current version 15
November 2022.
This work was supported in part by the FRG17-R44 research grant, a profes-
sional development grant from the College of Engineering, and the Open
Access Program under Grant OAPCEN-1410-E00085, all from the American
University of Sharjah. This paper represents the opinions of the authors and
does not mean to represent the position or opinions of the American University
of Sharjah. The work of Dr. Dhall was supported by Australian Research
Council under Grant DP190102919.
(Corresponding author: Usman Tariq.)
Recommended for acceptance by A. A. Salah.
Digital Object Identiﬁer no. 10.1109/TAFFC.2022.3197622IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022 2071
This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creative commons.org/licenses/by-nc-nd/4.0/performance of our network and other baseline optical ﬂow
CNNs by testing on both the Extended Cohn-Kanade data-
set [7] and a portion of BP4D-Spontaneous dataset. Finally,
we test the usefulness of our network by using the learned
optical ﬂow predictions for micro-expression detection
using optical ﬂow and the Shallow Triple Stream Three-
dimensional CNN (STSTNet) [8].
Hence, the contributions of this paper are:
/C15Introduction of a “noisy” optical ﬂow dataset for
faces, making use of the peculiar structure of faces.
The noisiness of the data comes from the sparse trian-
gulation over the 68 facial landmarks that are used
to generate the dataset.
/C15Learning a network for optical ﬂow estimation, spe-
cialized for movements induced by facial expressions.
We then complement the structure with a cyclic loss.
Our modiﬁed architecture outperforms several other
networks used for optical ﬂow estimation.
/C15Exhibiting the usefulness of our trained network by
applying it for micro-expression detection.
The remainder of the paper is organized as follows.
Section 2 contains related literature in the relevant topics.
Section 3 describes the details of the automatic dataset gener-
ation used in this paper, while details of the networks trained
on the generated dataset are explained in Section 4. The
results of the ablation study and micro-expression recogni-
tion are presented in Section 5. And ﬁnally, we present the
concluding remarks and recommendations for improvement
and future work in Section 6.
2R ELATED WORK
First, we discuss works related to optical ﬂow estimation
using classical and deep learning techniques, along with
some of the common challenges. We follow this up by a sur-
vey of optical ﬂow methods as applied to faces in particular,
and how optical ﬂow is used in tasks such as micro-expres-
sion detection.
2.1 Optical Flow Estimation
Optical ﬂow in images is used to estimate the motion of sets
of pixels across images. Classical methods, such as [9] and
[10], use the intensity derivatives and energy methods to
estimate the optical ﬂow.
2.1.1 Optical Flow Challenges
Over the last four decades, notable challenges have been
identiﬁed in optical ﬂow generation and the methods were
speciﬁcally developed to overcome them. Shah and Xuezhi
[11] provide an extensive review on each challenge, which
include motion discontinuities, motion blur and occlusions,
brightness, and large motions.
In-the-wild datasets are prone to occlusions in their
scenes, since an uncontrolled environment is likely to con-
tain moving objects that overlap in the video sequences. No
one method is sufﬁcient to solve the problem in general.
Some popular solutions are reviewed in [11], which are
image warping and bidirectional inconsistency. Janai et al.
[12] approach the problem by considering a triplet instead of
a pair of frames and a photometric loss to handle theocclusions. Meister et al. [13] build on these concepts by
applying their own loss function to improve results of unsu-
pervised learning of optical ﬂow, as well as learning the ﬂow
in the forward and reverse directions as in [12], and Ren et al.
[14] also use a consistency loss to mitigate the occlusion.
Motion discontinuities can arise in occluded settings and
in non-rigid motion, and can result in erroneous optical
ﬂow continuation in regions of discontinuity [11]. Sun et al.
[15] devise a non-local term that assists the objective function
in accounting for motion discontinuity. Tian et al. [16] mod-
ify this non-local term in a CNN-based method to account
for discontinuity in their objective. In addition, Wang et al.
[17] adapt it as a CNN block for the same purpose. Other
existing approaches include detecting the discontinuous
boundary and correcting for the ﬂow [18] and a suitable
energy-minimization [19].
2.1.2 Deep Learning for Optical Flow Estimation
With the surge and success of deep learning applications in
the past decade, there has also been a rise in using convolu-
tional neural networks to learn optical ﬂow, beginning with
the groundbreaking work of Fischer et al. [6] with their Flow-
NetCNN architecture. Building on the success of FlowNet,
FlowNet2.0 [20] was introduced a few years later to improve
performance by stacking networks, scheduling the training
data, and learning on small-motion datasets. FlowNet3.0 [21]
was also proposed afterwards for scene ﬂow estimation. For
our experiments, we use the FlowNetS architecture adapted
from [6] to train on our dataset. By demonstrating how we
can adapt FlowNetS to perform well on datasets consisting
of only faces, we can later improve even further by training
more advanced architectures on such datasets.
While FlowNet is one of the most popular optical ﬂow
deep learning architectures, several other architectures have
since been proposed to deal with certain challenges.
Sun et al. [22] used the pyramid-structure CNN architec-
ture PWC-Net for optical ﬂow prediction, which we use in
this work to test on the face optical ﬂow dataset as a bench-
mark implementation and compare with our performance.
Another optical ﬂow CNN we use for comparison in this
work is LiteFlowNet by Hui et al. [23], which surpassed Flow-
net2.0’s performance on the KITTI and Sintel ﬁnal datasets.
In their pioneering work, Zhu et al. [24] developed the
cycleGAN , which is a type of generative adversarial network
(GAN), that implements a cyclic loss function which is used
as a metric to evaluate the network’s prediction as compared
with one of the inputs. Both Yu et al. [25] and Lai et al. [26]
adapt the cyclic loss for optical ﬂow learning. Both of the lat-
ter architectures used a differentiable spatial transformer
layer with learnable parameters, adapted from Jaderberg
et al. [27].
2.2 Optical Flow and Facial Expression Analysis
We now discuss various optical ﬂow methods as applied to
facial expression analysis, many of which are based on deep
networks. One important work in learning optical ﬂow for
facial expressions, by Snape et al. is Face Flow [28], which
minimizes a proposed energy to learn the ﬂow ﬁeld for a
sequence of frames consisting of facial expressions. Another
relevant work is optical ﬂow dataset generation done by Le
et al. [29] who are also concerned with producing optical2072 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022ﬂow ground-truth data for general video sequences.
According to them, little prior work exists on how the per-
formance of CNNs is inﬂuenced by optical ﬂow datasets,
and their main focus is that of non-rigid motion. Our work
can be considered to be a contribution to the study of optical
ﬂow’s effects on CNNs, with the difference being that we
focus on facial datasets instead. We attempt to learn optical
ﬂow from the face movements themselves. On a side note,
an evaluation of different optical ﬂow techniques applied
for facial expression recognition can be found in [30].
We mention a few implementations of deep networks in
facial expression analysis using optical ﬂow. Koujan et al.
[31] recently proposed DeepFaceFlow , in which they con-
struct a 3D optical ﬂow dataset for faces from a large collec-
tion of videos and compare the performance of their U-net
trained on their dataset with other CNN architectures for
both 2D and 3D optical ﬂow estimation. One key difference
between our work and theirs is that we incorporate a cyclic
loss to test how well the ﬂow ﬁelds reconstruct the second
image in the image pairs. Additionally, the training data we
generate is based upon the BP4D-Spontaneous dataset,
which is speciﬁcally tuned to exhibit various emotions and
thus more specialized for expression recognition tasks. In
addition, we also test our network’s performance on micro-
expression detection.
Several works also use optical ﬂow for action unit recog-
nition. Ma et al. [32] proposed Action Unit (AU) R-CNN to
improve AU recognition by using expert prior knowledge,
which can be in the form of optical ﬂow, to guide an R-
CNN in locating the action region. Yang and Yin [33] learn
both optical ﬂow and facial action units for static images in
one combined CNN architecture. They learn optical ﬂow in
an unsupervised manner, as an intermediate output of a
deep model ( OFNet ), which when combined with the ﬁrst
image in a pair (also the input of the model), gives the sec-
ond reconstructed image at the ﬁnal output. They call this
intermediate output as coming from a hidden layer . They use
the output of this hidden layer as an input to another net-
work ( AU-Net ) to detect facial AUs. They train both AU-Net
and OFNet jointly.
In another work that uses a cyclic loss, Li et al. [34] learn a
symmetric encoder-decoder architecture to learn AU repre-
sentations in a semi-supervised manner. They train it with
pairs of face images of the same person in a video with differ-
ent facial actions and head poses. Hence, these images are not
coming from consecutive frames. They attempt to disentan-
gle the embeddings related to head pose and action units, by
constraining the pixel movements related to the AUs to be
subtle, compared to those related to head-pose. They then
use the learned AU and pose related displacements to recon-
struct the second image in an image pair, given the ﬁrst one.
After learning, they use the AU embeddings for facial AU
detection. Note, that since embeddings are being learnt here
on frames of a person that are not consecutive, within a video,
these embeddings will not be able to learn the subtle pixel
movements that happen within a certain AU. Other works
that use optical ﬂow for action unit recognition can be found
in [35], [36], and [37]. Our work, on the other hand, focuses
on learning optical ﬂow specialized for faces. We introduce a
noisy optical ﬂow dataset, that we generate using the motion
of sparse facial landmarks. We then learn a network foroptical ﬂow estimation, specialized for movements induced
by facial expressions. We then complement the structure
with a cyclic loss. We show that our modiﬁed architecture
outperforms several other networks used for optical ﬂow
estimation. In addition, we demonstrate the usefulness of our
approach by applying it for micro-expression detection.
Liong et al. [38] exploit the optical ﬂow in a video
sequence between the frame with the highest intensity,
called apex, and each of the rest of the frames, using the opti-
cal ﬂow as input to a deep network for micro-expression
detection. They also use apex and onset frames in [8] to com-
pute optical ﬂow along with an added feature, the optical
strain, as input to STSTNet, which we adapt in this work to
test for micro-expression recognition. Verburg and Menkov-
ski [39] use optical ﬂow histograms as feature inputs to a
recurrent neural network for the recognition of micro-
expressions. Li et al. [40] use a CNN to locate facial keypoints
and FlowNet2.0 to compute optical ﬂow, and the ﬂow fea-
tures are then used with a support vector machine for micro-
expression detection.
After having reviewed several related works, we now
describe the dataset preparation in our work.
3D ATASET PREPARATION
Our method is inspired by the progress in self supervised
learning techniques for action recognition [41] and eye gaze
prediction [42]. We use the BP4D-Spontaneous dataset [5],
which consists of 41 subjects with 8 video sequences each,
containing videos of elicited emotions. The motivation for
using BP4D-Spontaneous is its inclusion of both head and
facial motion. While local non-rigid facial motion estimation
is the primary focus of this work, it is also useful to capture
this local facial ﬂow in the presence of head motion. Since
BP4D-Spontaneous is concerned with spontaneously elicited
expression sequences and 3D encoding, more general motion
is available. Other datasets, such as the Extended CK+ [7], are
more specialized for AU or facial expression detection, and
thereby are less suited for a more general motion framework.
Moreover, this allows us to test how optical ﬂow performs on
micro-expression detection when trained on a dataset not
specialized for micro-expression detection.
Fig. 1 shows the overall pipeline for a pair of frames
and how they can be used for dataset generation and CNN
training.1
We introduce the notation that we’ll use throughout this
section to generate the optical ﬂow ground truth from the
BP4D-Spontaneous dataset [5]. For a given sequence Sin the
dataset, we denote the frames contained in SbyF¼ffkgNf
k¼0,
where fk2RH/C2W/C23are the ordered frames. Our aim in this
section is to compute a set of optical ﬂow ﬁelds, UUUUUUUseparately
for every ordered set of frames, F, where UUUUUUU¼fuuuuuuukgNf/C01
k¼0con-
tains the optical ﬂow ﬁelds uuuuuuuk:RH/C2W7!RH/C2W/C22for each
frame except the ﬁnal one in that sequence. The uuuuuuukare vec-
tor-valued functions deﬁned on the image grid.
Landmarks PPPPPPPon the face in Sare tracked for each frame
using the open source OpenFace pipeline [43], which uses
1.Our code implementing the algorithm in this section will be made
publicly available at https://github.com/malkaddour/Self-Supervised-
Approach-for-facial-movement-based-optical-ﬂowALKADDOUR ET AL.: SELF-SUPERVISED APPROACH FOR FACIAL MOVEMENT BASED OPTICAL FLOW 2073the Convolutional Experts Constrained Local Model [44] to
obtain 68 landmarks perfor the kth face, PPPPPPPk¼ðppppppp0000000ppppppp68686868686868ÞT
k2
R68/C22, where Tdenotes the transpose operation.
Next, we use Scipy’s Delaunay triangulation package to
obtain a triangular mesh over PPPPPPP0in the ﬁrst face f0.T h i sm e s h
divides the face in f0intoNtdisjoint triangles TTTTTTT0¼ftttttttlgNt
l¼0,
where each tttttttl¼vvvvvvv0;vvvvvvv1;vvvvvvv2 ðÞT
l2R3/C22is the matrix with
rows composed of vertices of triangle l. After triangulating f0,
we use similar triangulation on the remaining frames in the
sequence, yielding the set of triangulations fTTTTTTTkgNf
k¼0onS.
We use the triangulation TTTTTTTk/C01to capture the local motion
on every triangle in the face partition from frame fk/C01to frame
fk. Given the triangle tttttttl
k/C01, we infer an afﬁne map AAAAAAAl
k/C012R3/C23
that sends its vertices to the vertices in tttttttl
k.S p e c i f y i n gt h r e e
mappings are sufﬁcient to uniquely deﬁne an afﬁne map [45].
We can deﬁne tttttttl;/C3¼tttttttlllllll;11111113/C21/C0/C1T2R3/C23to be the matrix of
homogeneous coordinates of each vertex. Then, for all trian-
gles in fk/C01andfk,AAAAAAAl
k/C01that sends tttttttl;/C3
k/C01totttttttl;/C3
kis uniquely
determined by,
Al
k/C01Al
k/C01Al
k/C01Al
k/C01Al
k/C01Al
k/C01Al
k/C01¼tttttttl;/C3
k/C01/C16/C17/C01
tttttttl;/C3
k: (1)
This gives the required matrix for the afﬁne map. Note that
if the triangle is degenerate, then ttttttt/C3
k/C01will be singular. Once
the correspondence between the two triangles across frames
is known, AAAAAAAl
k/C01also maps the interior of tttttttl
k/C01to the interior
oftttttttl
k, since barycentric coordinates [46] are invariant under
afﬁne maps [45].
We use the barycentric coordinates to compute the interi-
ors of all the triangles in TTTTTTT0, and then learn each afﬁne map
AAAAAAAl
0as described above to map all the triangle interiors from
TTTTTTT0toTTTTTTT1. To compute the interior of the triangle using bary-
centric coordinates, an efﬁcient algorithm from [47] can be
used to test if an arbitrary point vvvvvvvis contained in a given tri-
angle by solving,
VVVVVVV/C21/C21/C21/C21/C21/C21/C21¼bbbbbbb, where,
VVVVVVV¼kvvvvvvv1/C0vvvvvvv0k2
2 ðvvvvvvv2/C0vvvvvvv0Þ/C1ðvvvvvvv1/C0vvvvvvv0Þ
ðvvvvvvv2/C0vvvvvvv0Þ/C1ðvvvvvvv1/C0vvvvvvv0Þk vvvvvvv2/C0vvvvvvv0k2
2 !
, and
bbbbbbb¼ðvvvvvvv/C0vvvvvvv0Þ/C1ðvvvvvvv1/C0vvvvvvv0Þ
ðvvvvvvv/C0vvvvvvv0Þ/C1ðvvvvvvv2/C0vvvvvvv0Þ/C18/C19
(2)
for/C21/C21/C21/C21/C21/C21/C21¼/C211;/C21 2 ðÞT, and /C213¼1/C0/C211/C0/C212[47]. and if the If
each /C21i2½0;1/C138, then vvvvvvvlies in the closure of the triangle ofinterest, i.e., vvvvvvvis a convex combination of the columns of ttttttt.
We test all points in this way using a rectangular discrete
grid surrounding the triangle. Repeating this for all fkin the
sequence is overall computationally expensive, so we only
do it for triangles in the ﬁrst frame of that video. By invari-
ance of barycentric coordinates under the afﬁne maps AAAAAAAl
k/C01,
this also determines the barycentric coordinates for all sub-
sequent frames fk,k> 0.
After determining the afﬁne maps and mapping the tri-
angles and their interior pixels vvvvvvvk/C01tovvvvvvvk, we compute the
per-pixel optical ﬂow vector ~uuuuuuuk/C01by
~uuuuuuuk/C01¼vvvvvvvk/C0vvvvvvvk/C01: (3)
However, when the domain is not a discrete grid, the opti-
cal ﬂow ﬁelds ~uuuuuuukare deﬁned on points that are not neces-
sarily pixel coordinates, which affects the frames after f0.
The optical ﬂow ﬁeld ~uuuuuuuk/C01from Equation (3) is deﬁned on
a discrete grid, but the pixels that are mapped from f0to
f1will subsequently be mapped from f1tof2,i nw h i c h
case it is not guaranteed that they also lie on a discrete
grid. To recover the optical ﬂow ﬁeld uuuuuuuk/C01on a discrete
grid in the target image, we use bicubic spline interpola-
tion over the irregular grid using ~uuuuuuuk/C01. We only do this to
deﬁne the optical ﬂow ﬁeld at each frame, but continue to
learn the afﬁne maps on the irregular grids, since we wish
t op r e s e r v et h es a m eb a r y c e n t r i cc o o r d i n a t e so b t a i n e di n
f0for all frames. The ﬂow ﬁelds are stored in .ﬂo formats
for later use in the experiments.
Together with the resampling stage, this procedure gives
us the ground-truth vector ﬁeld for all pixels of frame fk/C01.
The details can be summarized as follows:
1) Starting from frame f0, determine the interiors of all
triangles tttttttl
0, using barycentric coordinates.
2) Learn the afﬁne maps sending all tttttttl
0totttttttl
1and trans-
form the entire face to obtain the ﬁrst optical ﬂow
ﬁeld uuuuuuu0.
3) For all frames starting from f1, again infer the afﬁne
maps sending all tttttttl
1totttttttl
2and apply the transforma-
tion on all the pixels which have already been
mapped from frame f0. This removes the need to
expensively compute the triangle interiors for frame
f1while still ﬁnding the optical ﬂow ﬁeld ~uuuuuuu1.
4) From ~uuuuuuu1, resample the ﬂow ﬁeld over a discrete grid
to yield the ground-truth ﬂow uuuuuuu1.
Fig. 1. Overall pipeline for data generation and network training: Two examples of the afﬁne maps are shown for some triangles l1,l2, and an illustra-
tion of the resampling process is shown on a 3/C23grid of a portion of the optical ﬂow ﬁeld.2074 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 20225) Repeat steps 3-4 for the remaining frames in a
sequence ffkgNf
k¼2, for all sequences and subjects.
The total number of images in the generated dataset is
325720, and these were partitioned into 228171, 65130, and
32419 for training, validation, and test data respectively.
The dataset generation was completed in a total of about
ﬁve days using multicore CPU parallel processing with four
parallel processes running at a time. Our method generates
optical ﬂow that captures the motion induced by facial
expressions but is noisy due to the relatively sparse number
of landmarks used to triangulate the face. However, the
overall mapping from one frame to the next is a piecewise-
afﬁne map that can capture the discontinuous nature of
facial expression motion, since there are no continuity
restrictions imposed between a given mesh and its neigh-
bors. Of course, smaller and denser meshes, such as those
around the eyes and mouth in our triangulation, can better
detect any nonlinear or discontinuous motion that may
arise. An improvement to our method would be to increase
the sampling of landmarks and allow for ﬁner resolution in
the piecewise-afﬁne map.
4B ASELINE NETWORKS
In this section, we describe the CNN architecture used to
train the optical ﬂow, followed by the training and ablation
study details. These details include the different hyperpara-
meters used in the different experimental setups, such as
the choices of loss functions, the loss weights, and training/
testing data split.
4.1 CNN Architecture: FlowNetS
To test the effects of having a large, “noisy” ground-truth opti-
cal ﬂow dataset specialized for faces on CNNs, the FlowNetS
[6] architecture was used. FlowNetS is one of the pioneering
CNNs on optical ﬂow learning. While more sophisticated
optical ﬂow architectures have been developed, our purpose
is to demonstrate the improvement of training a CNN with
face data compared with some other datasets, e.g., the Flying-
Chairs dataset, as a proof of concept. Should we discover animprovement, we can expand it in the future to tackle other
problems (e.g., robustness to occlusions).
FlowNetS is a convolutional autoencoder architecture
which accepts a pair of images as input and outputs the
per-pixel optical ﬂow from the ﬁrst image to the second. It
consists of a sequence of downsampling convolutional
layers in the encoder followed by upsampling layers in the
decoder, in addition to intermediate operations and concat-
enations. Another variant of FlowNet, which is FlowNet-
Corr, is characterized by a cross-correlation layer which
fuses two input streams together, contrasted to FlowNetS
which combines them with a simple concatenation. The dif-
ference in performance reported in [6] is not too signiﬁcant,
and including the cross-correlation layer during training
resulted in the inconvenience of much longer training times.
The output resolutions of each of the ﬂow predictions in
our network are slightly different than the original Flow-
NetS. Speciﬁcally, the ratio of our ﬂow prediction heights to
theirs is 24:17, and our widths to theirs is 4:5. The reader is
referred to [6] for speciﬁc details on the network architecture.
4.2 Cyclic Loss for Image Reconstruction
For some of the experiments described in the next section, a
cyclic loss is implemented to minimize the difference
between the output predicted using the ﬂow prediction and
the second input image. This resulted in an additional
warping layer to the network that acts on the ﬂow predic-
tion with highest resolution. The warping layer uses the
predicted per-pixel ﬂow ﬁeld vectors to warp the ﬁrst input
image, and the result is recovered using bilinear interpola-
tion. We note that structures inherent only to the second
input cannot be reproduced in the warped output, since the
warping function only changes pixel locations from the ﬁrst
input, and does not contain any learnable parameters. Fig. 2
shows two examples of this phenomena from FlyingChairs
and our face dataset, showing the original input image pair
ðX1,X2Þ, the image X0
2deformed using the ﬂow ﬁeld, and
visualization of the ﬂow ﬁeld Y.
The dominant motion in the FlyingChairs image pair
from the ﬂow ﬁeld is rightward motion of the left armchair.
Fig. 2. Effect of using ﬂow ﬁeld Yto warp X1toX0
2is demonstrated for images with large (top) and small (bottom) motion. Flow vector magnitudes are
not to scale and are ampliﬁed for illustrative purposes.ALKADDOUR ET AL.: SELF-SUPERVISED APPROACH FOR FACIAL MOVEMENT BASED OPTICAL FLOW 2075The location of the armchair in the warped image is correct,
but the reconstruction of the warped portion is missing.
This is also present in the smaller desk chair, making a copy
of itself at the warped location during reconstruction. Due
to these large differences in the images, adding a warping
layer while training on the FlyingChairs dataset is likely to
worsen the network’s performance. However, this effect is
much more subtle in our face dataset due to the higher
frame rate of the sequences, which causes lower magnitude
motion between every two consecutive frames. For the face
example in Fig. 2, the deformed image X0
2is perceptually
similar to the actual X2, particularly in the upwards motion
of the eyes and the slight rightward motion caused by the
furrowing of the brow. Since the time difference between
two frames is very small in the face dataset, it is very
unlikely for new structures to be introduced in X2. A nota-
ble exception to this is the opening (closing) of the mouth
due to revealing (hiding) teeth, which cannot be reproduced
by pixel rearrangement alone. Another exception would be
the squinting or widening of the eyes for the same reason,
since the eyelid or eyeball would not be present in the ﬁrst
image. Although the artifacts caused by the warping pro-
duced a ﬂawed image in the FlyingChairs dataset, we
hypothesize and show that it still helps guide the directions
of the predicted ﬂow when training on faces since the unde-
sirable effects are considerably less due to the lower amount
of new structure.
4.3 Training and Ablation Studies Details
The training details of the aforementioned architecture are
described in this section2. Ablation studies are performed
on FlowNetS by training the network with different loss
functions and their corresponding weights.
We denote by ðXi;Xiþ1Þthe pair of successive input
frames, where Xi;Xiþ12R384/C2512/C23and YYYYYYYi¼f ðYiÞkg5
k¼1,
^YYYYYYYi¼f ð ^YiÞkg5
k¼1contain the intermediate multi-scale
ground-truth and prediction ﬂow ﬁelds respectively, where
the elements ðYiÞk,ð^YiÞk2RHk/C2Wk/C22. The ienumerates the
entire training set, and successive image frames are input to
the network at every iteration. The resolutions of the inter-
mediate ﬂow ﬁelds are ðHk;WkÞ¼ð 384/C22/C0k;512/C22/C0kÞfor
k2f1;...;5gin the decoder. ðYiÞ1,ð^YiÞ1are the largest ﬂow
ﬁelds, as in the original FlowNetS network. Note that, in the
following, we drop the added subscript and refer to them as
Yiand ^Yi.
Since we assume that the background is stationary, much
of the ground-truth ﬂow ﬁeld outside of the boundaries
deﬁned by the key-points are zero vectors. To make the train-
ing more practical, we zoom on the box with vertices deﬁned
by the key-points with maximal and minimal coordinates
plus somean offset of 10 pixels each in the xandydirections.
The cropped images and ﬂow ﬁelds are then resized using
bilinear interpolation. To preserve the units of the ﬂow vec-
tors as pixels, they are scaled accordingly in the horizontal
and vertical directions.
Next, we describe the different experimental setups used
to train the networks.4.3.1 Experimental Setup 1: No Cyclic Loss
In this experiment, the architecture is used without the addi-
tional warping layer. The network was trained for 30, 40, and
400 epochs on the face, FlyingChairs, and Sintel datasets
respectively, with 15000, 21592, and 870 training and 1000,
640, and 271 validation input image pairs each. The batch
size used for training is 16 input pairs. The loss function is
the average endpoint error (EPE), Li
1ðYYYYYYYi;^YYYYYYYiÞ, deﬁned for one
output by,
Li
1ðYYYYYYYi;^YYYYYYYiÞ¼X5
k¼1wk
HkWkXHkWk
j¼1ðyyyyyyyijÞk/C0ð^yyyyyyyijÞk/C13/C13/C13/C13
2: (4)
Here, the wkare loss weights for each intermediate ﬂow
prediction loss, given by wk¼2/C0k.Hk;Wkare the sizes of
the intermediate predictions and the ðyyyyyyyijÞk;ð^yyyyyyyijÞkare the
ﬂow vectors for the jth pixel of the kth ground-truth and
predicted ﬂow ﬁelds ðYiÞkandð^YiÞk. The optimizer used is
Adam, with b1¼0:9andb2¼0:999as in [6]. This performs
better than alternative optimizers. We initialized the learn-
ing rate aat1e/C04for faces and 5e/C05for FlyingChairs and
scheduled similar to [6].
For preliminary experimentation, we trained the network
once on the face data for a 15k and 1k training and valida-
tion split. We then used a disjoint set of 3k image pairs from
the face dataset as the test set. Also, we used the aforemen-
tioned validation sets for FlyingChair and Sintel validation
sets as test sets, since we do not have access to their original
test sets.
We then tested the model, learnt on 15k training image
pairs from the face data, separately on the face, FlyingChairs,
and Sintel test sets [48]. Then we repeated this by training on
FlyingChairs and Sintel training datasets individually and
testing them on all three test sets. We report these results in
Table 1.
After the preliminary experiment, we trained the same
network again from scratch on faces only for a 228k, 65k,
and 32.5k train/val/test split, exactly as in the next two
experiments, to make them comparable. The latter setup is
referred to as Experiment 1 , from here onwards.
4.3.2 Experimental Setup 2: With Cyclic Loss
When the warping layer [26] at the end of the network is
included, it is necessary to deﬁne a cyclic loss function
for the warped output ^Xiþ1and the second input Xiþ1.
We expect to see an improvement in the ﬂow prediction
due to the cyclic loss. For this experiment, we deﬁne the
additional cyclic loss function Li
2ðXiþ1;^Xiþ1Þfor one out-
put pair ias:TABLE 1
The Average EPE for Each Network Described in Section 4.3.1,
Trained and Tested on All Three Datasets
Tested on
Trained on Faces FlyingChairs Sintel
Faces 0.4054 5.8495 5.1731
FlyingChairs 1.4040 1.4413 3.0300
Sintel 0.8282 7.7613 6.23580
2.Source code for training and evaluation, as well as the trained
models, will be available at https://github.com/malkaddour/Self-
Supervised-Approach-for-facial-movement-based-optical-ﬂow2076 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022Li
2ðXiþ1;^Xiþ1Þ¼1
HWXHW
j¼11
3X3
k¼1xiþ1;j;k/C0^xiþ1;j;k/C13/C13/C13/C13
H1
xkkH1¼1
2x2jxj/C20d
1
2d2þdðjxj/C0dÞjxj>d(
(5)
which uses the Huber loss function kxkH1[49], a variant of
theL1loss that is everywhere differentiable, since it is qua-
dratic for small values of x. The xiþ1;j;k;^xiþ1;j;kare values of
thejth pixel of Xiþ1;^Xiþ1at color channel k. We also note
that ^Xiþ1is a function of the ﬁrst image of the input pair,
Xi, and ^Yi, which is the ﬂow prediction with largest resolu-
tion. The total loss function JðX;^X;Y; ^YYYYYYYÞfor all training
pairs is then
JðX;^X;YYYYYYY;^YYYYYYYÞ¼1
MXM/C01
i¼0/C211Li
1ðYYYYYYYi;^YYYYYYYiÞþ/C212Li
2ðXiþ1;^Xiþ1Þ/C2/C3
(6)
with the Li
1;Li
2deﬁned in Equations (4) and (6) and /C211;/C212to
be speciﬁed, averaged over all Mtraining examples. In this
experiment, we train the network on both faces and Flying-
Chairs datasets using two different sets of loss weights
/C211;/C21 2. One network has more emphasis on reconstruction,
with /C212¼0:6;/C21 1¼0:4. We refer to this as Case I. The other
network has higher weight assigned to the EPE with /C211¼
0:75;/C21 2¼0:25. We refer to this as Case II. Note that the wi
in Equation (4) should sum to /C211. For both cases, we trained
the network on faces for 15 epochs and 228160 training
pairs. Learning rates were kept constant for these experi-
ments throughout training, since scheduling them as previ-
ously done lead to very large gradients halfway through
training. In Case I, the learning rates were 2:5e/C06and
1:25e/C06for faces and FlyingChairs respectively, and in Case
II, they were both set to 2:5e/C06. We then tested the trained
networks on the test set of 32416 image pairs.
4.3.3 Experimental Setup 3: With Cyclic Loss,
Smoothness Constraint, and Average Angular
Error
In this experiment, we added an additional loss function
Li
3ð^YiÞ. In Case I of this experiment, a smoothness constraint
was imposed on the ﬂow prediction by minimizing the ﬂow
gradients, deﬁned as:
Li
3ð^YiÞ¼1
HWXHW
j¼1 
@^uij
@x/C13/C13/C13/C13/C13/C13/C13/C13
H1þ@^uij
@y/C13/C13/C13/C13/C13/C13/C13/C13
H1
þ@^vij
@x/C13/C13/C13/C13/C13/C13/C13/C13
H1þ@^vij
@y/C13/C13/C13/C13/C13/C13/C13/C13
H1!
(7)
where ð^uij;^vijÞare the components of the predicted ﬂow
vector ^yyyyyyyijat every pixel j.
Another common metric to quantify performance of opti-
cal ﬂow algorithms [50] is the average angular error (AAE).
The average angular error between two ﬂow vectors is the
average of the angle difference between every ground-truth
and predicted ﬂow vectors in the homogeneous coordi-
nates, which are yyyyyyy/C3
j¼ðyyyyyyyT
j;1ÞTand ^yyyyyyy/C3
j¼ð^yyyyyyyT
j;1ÞTrespectively.
In Case II, the loss function Li
3ðYi;^YiÞis deﬁned as:Li
3ðYi;^YiÞ¼1
HWXHW
j¼1arctanyyyyyyy/C3
ij/C2^yyyyyyy/C3
ij/C13/C13/C13/C13/C13/C13
2
yyyyyyy/C3
ij/C1^yyyyyyy/C3
ij0
@1
A (8)
The total loss function is then a weighted sum of the loss
functions
JðX;^X;YYYYYYY;^YYYYYYYÞ¼1
MXM/C01
i¼0½/C211Li
1ðYYYYYYYi;^YYYYYYYiÞþ/C212Li
2ðXiþ1;^Xiþ1Þ
þ/C213Li
3ðYi;^YiÞ/C138 (9)
We trained the network on only the faces dataset for 14
epochs and 228160 training pairs, with /C211¼0:3,/C212¼0:5,
/C213¼0:2, and learning rate 2:5e/C06. We initialized the
weights from the results of Experiment 2 (Case I), to see if
there is any improvement in ﬂow prediction after adding
L3. In the next sections, we will use abbreviations for experi-
ment and case numbers in the discussions for brevity. For
example, Experiment 2, Case II is referred to as Exp. 2II,
and no Roman numerals mean we refer to both cases of that
particular experiment.
4.4 Micro-Expression Detection
In this section, we describe how optical ﬂow features are
used for a micro-expression recognition task to demonstrate
the efﬁcacy of the optical ﬂow generated using our method.
The use of optical ﬂow in micro-expression recognition
has proven useful in several prior works, as described in
Section 2.2.
4.4.1 CNN and Optical Flow Features
To train the optical ﬂow features, we use the three-dimen-
sional lightweight CNN proposed by Liong et al. [8], named
the ”Shallow Triple Stream Three-dimensional CNN”, or
STSTNet, which shows improved results compared with
their previous work and other deep networks for micro-
expression recognition. Their algorithm is evaluated on the
CASME II [51], SAMM [52], and SMIC [53] datasets, com-
posed of videos containing micro-expressions that represent
either negative, positive, or surprise emotions (three-class
classiﬁcation). For comparison, we do the same using our
optical ﬂow features on the SAMM and SMIC datasets. For
each video sequence, they compute the optical ﬂow between
the onset and apex frames, and use this optical ﬂow as input
to train STSTNet classiﬁer. The apex frames in SAMM are
provided with the dataset, which is not the case with SMIC.
The apex frames were also used for micro-expression recog-
nition on SMIC dataset by Quang et al. [54]. We make use of
their labeling for the SMIC dataset. We crop the faces based
on keypoints obtained using the OpenFace 2.0 toolbox [43]
for SAMM. For SMIC, since OpenFace failed to detect the
keypoints for some images, we instead use the dlib facial
landmark detector [55], which is based on an ensemble of
regression trees [56], and deﬁne the crop border at 15 pixels
away from the maximum and minimum xandyimage
coordinates.
We follow their the recommended approach in [8] to
train the STSTNet. The optical ﬂow from the onset to the
apex frame is used to compute the optical ﬂow strain /C15/C15/C15/C15/C15/C15/C15ðUUUUUUUÞ
for a given ﬂow ﬁeld, UUUUUUU¼ðuðx; yÞ;vðx; yÞÞ. The strain isALKADDOUR ET AL.: SELF-SUPERVISED APPROACH FOR FACIAL MOVEMENT BASED OPTICAL FLOW 2077deﬁned [8] by the symmetric matrix known as the strain
tensor
/C15/C15/C15/C15/C15/C15/C15¼1
2rUUUUUUUþð r UUUUUUUÞThi
¼@u
@x1
2@u
@yþ@v
@x/C16/C17
1
2@u
@yþ@v
@x/C16/C17
@v
@y0
B@1
CA: (10)
The strain of a planar displacement ﬁeld ðu; vÞis well-
known in solid mechanics, consisting of normal strains
/C15xx;/C15yy, which are the diagonal elements, and shear strains
/C15xy¼/C15yx, which are the off-diagonal elements [8]. The strain
values represent the type of local deformation that occurs
at each point in the ﬂow ﬁeld. The optical strain norm
jj/C15/C15/C15/C15/C15/C15/C15ðuðx; yÞ;vðx; yÞÞjjsis then deﬁned [8] as:
jj/C15/C15/C15/C15/C15/C15/C15ðuðx; yÞ;vðx; yÞÞjjs¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
/C152
xxþ/C152
yyþ2/C152
xyq
; (11)
The optical strain feature VVVVVVV2RH/C2W/C23is an RGB image
and for a given pixel coordinate ðxh;yhÞ, takes the value
ðuðxh;yhÞ;vðxh;yhÞ;jj/C15/C15/C15/C15/C15/C15/C15ðu; vÞjjsÞ2R3. Fig. 3 shows an exam-
ple of the optical ﬂow feature computed for an image pair,
using the optical ﬂow obtained from each network. In this
example, the salient motion is an upwards curling of the
lips plus a subtle leftwards shift in glance.
4.4.2 Micro-Expression Detection Experimental Setup
The authors of STSTNet [8] evaluate their model using leave-
one-subject-out cross-validation (LOSOCV), and we do the
same to train the micro-expression recognition networks.
The SAMM (normal) and SMIC HS datasets were both used
for the task. All optical ﬂow networks described in Sec-
tion 4.3.2 are used separately to train STSTNet. For every
optical ﬂow network, we train the network three times: once
on SAMM, once on SMIC, and once on the combined dataset
consisting of both. Prelabeled onset and apex frames for each
sequence were used to compute the optical ﬂow. The total
number of samples across all subjects is 290, which was split
into a distinct training and test set for each subject due to the
LOSOCV. This caused the number of samples in the test set
to vary between the minimum at 1 (0.345%) and the maxi-
mum at 37 (12.76%), since the number of samples across sub-
jects was not uniform. We use the publicly available codeprovided by the authors [8], and thus replicate the exact
same network architecture, with a learning rate of 5e /C05and
maximum epochs set to 500. We note that the RGB input
images, described in Section 4.4, are resized to a resolution of
28/C228/C23. We also compute the TVL1 optical ﬂow on
SAMM and SMIC, as done in [8], to compare its performance
with the optical ﬂow features obtained from other networks.
To deal with the class imbalance, we use macro-averaged
recall, precision, and F1-scores to evaluate the performance
of every trained network. Additionally, the metrics speci-
ﬁed by Yap et al. [57] are the micro-averaged F1-score and
Unweighted Average Recall (UAR). The deﬁnition of UAR
is equivalent to macro-averaged recall RM. UAR is also pop-
ular for imbalanced multiclass problems [8]. The perfor-
mance measures are deﬁned as [58]:
RM¼1
nXn
i¼1Pm
j¼1tpj
iPm
j¼1tpj
iþfnj
i;Rm¼Pn
i¼1Pm
j¼1tpj
iPn
i¼1Pm
j¼1tpj
iþfnj
i;
PM¼1
nXn
i¼1Pm
j¼1tpj
iPm
j¼1tpj
iþfpj
i;Pm¼Pn
i¼1Pm
j¼1tpj
iPn
i¼1Pm
j¼1tpj
iþfpj
i;
F1x¼2PxRx
PxþRx;G M¼ﬃﬃ
½p
n/C138Yn
i¼1Pm
j¼1tj
piPm
j¼1tj
piþfj
ni:(12)
The subscripts M;mdenotes macro and micro-averaging,
respectively, and for the F1-score, x2fM;mg.tpj
i,fpj
i, and
fnj
idenote the true positive, false positive, and false nega-
tive of class i, sample j, for a total of nð¼3Þclasses and m
samples. Note that when the prediction for a given class is a
true positive, this also counts as a true negative for each of
the other two classes. The macro-averaged metrics tend to
remove the bias caused by the imbalance degree, since it
does not ”favor” the classes with higher number of exam-
ples, as opposed to micro-averaging [58].
Moreover, since LOSOCV is used, this yields one metric
per subject. We will combine the metrics to a single scalar,
which we will refer to as the aggregated metric . This has been
done in other works such as [59] (following a different
experimental setup), and the aggregation is done by taking
the mean of the metric across all subjects for every iteration.
5R ESULTS AND DISCUSSION
To evaluate the ﬂow network, we ﬁrst report preliminary
results as discussed in Section 4.3.1. As mentioned earlier,
Fig. 3. An example of the computed optical ﬂow features used as inputs to train STSTNet for each network variant. Source: subject 03, SMIC [53].2078 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022the performance measure for this experiment is the average
EPE. We then show the results of the ablation study for the
experiments trained on the full dataset, and compare the
networks using the average EPE and AAE. Next, we evalu-
ate a number of other popular optical ﬂow methods on the
test set. These include FlowNet2.0 [20], FlowNet3.0 [21],
LiteFlowNet [23], PWC-Net [22], and the classic Gunnar-
Farneback optical ﬂow [60]. These CNN-based methods are
chosen due to their impact and relevance in the community.
To select Gunnar-Farneback ﬂow, we used Allaert et al.’s
[30] review, who recommend optical ﬂow methods well-
suited for facial motion, namely Gunnar-Farneback, Flow-
Field, and Normalized Convolutional Upsampling (NCUP).
Performance varied depending on the datasets, but Farne-
back, FlowField, and TV-L1 were consistently among the
top. Based on this, we felt that Farneback would be an
appropriate candidate from the non-CNN category.
Finally, the results of the micro-expression detection task
using all networks are presented, which shows the useful-
ness of our method for a practical application. For this task,
we use the same networks as in optical ﬂow prediction with
the addition of TV-L1, since the latter was used to compute
the optical ﬂow in STSTNet [8], the microexpression classi-
ﬁer we used in this work.
5.1 Results for Ablation Studies
We ﬁrst describe the initial results of Exp. 1, which comprise
of the networks trained and tested on faces, FlyingChairs,
and Sintel datasets. We report the performance in terms of
average EPE. The average EPE is computed for only the
largest ﬂow prediction, which is deﬁned by the k¼1term
from Equation (4) and obtained by setting w1¼1. This rep-
resents the average EPE for the largest ﬂow prediction.
Table 1 shows these preliminary results as described in Sec-
tion 4.3.1. It is worth noting that the subjects that appear in
the training set do not appear in the validation or test sets of
our face data.
The error values in Table 1 are in pixels, averaged over
each of the test sets. Row 1 shows the results when the net-
work was trained on faces and tested on all three datasets.
Similarly, rows 2 and 3 are trained on FlyingChairs and Sin-
tel and tested on all three. From Table 1, we observe that the
network trained on our BP4D-derived face dataset performs
best when tested on faces. This is likely due to the nature of
the dataset the network was trained on. The ﬂow ﬁelds on
our face dataset consist of small, non-rigid motions, espe-
cially when the head motion is lacking, whilst the motion
ﬁelds in the FlyingChairs dataset have larger magnitude and
is more rigid. The Sintel dataset is also different in nature
than the face dataset. It is likely that the network trained on
FlyingChairs overestimates the motion on the face dataset.
Note that the results in Table 1 are comparable to state of the
art methods on the Sintel dataset, as can be seen in [48].
One interesting thing to note here is that the performance
of the optical ﬂow algorithm when trained on face data and
tested on Sintel data, is much better compared to when
trained on Sintel data and tested on Sintel data. We conjec-
ture that this may show the usefulness of our generated
dataset to problems that are even unrelated to faces. On a
side note, the optical ﬂow contained in the Sintel dataset is
notable for its large motion and occlusions [48]. Due to thelarge ﬂow vectors present in Sintel, we suspect that the net-
work trained on Sintel tends to also predict ﬂow ﬁelds with
large vector magnitudes when tested on Sintel. Hence, in
our FlownetS implementation, large erroneous predictions
may have impacted the EPE more than the smaller-valued
predictions from the face-trained model. It is also possible
that the range of motions present in the face dataset trains
the network for a variety of scenarios. However, the results
for training and testing on Sintel dataset may be further
improved by using a model better adapted to the optical
ﬂow challenges present in the Sintel dataset, but this may
make the model more dataset speciﬁc.
After adding the cyclic loss and training for more data
and epochs, we expect to observe a difference in perfor-
mance compared with Exp. 1. Here, we train the setup for
Exp 1 again, using the same data split as the other experi-
ments, for comparison purposes. Now we show the results
of the networks trained with cyclic loss as described in Sec-
tions 4.3.2 and 4.3.3.
Table 2 summarizes the statistics computed based on the
results of Exp. 2 and Exp. 3, evaluated on all 32.5k image
pairs in the BP4D-Spontaneous test set and on 9930 out of a
total of 10115 available consecutive image pairs of the CK+
dataset. The ground-truth for the image pairs from CK+
were computed in an identical manner to those in BP4D-
Spontaneous, but some samples were omitted due to some
errors in the generation process.
The average EPE is deﬁned the same way as in Table 1,
and the AAE is computed exactly as deﬁned in Equation (8),
since the loss function for the AAE is only evaluated on the
largest ﬂow prediction, contrary to the EPE. As outlined at
the end of Section 4.3, Exp. 2I and Exp. 2II represent, respec-
tively, the higher and lower reconstruction weight experi-
ments, while Exp. 3I and Exp. 3II represent the experiment
with smoothness constraint and the experiment with aver-
age angular error.
There are several observations to be made from these
results. Adding the cyclic loss but with lower reconstruction
weights (Exp. 2II) improves the ﬂow prediction compared
to using only the EPE loss (Exp. 1), since both EPE and AAE
decrease signiﬁcantly. When there is higher weight on
reconstruction loss (Exp. 2I), the network alters the pre-
dicted ﬂow to improve the warped output’s semblance to
X2. However, the higher focus on reconstruction worsens
the performance of the AAE and EPE. One reason could be
that the noisy ground truth does not necessarily reconstruct
X2from X1very well, i.e., the reconstruction capability of a
predicted ﬂow ﬁeld is adversarial to the ground-truth ﬂow
EPE and AAE.TABLE 2
Flow Performance for the Ablation Studies
BP4D-Spontaneous CK+
Experiments Ave. EPE AAE Ave. EPE AAE
Exp. 1 0.2856 0.1975 0.2343 0.2080
Exp. 2I 0.4610 0.3033 0.7936 0.4786
Exp. 2II 0.2498 0.1728 0.2821 0.2440
Exp. 3I 0.7010 0.4524 1.1573 0.5869
Exp. 3II 0.4660 0.2887 1.0082 0.4640ALKADDOUR ET AL.: SELF-SUPERVISED APPROACH FOR FACIAL MOVEMENT BASED OPTICAL FLOW 2079Exp. 3 with the smoothness and AAE losses yields worse
outcomes than the other two in terms of predicted ﬂow, par-
ticularly compared to Exp. 2I. Note that Exp. 3 weights are
initialized from the latter to test any change in performance.
This could be due to the decreased weight in the EPE loss,
which suggests that the EPE is a stronger indicator of ﬂow
performance than the AAE. The EPE encodes the direction
in addition to the magnitude information. Another explana-
tion would be that training data with angular error as a loss
metric does not generalize well to test data, unlike the EPE.
Exp. 3I exhibits the worst performance in both EPE and
AAE amongst our network variants. This is likely due to the
imposed smoothness constr aints, which impose ﬂow
ﬁeld values in the otherwise null regions outside the face
boundary.
The performance of the network trained on the BP4D-
Spontaneous face data and tested on the CK+ data, reveals
interesting insights. Indeed, for almost all experiments,
there is a decrease in performance from BP4D-Spontaneous
to CK+. This is less pronounced in terms of AAE perfor-
mance. Note that this decrease in performance may be
expected due to cross-database variations. In addition, even
with the decrease, the performance is fairly decent and
shows that the learnt knowledge is transferable to another
dataset. This addresses any concern of database bias.
5.2 Comparison With Other Networks
We now compare the results with other notable optical ﬂow
implementations. Table 3 shows the ﬂow statistics computed
for the network variants described earlier.3The improvement
is deﬁned to be the percentage improvement of the best per-
forming networks on BP4D-Spontaneous and CK+ test sets
(which are Exp. 2II and Exp. 1 respectively), over the others.
In all cases, the networks trained on our automatic face
dataset perform better in both metrics than PWC-Net [22] and
LiteFlowNet [23], which are some of the popular CNN-based
optical ﬂow methods. PWC-Net demonstrates a notably high
average EPE, but a more competitive AAE. This is likely due
to an overestimation of the ﬂow prediction magnitudes. Flow-
Net2.0 and FlowNet3.0-CSS, which are both state of the art
improvements on FlowNetS, are both outperformed by all of
our network variants with the exception of average EPE in
Exp. 3I. The Gunnar-Farneback optical ﬂow performs betterthan all methods in both average EPE and AAE, but is outper-
formed by Exp. 1 and Exp. 2II. These results conﬁrm
Farneback’s high performance for facial motion as reported in
[30], as it outperforms all of the other CNN-based works. It
also helps conﬁrm that our generated ground-truth is reliable,
since the ﬂow computed using Farneback is the closest ﬂow
ﬁeld amongst the other network variants.
Our work (Exp. 1 and Exp. 2II) outperforms both the CNN-
based methods and Gunnar-Farneback in both metrics, when
tested on both BP4D-Spontaneous and CK+. Gunnar-Farne-
back is still the most accurate amongst the CNN-based meth-
ods. Note that in the case of CK+, we trained the networks on
BP4D-Spontaneous data, which is a completely different data-
set. This suggests that our method can be extended to face
datasets that are different from those in the training set and
still achieve superior performance than optical ﬂow CNNs
that were not trained on faces, indicating that the learnt
knowledge is fairly independent of the dataset. An improve-
ment would be to also include grayscale images in the train-
ing set, since their presence in CK+ is a possible reason for the
decrease in performance seen by our networks.
To investigate the type of ﬂow produced by each of the
networks on the facial images, Fig. 4 shows a sample subset
of image pairs in the test set with their respective ground-
truth and ﬂow predictions from each network. The EPE and
AAE for each prediction are also labeled, computed the
same way as in Tables 2 and 3. The saturation intensity in a
given image is only representative of the intensity of that
region relative to the other pixels of the same image. The
same intensity in two images may have substantially differ-
ent optical ﬂow vector values. This is common practice in
optical ﬂow visualization, since it places emphasis on which
motion is more salient for a given image. In images with
small motion, as is the case in many frames in the BP4D
dataset, using to-scale visualization would not convey
important local motion information. We note that the fol-
lowing remarks for the remainder of this section are qualita-
tive in nature and are based on a very small subset, but
nevertheless yield some insight to accompany the statistics
from Tables 2 and 3. We ﬁrst observe the differences in ﬂow
predictions among the networks trained on our dataset.
From these ﬁve, Exp. 1 shows the sparsest predictions,
which is expected as it only minimizing the EPE from the
sparse ground-truth ﬂow. After introducing the cyclic loss
in the other four experiments, denser optical features start
to appear, caused by the added emphasis on image recon-
struction. For example, this denser optical ﬂow allowed the
network to better predict the eye motion in rows 3 and 4 ofTABLE 3
Comparing Various Optical Flow Methods
BP4D-Spontaneous Improvement CK+ Improvement
Optical ﬂow methods Ave. EPE AAE Ave. EPE AAE Ave. EPE AAE Ave. EPE AAE
Exp. 1 (this work) 0.2856 0.1975 12.54% 12.51% 0.2343 0.2080 0% (ref.) 0% (ref.)
Exp. 2II (this work) 0.2498 0.1728 0% (ref.) 0% (ref.) 0.2821 0.2440 16.94% 14.75%
PWC-Net 1.1538 0.4643 78.35% 62.78% 1.2340 0.7049 81.01% 70.49%
FlowNet2.0 0.6719 0.4347 62.82% 60.25% 0.6496 0.5078 63.93% 59.04%
FlowNet3.0-CSS 0.6839 0.4457 63.47% 61.23% 0.5168 0.4029 54.66% 48.37%
LiteFlowNet 0.7226 0.4771 65.43% 63.78% 0.6306 0.4826 62.84% 56.90%
Gunnar-Farneback 0.3670 0.2294 31.93% 24.67% 0.3837 0.3118 38.94% 33.29%
3.The interested reader is referred to the supplemental material,
available at https://www.dro pbox.com/s/o7158gi46tppvb1/
SupplementalMaterial_OpticalFlow.docx?dl ¼0, for the error histo-
grams for both ablation studies and comparison results.2080 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022Fig. 4. Thus, the EPE loss taught the network to predict well
the regional directions and magnitudes, and the cyclic loss
helped it further localize the motion in these regions.
There is higher motion variance across the face in Exp. 2I
compared to Exp. 2II. Although both were trained with cyclic
loss, there is higher emphasis on the loss in Exp. 2I than in
Exp. 2II, which more clearly shows the effect of the cyclic loss,
since no other losses were introduced in Exp. 2. The outputs
of the networks with cyclic loss also show coarser representa-
tions compared to the outputs of FlowNet2.0 and Flow-
Net3.0-CSS, such as in rows 9 and 10 in Fig. 4. When the
smoothness constraints were imposed in Exp. 3I, the face seg-
mentation learned by the network was affected, since the
large values of the ﬂow derivatives at the face boundaries
enlarged the gradients in the smoothness loss function.
By both visual perception of these examples and the average
EPE and AAE values from Table 3, the Gunnar-Farneback opti-
cal ﬂow shows similarity in bot h direction and magnitude.
Since the method is unbiased by any training data, this similar-
ity provides a degree of validatio n to the ground-truth optical
ﬂow. However, it still underestimates optical ﬂow in some
instances, such as the near-zero regions in rows 6, 8, and 13.
The Gunnar-Farneback ﬂow also segments the face, since the
background has zero motion. This is in contrast to the outputs
of the other four networks (FlowNe t2.0, FlowNet3.0-CSS, Lite-
FlowNet, and PWC-Net). The outputs of FlowNet2.0,FlowNet3.0-CSS, and LiteFlowNet all tend to estimate back-
ground ﬂow. The ﬂow trend of their outputs from the examples
of Fig. 4 can be matched with the outputs of the other networks,
although some examples — especially those with global
motion, such as in rows 4, 5, and 13 — show appreciable differ-
ences. PWC-Net demonstrates a more consistent ﬂow pattern
similar to our networks and Gunnar-Farneback. However, the
E P Ev a l u e si nb o t ht h eF i g .4e x a mples and Table 3 suggest that
the network, perhaps, overestimates the magnitude of the opti-
cal ﬂow vectors in the ﬁeld. Although its AAE is the second-
highest, its value is close to sev eral of the other methods. How-
ever, its EPE is signiﬁcantly hig her in comparison. This fact,
complemented with the shown examples, suggests that the
direction is a lesser problem than magnitude in PWC-Net.
The examples in rows 5, 6, 12, and 13 are characterized by
predominantly global motion in one direction only. In these
examples, the subject is mainly t ilting their heads without any
c h a n g ei ne x p r e s s i o n .T h o s ei nr o w s4 ,7 ,8 ,9a n d1 0h a v et h e
local motion as their salient feature, mainly in the eyes and
mouth regions. Local face motion is more indicative of changes
in facial expression, and the netw ork’s ability to identify the
local motion can be used in FER. The remaining examples are
rich with both global and local motions, indicating more
aggressive motion along with the change in expressions. From
these examples, all the networks were able to identify the local
motions, except row 2, where t hree of the networks were not
Fig. 4. Color-coded optical ﬂow predictions for a small subset of the test set for the networks trained in each of the experimental setups. The exam-
ples contain different types of facial motion, meant to illustrate the type of ﬂow outputs produced by each network for qualitative assessment.ALKADDOUR ET AL.: SELF-SUPERVISED APPROACH FOR FACIAL MOVEMENT BASED OPTICAL FLOW 2081able to pick up the eye movements. The differences in network
outputs are clearer in the examples with global motion.
From the overall results of the experiments, one may note
that our networks trained on the automatically generated
face dataset are better-suited at predicting the optical ﬂow
on faces compared to other networks. Our model can be
applied to predict optical ﬂow on any sets of frontal / near
frontal faces. The only pre-processing required is the detec-
tion of key-points using OpenFace [43], cropping the face
using the maximum and minimum x-y coordinate values
plus a small offset in each coordinate, and resizing the result-
ing image to a resolution of 512/C2384pixels to prepare it as
an input for FlowNetS. We use the offset values of 10 pixels
in all four directions, for cropping.5.3 Results of Micro-Expression Detection
We now report on the results of the experiments described
in Section 4.4.2 for micro-expression detection. The details
of the SAMM and SMIC datasets used for training and test-
ing are given in Table 4 [52], [53], [8].
The micro- and macro-averaged metrics are shown for
every network on each of the SAMM, SMIC, and com-
bined datasets in Tables 5 and 6. In these tables, the
aggregation of the metrics across the subjects from the
LOSOCV is the mean of the metric across the subjects.
The delta values are also computed, where the delta is
deﬁned to be the absolute value of the difference between
a statistic and the maximum of that statistic in the
column.TABLE 4
Details of the SMIC and SAMM Datasets Used for Micro-Expression Detection [8], [52], [53]
No. of Samples per Class Gender Ethnicities Mean age
Dataset Positive Negative Surprise Male Female
SAMM 26 92 15 16 16 13 33.2
SMIC 51 70 43 10 6 2 28.1
Combined 77 162 58 26 22 13 30.4
The number of samples per class is also the number of video clips.
TABLE 5
Results of the Aggregated Performance Metrics With Their Deltas for Micro-Expression Recognition
on the SAMM andSMIC Datasets Separately, Using TVL1 Optical Flow as Done in [8],
Our Network With Different Variants, and the Other Optical Flow CNN Architectures
The best results in each column are in bold blue font, the second best are underlined, and the third best are in blue italic font.2082 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022The results in Table 5 indicate that the performance of
STSTNet trained on optical ﬂow features from different net-
works also signiﬁcantly depends on the dataset it is trained
on. For each evaluation measure, the top three performing
networks are indicated in bold blue, underlined, and italic
fonts respectively.
We note that the F1-scores are typically lower than the pre-
cision and recall since these are aggregated metrics, i.e., the
F1-score averaged over all F1-scores in the LOSOCV, and is
not the harmonic mean of the aggregated precision and recall.
For macro-averaged precision PM,a sw e l la st h em a c r oa n d
micro-averaged F1-scores on the SAMM dataset, the top scores
are achieved from Experiments 1, 2II, and 3II, followed closely
by FlowNet3.0-CSS and TVL1. The higher F1-scores are more
inﬂuenced by the precision values and less so by the recalls.
Exp. 1 is the highest for these thr ee metrics, while TVL1 scored
highest in RM, and FlowNet3.0-CSS in geometric mean. This is
one testimony to the complexity of c apturing the overall classi-
ﬁcation performance with a single scalar metric for multi-class
problems, since the proposed metrics can each emphasize dif-
ferent features of the classiﬁer performances. Exp. 3II is the
only variant which is consistently among the top 3 for all met-
rics, at either second or third.
The SMIC results allow for a more consistent inference on
the performance of the classiﬁers. Across all metrics, the topthree networks were FlowNet2.0, FlowNet3.0-CSS, and Lite-
FlowNet. Both the precision and recall, and consequently the
F1-scores, follow more similar trends, in contrast with the
SAMM and combined training protocols. For precision, recall,
and F1-scores, the lowest three scores are interchanged
amongst Exp. 1, PWC-Net, and Gunnar-Farneback. In fact,
Gunnar-Farneback and PWC-Net a re consistently the least per-
forming across all three training protocols.
By comparing the results across the three training proto-
cols, it is difﬁcult to conclude that optical ﬂow features com-
puted from one speciﬁc method will be optimal for training
the STSTNet classiﬁer for micro-expression detection.
Although the networks trained using our method performed
well when trained and tested on SAMM, they were somewhat
outperformed in the other two protocols. However, even in
these cases, they were not as consistently behind when com-
pared to Gunnar-Farneback and PWC-Net, which can be seen
by the delta values in Tables 5 and 6. This could be due to the
sparse nature of the learned optical ﬂow representations from
our generated dataset. It is also plausible that the accuracy of
the ﬂow magnitude prediction may not be a consistent predic-
tor of its performance on micro-expression detection. What
we mean here, is not the magnitude in general, but rather
magnitude in regions that do not correspond or assist in
microexpression detection. For example, large head motion
will have large error magnitudes associated with it. However,
smaller regions in the same face such as eyes or mouth may
have smaller error magnitudes that may be key for a microex-
pression. Hence, a good global EPE statistic can be a result of
the (correct) detection of the head motion despite a relatively
less accurate estimate of the motion in the mouth/eye region.
We hypothesize that our method will overcome the per-
formance difference in some of the results if we use a denser
keypoint tracker during the optical ﬂow training phase to
generate the BP4D ground-truth. This will likely improve
the network’s ability to more consistently capture ﬁne local
facial motion which may otherwise have been missed in the
current work. Furthermore, as previously discussed, we
have used FlowNetS to train the face data to benchmark its
efﬁcacy compared to other networks, and thus using a bet-
ter-designed CNN along with the denser keypoint ground-
truth will likely further improve the performance.
6C ONCLUSION AND FUTURE WORK
In this paper, we explore the possibility of using a facial
expression dataset to learn optical ﬂow representations
based on a self-supervised technique. Motion information on
faces has been shown to be useful in facial expression analy-
sis in multi-modal techniques.
The dataset is generated by using the image sequences
from the BP4D-Spontaneous dataset to compute the optical
ﬂow ground-truth. The OpenFace 2.0 toolbox, which uses a
constrained local model, is used to locate the facial landmarks
on every image. Delaunay triangulation is then used on the
resulting set of points to form the face mesh and allow the
computation of the optical ﬂow for every pair of images using
triangle-to-triangle afﬁne maps to develop an automatic facial
optical ﬂow dataset. The generated dataset, with a total of
nearly 324k image pairs, is used as a noisy ground-truth for
optical ﬂow to train the FlowNetS convolutional autoencoder
architecture with 228k pairs in the training partition.TABLE 6
Results of the Aggregated Performance Metrics With Their
Deltas for Micro-Expression Recognition on the combined
SAMM and SMIC Dataset , Using TVL1 Optical Flow
as Done in [8], Our Network With Different Variants,
and the Other Optical Flow CNN Architectures
The best results in each column are in bold blue font, the second best are under-
lined, and the third best are in blue italic font.ALKADDOUR ET AL.: SELF-SUPERVISED APPROACH FOR FACIAL MOVEMENT BASED OPTICAL FLOW 2083It was observed that training t he FlowNetS architecture for
optical ﬂow on this automatica lly generated noisy ground-
truth data improved the networ k’s ability to predict optical
ﬂow on face data in particular. The learned representations
also helped the network give good accuracy on the Flying-
Chairs and Sintel datasets. This demonstrates that the facial
movements are nicely encoded in our data which enables the
network to learn subtle movements that are useful on the chal-
lenging Sintel dataset as well. A cyclic loss was also added for
optimization to help the network use the predicted ﬂow to
reconstruct the second image, and the ﬂow results from differ-
ent experimental setups are compared. It was seen that the
ﬂow predictions are best when there is less emphasis on recon-
struction, due to denser repre sentations learned with recon-
struction that are not present i n the ground-truth ﬂow ﬁelds.
Compared with other optical ﬂow methods (Gunnar-Farne-
back, FlowNet2.0, FlowNet3.0-CSS, LiteFlowNet, and PWC-
Net), it was shown that the netw orks trained on the generated
dataset predict better ﬂow repre sentations, as quantiﬁed by the
ﬂow error metrics. This implie s that a network trained on good
face optical ﬂow ground-truth have the propensity to outper-
form networks trained on other datasets.
To investigate the performance of the different optical
ﬂow network variants in an FER application, the optical ﬂow
features were used to train STSTNet for micro-expression
detection. The experimental results using different perfor-
mance metrics were mixed, e.g., FlowNet3.0-CSS performed
better in a number of metrics. However, our method also
demonstrated promising results in a number of cases, partic-
ularly those on the SAMM dataset. Note that further
improvements and extension to this baseline work can help
improve its application to FER.
For further investigation and improvement, future work
related to this work can include the following:
1) Use a denser tracker such as Zface [61] to track a
higher number of key-points for a ﬁner triangulation
and denser optical ﬂow ground-truth in our auto-
matic data generation algorithm.
2) Use a more complex CNN architecture to train the
denser optical ﬂow ground-truth.
3) Train the optical ﬂow network on faces with some
head rotation, such as pan and tilt, to learn optical
ﬂow for non-frontal faces.
4) Tackle challenges in optical ﬂow learning, such as in
environments with occlusion and illumination, to
increase the robustness of facial optical ﬂow.
In addition to these improvements for optical ﬂow learn-
ing, the empirical analysis can be extended to evaluate the
performance of the face-trained optical ﬂow CNN in other
problems in facial expression analysis, such as action unit
recognition.
ACKNOWLEDGMENTS
We thank Ms. Florence Wacheux for her input in the review
of the manuscript draft.
REFERENCES
[1] S. S €oderkvist, K. Ohl /C19en, and U. Dimberg, “How the experience of
emotion is modulated by facial feedback,” J. Nonverbal Behav. ,
vol. 42, no. 1, pp. 129–151, Mar. 2018.[2] C. F. Benitez-Quiroz, R. B. Wilbur, and A. M. Martinez, “The not
face: A grammaticalization of facial expressions of emotion,” Cog-
nition , vol. 150, pp. 77–84, May 2016. [Online]. Available: http://
www.sciencedirect.com/science/article/pii/S0010027716300324
[3] S. M. Gillespie, P. Rotshtein, L. J. Wells, A. R. Beech, and I. J. Mitchell,
“Psychopathic traits are associated with reduced attention to the
eyes of emotional faces among adult male non-offenders,” Front.
Hum. Neurosci. , vol. 9, Oct. 2015, Art. no. 552. [Online]. Available:
https://www.frontiersin.org/article/10.3389/fnhum.2015.00552
[4] A. J. O’Toole, Psychological and Neural Perspectives on Human Face
Recognition , Berlin, Germany: Springer, 2005, pp. 349–369.
[5] X. Zhang et al., “BP4D-spontaneous: A high-resolution spontane-
ous 3D dynamic facial expression database,” Image Vis. Comput. ,
vol. 32, pp. 692–706, Jun. 2014.
[6] A. Dosovitskiy et al., “FlowNet: Learning optical ﬂow with convo-
lutional networks,” in Proc. IEEE Int. Conf. Comput. Vis. , 2015,
pp. 2758–2766. [Online]. Available: http://lmb.informatik.uni-
freiburg.de/Publications/2015/DFIB15
[7] P. Lucey, J. F. Cohn, T. Kanade, J. Saragih, Z. Ambadar, and I. Mat-
thews, “The extended cohn-kanade da taset (ck+): A complete dataset
for action unit and emotion-speciﬁed expression,” in Proc. IEEE Com-
put. Soc. Conf. Comput. Vis. Pattern Recognit. Workshops , 2010, pp. 94–101.
[8] S. Liong, Y. S. Gan, J. See, H. Khor, and Y. Huang, “Shallow triple stream
three-dimensional cnn (ststnet) for micro-expression recognition,” in
P r o c .1 4 t hI E E EI n t .C o n f .A u t o m .F a c eG e s t u r eR e c o g n i t . , 2019, pp. 1–5.
[9] B. D. Lucas and T. Kanade, “An iterative image registration tech-
nique with an application to stereo vision,” in Proc. 7th Int. Joint
Conf. Artiﬁcal Intell. , 1981, pp. 674–679. [Online]. Available:
http://dl.acm.org/citation.cfm?id ¼1623264.1623280
[10] M. R. BalazadehBahar and G. Karimian, “High performance
implementation of the horn and schunck optical ﬂow algorithm
on FPGA,” in Proc. 20th Iranian Conf. Elect. Eng. , 2012, pp. 736–741.
[11] S. Shah and X. Xuezhi, “Traditional and modern strategies for opti-
cal ﬂow: An investigation,” SN Appl. Sci. , vol. 3, pp. 1–14, 2021.
[12] J. Janai, F. G €u n e y ,A .R a n j a n ,M .B l a c k ,a n dA .G e i g e r ,“ U n s u p e r v i s e d
learning of multi-frame optical ﬂow with occlusions,” in Proc. 15th
Eur. Conf. Comput. Vis. , 2018, pp. 713–731.
[13] S. Meister, J. Hur, and S. Roth, “UnFlow: Unsupervised learning of
optical ﬂow with a bidirectional census loss,” Proc. AAAI Conf. Art.
Intell. , vol. 32, no. 1, pp. 7251–7259, 2018.
[14] Z. Ren, J. Yan, X. Yang, A. Yuille, and H. Zha, “Unsupervised
learning of optical ﬂow with patch consistency and occlusion
estimation,” Pattern Recognit. , vol. 103, 2020, Art. no. 107191.
[Online]. Available: https://www.sciencedirect.com/science/
article/pii/S0031320319304911
[15] D. Sun, S. Roth, and M. J. Black, “Secrets of optical ﬂow estimation
and their principles,” in Proc. IEEE Comput. Soc. Conf. Comput. Vis.
Pattern Recognit. , 2010, pp. 2432–2439.
[16] L. Tian, Z. Tu, D. Zhang, J. Liu, B. Li, and J. Yuan, “Unsupervised
learning of optical ﬂow with CNN-based non-local ﬁltering,”
IEEE Trans. Image Process. , vol. 29, pp. 8429–8442, 2020.
[17] X. Wang, R. Girshick, A. Gupta, and K. He, “Non-local neural
networks,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. ,
2018, pp. 7794–7803.
[18] Y. Niu, A. Dick, and M. Brooks, “Discontinuity-preserving optical
ﬂow computation by a dynamic overdetermined system,” in Proc.
9th Biennial Conf. Aust. Pattern Recognit. Soc. Digit. Image Comput.
Techn. Appl. , 2007, pp. 352–359.
[19] L. Alvarez, R. Deriche, T. Papadopoulo, and J. Snchez, “Symmetrical
dense optical ﬂow estimation with occlusions detection,” Int. J. Com-
put. Vis. , vol. 75, no. 3, pp. 371–385, Dec. 2007.
[20] E. Ilg, N. Mayer, T. Saikia, M. Keuper, A. Dosovitskiy, and T. Brox,
“FlowNet 2.0: Evolution of optical ﬂow estimation with deep
networks,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. , 2017,
pp. 1647–1655. [Online]. Available: http://lmb.informatik.uni-
freiburg.de/Publications/2017/IMSKDB17
[21] E. Ilg, T. Saikia, M. Keuper, and T. Brox, “Occlusions, motion and
depth boundaries with a generic network for disparity, optical
ﬂow or scene ﬂow estimation,” in Proc. Eur. Conf. Comput. Vis. ,
2018, pp. 626–643. [Online]. Available: http://lmb.informatik.uni-
freiburg.de/Publications/2018/ISKB18
[22] D. Sun, X. Yang, M. Liu, and J. Kautz, “PWC-Net: CNNs for optical ﬂow
using pyramid, warping, and cost volume,” in Proc. IEEE Conf. Comput.
Vis. Pattern Recognit. , 2018, pp. 8934 /C08943.
[23] T.-W. Hui, X. Tang, and C. C. Loy, “LiteFlowNet: A lightweight con-
volutional neural network for optical ﬂow estimation,” in Proc. IEEE
Conf. Comput. Vis. Pattern Recognit. , 2018, pp. 8981–8989. [Online].
Available: http://mmlab.ie.cuhk.edu.hk/projects/LiteFlowNet/2084 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022[24] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros, “Unpaired image-to-
image translation using cycle-consistent adversarial networks,” in
Proc. IEEE Intl. Conf. Computer Vis. , 2017, pp. 2242–2251.
[25] J. J. Yu, A. W. Harley, and K. G. Derpanis, “Back to basics: Unsuper-
vised learning of optical ﬂow via brightness constancy and motion
smoothness,” in Proc. Eur. Conf. Comput. Vis. , 2016, pp. 3–10.
[26] W.-S. Lai, J.-B. Huang, and M.-H. Yang, “Semi-supervised learning
for optical ﬂow with generative adversarial networks,” in Proc. 31st
Int. Conf. Neural Informat. Process. Syst. , 2017, pp. 353–363.
[27] M. Jaderberg, K. Simonyan, A. Zisserman, and K. Kavukcuoglu,
“Spatial transformer networks,” Adv. Neural Info. Process. Sys. ,v o l .2 8 ,
2015.
[28] P. Snape, A. Roussos, Y. Panagakis, and S. Zafeiriou, “Face ﬂow,”
inProc. IEEE Int. Conf. Comput. Vis. , 2015, pp. 2993–3001.
[29] H. Le, T. Nimbhorkar, T. Mensink, A. S. Baslamisli, S. Karaoglu,
and T. Gevers, “Unsupervised generation of optical ﬂow datasets
from videos in the wild,” 2018, arXiv:1812.01946 .
[30] B. Allaert, I. R. Ward, I. M. Bilasco, C. Djeraba, and M. Benna-
moun, “Optical ﬂow techniques for facial expression analysis: Per-
formance evaluation and improvements,” 2019, arXiv: 1904.11592 .
[31] M. R. Koujan, A. Roussos, and S. Zafeiriou, “Deepfaceﬂow: In-the-
wild dense 3D facial motion estimation,” in Proc. IEEE Conf. Comput.
Vis. Pattern Recognit. , 2020, pp. 6617–6626.
[32] C. Ma, L. Chen, and J. Yong, “Au r-cnn: Encoding expert prior
knowledge into r-cnn for action unit detection,” Neurocomputing ,
vol. 355, pp. 35–47, Aug. 2019. [Online]. Available: http://www.
sciencedirect.com/science/article/pii/S0925231219305338
[33] H. Yang and L. Yin, “Learning temporal information from a single
image for au detection,” in Proc. 14th IEEE Int. Conf. Autom. Face
Gesture Recognit. , 2019, pp. 1–8.
[34] Y. Li, J. Zeng, S. Shan, and X. Chen, “Self-supervised representation
learning from videos for facial action unit detection,” in Proc. IEEE/
CVF Conf. Comput. Vis. Pattern Recognit. , 2019, pp. 10924–10933.
[35] A. Romero, J. Le /C19on, and P. Arbel /C19aez, “Multi-view dynamic
facial action unit detection,” Image Vis. Comput. , vol. 122, 2018,
Art. no. 103723. [Online]. Available: http://www.sciencedirect.
com/science/article/pii/S0262885618301598
[36] W. Chu, F. De la Torre, and J. F. Cohn, “Learning spatial and tem-
poral cues for multi-label facial action unit detection,” in Proc.
12th IEEE Int. Conf. Autom. Face Gesture Recognit. , 2017, pp. 25–32.
[37] N. Perveen, D. Roy, and C. K. Mohan, “Spontaneous expression
recognition using universal attribute model,” IEEE Trans. Image
Process. , vol. 27, no. 11, pp. 5575–5584, Nov. 2018.
[38] Y. Gan, S.-T. Liong, W.-C. Yau, Y.-C. Huang, and L.-K. Tan, “Off-
ApexNet on micro-expression recognition system,” Signal Process.
Image Commun. , vol. 74, pp. 129–139, May 2019. [Online]. Available:
http://www.sciencedirect.com/science/article/pii/S0923596518310038
[39] M. Verburg and V. Menkovski, “Micro-expression detection in long
videos using optical ﬂow and recurrent neural networks,” in Proc.
14th IEEE Int. Conf. Autom. Face Gesture Recognit. , 2019, pp. 1–6.
[40] Q. Li, J. Yu, T. Kurihara, and S. Zhan, “Micro-expression analysis by
fusing deep convolutional neural network and optical ﬂow,” in Proc.
5th Int. Conf. Control, De cis. Informat. Technol. , 2018, pp. 265–270.
[41] X. Wang and A. Gupta, “Unsupervised learning of visual repre-
sentations using videos,” in Proc. IEEE Int. Conf. Comput. Vis. ,
2015, pp. 2794–2802.
[42] N. Dubey, S. Ghosh, and A. Dhall, “Unsupervised learning of eye
gaze representation from the web,” in Proc. IEEE Int. Joint Conf.
Neural Netw. , 2019, pp. 1–7.
[43] T. Baltrusaitis, P. Robinson, and L. Morency, “Constrained local
neural ﬁelds for robust facial landmark detection in the wild,” in
Proc. IEEE Int. Conf. Comput. Vis. Workshops , 2013, pp. 354–361.
[44] A. Zadeh, T. Baltrusaitis, and L. Morency, “Deep constrained local
models for facial landmark detection,” 2016, arXiv:1611.08657 .
[45] J. Gallier, Geometric Methods and Applications: For Computer Science
and Engineering , 2nd ed., Berlin, Germany: Springer, 2013.
[46] S. Yan, Z. Zhang, Y. Fu, Y. Hu, J. Tu, and T. Huang, “Learning a per-
son-independent representation for precise 3D pose estimation,” in
Multimodal Technologies for Perception of Humans , Berlin, Germany:
Springer, 2007, pp. 297–306.
[47] C. Ericson, Real-Time Collision Detection . Boca Raton, FL, USA: CRC
Press, 2004.
[48] D. J. Butler, J. Wulff, G. B. Stanley, and M. J. Black, “A naturalistic
open source movie for optical ﬂow evaluation,” in Proc. 12th Eur.
Conf. Comput. Vis. , 2012, pp. 611–625.
[49] P. J. Huber, “Robust estimation of a location parameter,” Ann.
Math. Statist. , vol. 35, no. 1, pp. 73–101, Mar. 1964.[50] S. Baker, D. Scharstein, J. P. Lewis, S. Roth, M. J. Black, and R. Sze-
liski, “A database and evaluation methodology for optical ﬂow,”
Int. J. Comput. Vis. , vol. 92, no. 1, pp. 1–31, Mar. 2011.
[51] W.-J. Yan et al., “Casme II: An improved spontaneous micro-
expression database and the baseline evaluation,” PLoS One ,
vol. 9, 2014, Art. no. e86041.
[52] A. K. Davison, C. Lansley, N. Costen, K. Tan, and M. H. Yap,
“SAMM: A spontaneous micro-facial movement dataset,” IEEE
Trans. Affec. Comput. , vol. 9, no. 1, pp. 116–129, Jan./Mar. 2018.
[53] X. Li, T. Pﬁster, X. Huang, G. Zhao, and M. Pietik €ainen, “A spon-
taneous micro-expression database: Inducement, collection and
baseline,” in Proc. 10th IEEE Int. Conf. Workshops Autom. Face Ges-
ture Recognit. , 2013, pp. 1–6.
[54] S. Liong, J. See, K. Wong, A. C. Le Ngo, Y. Oh, and R. Phan,
“Automatic apex frame spotting in micro-expression database,”
inProc. 3rd IAPR Asian Conf. Pattern Recognit. , 2015, pp. 665–669.
[55] D. E. King, “Dlib-ml: A machine learning toolkit,” J. Mach. Learn.
Res., vol. 10, pp. 1755–1758, 2009.
[56] V. Kazemi and J. Sullivan, “One millisecond face alignment with
an ensemble of regression trees,” in Proc. IEEE Conf. Comput. Vis.
Pattern Recognit. , 2014, pp. 1867–1874.
[57] M. H. Yap, J. See, X. Hong, and S. Wang, “Facial micro-expres-
sions grand challenge 2018 summary,” in Proc. 13th IEEE Int. Conf.
Autom. Face Gesture Recognit. , 2018, pp. 675–678.
[58] M. Sokolova and G. Lapalme, “A systematic analysis of perfor-
mance measures for classiﬁcation tasks,” Informat. Process. Man-
age., vol. 45, no. 4, pp. 427–437, 2009. [Online]. Available: http://
www.sciencedirect.com/science/article/pii/S0306457309000259
[59] D. Gholamiangonabadi, N. Kiselov, and K. Grolinger, “Deep neu-
ral networks for human activity recognition with wearable sen-
sors: Leave-one-subject-out cross-validation for model selection,”
IEEE Access , vol. 8, pp. 133 982–133 994, 2020.
[60] G. Farneb €ack, “Two-frame motion estimation based on polynomial
expansion,” in Proc. 13th Scand. Conf. Image Anal. , 2003, pp. 363–370.
[61] L. A. Jeni, J. F. Cohn, and T. Kanade, “Dense 3D face alignment
from 2D video for real-time use,” Image Vis. Comput. , vol. 58,
pp. 13–24, 2017. [Online]. Available: http://zface.org
Muhannad Alkaddour received the MS degree
from the Mechatronics Engineering Graduate Pro-
gram, the American University of Sharjah, in 2020.
His research experience and interests include arti-
ﬁcial intelligence, with emphasis on deep learning
and computer vision, as well as robotics, control
systems, and dynamical systems.
Usman Tariq (Member, IEEE) received the PhD
degrees in electrical and computer engineering
from the University of Illinois at Urbana-Cham-
paign, in 2013. He is a faculty member with the
Department of Electrical Engineering, American
University of Sharjah, UAE. He has also worked
as a research scientist with Computer Vision
Group, Xerox Research Center Europe, France.
His research interests include computer vision
and affective computing.
Abhinav Dhall (Member, IEEE) received the PhD
degree in computer science from the Australian
National University , Canberra, Australia, in 2014.
He currently leads the Centre for Applied Research
in Data Sciences, Indian Institute of Technology
Ropar , India, and also an adjunct senior lecturer
with Monash University , Australia. His research in-
terests include computer vision and affective com-
puting. He was awarded the Best Doctoral Paper
Award for ACM ICMR 2013, Best Student Paper
Honourable mention for IEEE 1372AFGR, and
Best Paper Nomination for IEEE ICME 2012. He is an associate editor of
theIEEE Transactions on Affective Computing .
"For more information on this or any other computing topic,
please visit our Digital Library at www.computer .org/csdl.ALKADDOUR ET AL.: SELF-SUPERVISED APPROACH FOR FACIAL MOVEMENT BASED OPTICAL FLOW 2085Modeling Real-World Affective and
Communicative Nonverbal Vocalizations
From Minimally Speaking Individuals
Jaya Narain , Kristina T . Johnson, Thomas F . Quatieri ,Fellow, IEEE ,
Rosalind W . Picard ,Fellow, IEEE , and Pattie Maes ,Member, IEEE
Abstract— Nonverbal vocalizations from non- and minimally speaking individuals who speak fewer than 20 words (mv* individuals)
convey important communicative and affective information. While nonverbal vocalizations that occur amidst typical speech and infant
vocalizations have been studied extensively in the literature, there is limited prior work on vocalizations by mv* individuals. Our work is
among the ﬁrst studies of the communicative and affective information expressed in nonverbal vocalizations by mv* children and adults.
We collected labeled vocalizations in real-world settings with eight mv* communicators, with communicative and affective labels
provided in-the-moment by a close family member . Using evaluation strategies suitable for messy , real-world data, we show that
nonverbal vocalizations can be classiﬁed by function (with 4- and 5-way classiﬁcations) with F1 scores above chance for all participants.
We analyze labeling and data collection practices for each participating family, and discuss the classiﬁcation results in the context of our
novel real-world data collection protocol. The presented work includes results from the largest classiﬁcation experiments with nonverbal
vocalizations from mv* communicators to date.
Index Terms— Affective computing, affect sensing and analysis, nonverbal speech, speech analysis
Ç
1I NTRODUCTION
INthe United States alone, there are over one million peo-
ple who are non- or minimally speaking with respect to
verbal language [1], [2], [3]. Here we focus on a subset of
this population, abbreviated as mv*, who have fewer than
20 words or word approximations and limited expressive
language through speech and writing. This includesindividuals with Autism, in addition to some individuals
with Down Syndrome, Rett Syndrome, Mowat-Wilson,
Rubinstein-Taybi syndrome, Pitt-Hopkins syndrome, and
other conditions associated with differences in speech and
language. Mv* individuals communicate richly through
many means including augmentative and alternative com-
munication (AAC) devices, gestures, and vocalizations.
Family members and those close to mv* individuals report
that nonverbal vocalizations (i.e., vocalizations that do not
have typical verbal content) from mv* individuals often have
self-consistent phonetic content and may vary in tone, pitch,
and duration depending on the individual’s emotional state
or intended communication. While these vocalizations con-
tain important affective and communicative information and
are understood by close family and friends, they are often
poorly understood by those who don’t know the communica-
tor well. Improved understanding of nonverbal vocalizations
could contribute to the development of technology to aug-
ment communication [4], enhance understanding of nonver-
bal affective expressions broadly, and expand awareness
around this form of communication.
Studying nonverbal vocalizations with mv* individuals
has unique challenges. Mv* individuals are a small, hetero-
geneous, and geographically distributed population. The
population of mv* communicators includes individuals
with diverse and multiple diagnoses; many individuals also
have co-occurring intellectual disabilities and other chal-
lenges like epilepsy. Moreover, studying vocalizations with
this population requires a ﬂexible and thoughtful study
design to minimize time burden on families.
Additionally, affective and communicative vocalizations
are motivation-driven and cannot be easily elicited in/C15Jaya Narain, Kristina T. Johnson, Rosalind W. Picard, and Pattie Maes are
with the Massachusetts Institute of Technology, Cambridge, MA 02139
USA. E-mail: jnarain8@gmail.com, ktj@mit.edu, {picard, pattie}@media.
mit.edu.
/C15Thomas F. Quatieri is with MIT Lincoln Laboratory, Lexington, MA
02421 USA. E-mail: quatieri@ll.mit.edu.
Manuscript received 6 September 2021; revised 22 June 2022; accepted 4 Sep-
tember 2022. Date of publication 21 September 2022; date of current version
15 November 2022.
Approved for public release. Distribution is unlimited. This material is based
upon work supported by the Under Secretary of Defense for Research and
Engineering under Air Force Contract No. FA8702-15-D-0001. Any opinions,
ﬁndings, conclusions or recommendations expressed in this material are those
of the author(s) and do not necessarily reﬂect the views of the Under Secretary
of Defense for Research and Engineering.
This work was supported in part by the MIT Media Lab Consortium and the
Deshpande Center Technology to Improve Ability Program. The work of Jaya
Narain was supported by Apple Scholars in AI/ML and the NSF Graduate
Research Fellowship Program. The work of Kristina Johnson was supported by
the Hugh Hampton Young Fellowship Program.
This work involved human subjects in its research. Approval of all ethical and
experimental procedures and protocols was granted by MIT Committee on the
Use of Humans as Experimental Subjects Application No. 1903760614.
(Corresponding author: Jaya Narain.)
Recommended for acceptance by J. Epps.
This article has supplementary downloadable material available at https://doi.
org/10.1109/TAFFC.2022.3208233, provided by the authors.
Digital Object Identiﬁer no. 10.1109/TAFFC.2022.32082332238 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022
1949-3045 ©2022 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See ht _tps://www.ieee.org/publications/rights/index.html for more information.
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:58 UTC from IEEE Xplore.  Restrictions apply. unnatural lab settings. Real-world data collection is critical to
capturing vocalizations as they are used organically to
express affect and communication. Naturalistic studies often
recreate real-life environments and activities in laboratories,
but still involve unfamiliar settings and people which might
induce anxiety and alter emotional expressions, particularly
for mv* individuals who can be sensitive to new sensory
environments and experiences [5]. Obtaining ground-truth
labels is also an unsolved problem. Many mv* communica-
tors cannot directly provide word-based labels, and external
annotators do not have the deep ﬁrsthand experience needed
to interpret these vocalizations. In our prior work [4], we
developed a novel longitudinal data collection process to col-
lect real-world audio with in-the-moment labels provided by
a close family member or caregiver. Here we extend our pre-
vious work, presenting new analytical approaches applied
to a larger number of vocalizations from more individuals.
In this paper, we present the results of the largest real-
world nonverbal vocalization classiﬁcation experiments to
date with vocalizations by eight mv* communicators. We
show that nonverbal vocalizations can be classiﬁed using
audio alone for each individual. We present evaluation and
sampling strategies to work with messy, real-world data
with uneven sample distributions and varying background
noise. We implement and evaluate a custom feature set
designed for nonverbal vocalizations for mv* individuals.
We also analyze the data collection and labeling practices
for each participant, and discuss model performance in the
context of how data was collected by each participant.
2R ELATED WORK
Prior studies have not focused on mv* individuals speciﬁ-
cally, but have studied neurodiverse individuals and/or
individuals with developmental differences more generally.
Our work is unique in its focus on mv* individuals.
2.1 Affect With Neurodiverse Populations
Prior work on affect recognition with neurodiverse populations
has focused on the emotional content of verbal speech [6], [7]
and facial expressions [8], [9]. These studies have focused on
individuals who communicate using verbal speech, and not on
mv* communicators. In this work, we use the term verbal speech
to specify speech with typical verbal content, which is different
from nonverbal speech which is also richly expressive and com-
municative (as from mv* communicators) but may not contain
verbal content like words or phrases. Prior work has also exam-
ined the relation between affect and physiological signals like
electrodermal activity (EDA) an d electrocardiography (ECG)
[10], [11], [12] in neurodiverse populations. Picard explored
using EDA to augment emotional communication with indi-
viduals with autism, and suggested approaches for integrating
sensors that record autonomic ne rvous system activation with
emotional communication [11]. Kushki et al. explored using an
electrocardiogram (ECG) to detect anxiety-related arousal in
children with autism [13]. While physiological studies can pro-
vide valuable insight into aff ective expression, they often
require wearing uncomfortable sensors and may be difﬁcult to
interpret in real-world settings where signals can be affected
by many factors. Studying voca lizations with neurodiverse
populations is important towards expanding inclusivecommunication and enhancing u nderstanding on how affect is
expressed by diverse populations.
2.2 Other Communication Modalities
Prior work has explored the development and usage of forms
of communication, including gestural communication and
AAC usage [14], [15], [16], [17], [18], [19], [20]. Like nonverbal
vocalizations, these forms of communication are highly
expressive and communicative, yet they are less commonly
used by the general population. These communication modal-
ities may be personalized to the communicator, and – like any
highly individualized communication approach – can require
time investment from the communicator and/or the commu-
nication partner to learn how to express or receive communi-
cation with a given modality [21]. Nonverbal vocalizations
are one component of communication from mv* individuals,
and it is important to note that these vocalizations occur along
with other communication modalities, which may include
AAC as well as non-vocal communication like body move-
ments and gaze [21], [22], [23].
2.2.1 AAC Usage
Prior work has highlighted the effectiveness and importance
of AAC as a communication modality [24], [25]. Couper et al.
studied the use of three types of AAC devices (picture
exchange, manual signs, and tablet-based speech generation)
with eight Autistic children and found that most of the chil-
dren preferred the tablet-based device [21] and that children
were able to learn to use an AAC device to request preferred
stimuli more quickly when using a preferred modality.
While AAC is a rich and important communication modality
– and one that should be treated equivalently to other types
of communication like verbal speech – not all mv* individu-
als use AAC devices and mv* individuals’ AAC vocabularies
may vary signiﬁcantly between one another.
2.2.2 Non-Vocal Communication
Non-vocal communication - including body movements,
posturing, gestures, and eye gaze - have been shown to con-
vey affect in both typical and neurodiverse populations [22],
[23], [26] though non-vocal communicative expression and
reception may differ for neurodiverse individuals [20], [22],
[26], [27]. Stone et al. studied non-vocal communication like
gestures and eye contact in two- and three-year old children,
with the goal of identifying differences in communication
styles between children with autism and typically develop-
ing children [14]. These researchers observed that Autistic
children in the study were more likely to communicate by
manipulating the communication partner’s hand but less
likely to communicate using eye gaze and pointing than typi-
cally developing children (n=28). In a retrospective video
study, Gordan et al. found a correlation between gestural use
and language outcomes among toddlers identiﬁed as having
a higher likelihood of having ASD (n=42), with less gestural
use at age 13-15 months being associated with lower expres-
sive and receptive language outcomes at 20-24 months [15].
Recent studies by Wilson et al. explored interactive com-
munication with neurodiverse individuals [28], [29]. These
researchers developed and tested ExpressiBall, a ball with
lights, sound, and motion sensors, to study self-expressionNARAIN ET AL.: MODELING REAL-WORLD AFFECTIVE AND COMMUNICATIVE NONVERBAL VOCALIZATIONS FROM MINIMALLY SPEAKING... 2239
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:58 UTC from IEEE Xplore.  Restrictions apply. and co-design with minimally verbal Autistic children. The
research team identiﬁed six self-expression modalities:
Words, Sound, Bodily Movements, Touch and Gestures,
Creativity, and Play. The ExpressiBall encouraged expres-
sion and communication through multiple modalities, and
their results emphasized that researchers and others should
listen and respond to all modalities of expressions [29]. In
another study, Wilson et al. used a similar device to identify
‘moments of interaction’ during which minimally verbal
children communicated in ways that extended beyond
words [28]. While the research presented here focused on
nonverbal vocalizations, understanding that these vocaliza-
tions occur as part of a complex communication system is
important to contextualizing this work and in developing
avenues for further exploration.
2.3 Clinically Oriented Studies on Vocalizations
Nonverbal vocalizations – particulary in infants and young
typically developing children – have been studied exten-
sively as part of language development [30], [31], [32], [33],
[34], [35], [36], [37]. Donnellan et al. experimentally studied
the relation between prelinguistic vocalizations in infants
and language development trajectories for typically devel-
oping children [31]. McDaniel et al. conducted a meta-anal-
ysis on the relationship between prelinguistic vocalizations
and expressive language development in children with
autism [32]. Bacon et al. [36] created a large naturalistic
dataset by manually coding toddler speech in clinic-visit
videos to study language development of toddlers with and
without autism.
Researchers have explored using nonverbal vocalizations
to diagnose autism using infant cries [38] and naturalistic
child vocalizations [35], [39], and as a marker for other
developmental differences like Fragile X syndrome [40],
Down Syndrome [41], and speciﬁc language impairments
(SLI) [42].
Studies with specialized populations primarily take
place in laboratory and clinical settings, and often attempt
to elicit vocalizations from participants. In a study with
twenty-four children with autism, Chiang et al. found that
children produced more spontaneous communication in
natural environments than e licited communication and
that spontaneous communication had different uses (e.g.,
requesting) [43]. Oller et al. conducted one of the only
known studies of nonverbal voc alizations from non-typi-
cally developing children in real-world environments, but
focused on diagnosis tasks using toddler speech not on
vocalization affect or intent [35]. Tools that track and
enhance communication with mv* individuals in real-
w o r l ds e t t i n g s ,w h i c hm a yp r o v i d ea f f e c t i v ea n dc o m m u -
nicative vocalization data not captured by laboratory tests
[44], are largely unexplored.
2.4 Nonverbal Vocalizations as Communication
Nonverbal vocalizations include both involuntary (e.g.,
coughing, hiccupping) and voluntary (e.g., grunting, sigh-
ing, screaming) sounds. Nonverbal vocalizations often
occur amidst typical verbal speech and can be used to con-
vey an emotion, express intention, and emphasize verbal
speech [45], [46], [47].2.4.1 As an Expression of Affect Amidst Typical
Verbal Speech
Nonverbal vocalizations that occur alongside typical lan-
guage have been studied anthropologically [48], [49] and
have been classiﬁed affectively with both natural and acted
vocalizations across numerous studies [45], [50], [51]. Trou-
vain and Truong categorized types and usages of nonverbal
vocalizations and identiﬁed ﬁve primary types of nonverbal
vocalizations: vegetative sounds (e.g., snoring), affective
sounds (e.g., laughter), interjections as semi-words (e.g.,
”shh”), ﬁller sounds as semi-words (e.g., ”uhm”), and
melodic utterances (e.g., humming) [52].
Holz et al. found that listeners could reliably identify
intensity and arousal in nonverbal vocalizations, but that
emotions expressed with maximal intensity were more difﬁ-
cult to categorize than more moderately expressed emotions
[50]. Schroder et al. also found that listeners could reliably
identify acted nonverbal vocalizations expressing ’affect
bursts’ across ten categories [53]. Sauter et al. found that
vocalizations communicating some basic emotions (anger,
disgust, fear, joy, sadness, and surprise) were recognized
cross-culturally by both individuals from Western countries
and from isolated villages in Namibia [48]. Anikin found
that listeners could reliably differentiate between acted and
authentic affective nonverbal vocalizations [49], and identi-
ﬁed correlations between voice quality and valence in non-
verbal vocalizations [51].
2.4.2 As Expressive Communication in Infants
Nonverbal vocalizations have been studied as expressions
of affect and communication from infants. In 1964, Wasz-
H€ockert identiﬁed speciﬁc meanings – pain, pleasure, and
hunger – in infant vocalizations in a study with trained
nurses in a hospital [54]. Since then, there has been exten-
sive work on classifying infant cries by need (e.g., hunger,
pain) using both humans and machines [55], [56], [57], [58].
Weisman et al. studied the dynamics of vocalizations dur-
ing infant-father interactions and found that vocalizations
played a signiﬁcant role in regulating social interactions
[59]. Recently, Liu et al. used linear predictive coding (LPC),
linear predictive cepstral coefﬁcients (LPCC), Bark fre-
quency cepstral coefﬁcients (BFCC), and Mel frequency
cepstral coefﬁcients (MFCC) to classify infant cries as being
related to hunger, sleepiness, needing a diaper change, a
need for attention, or general discomfort [55].
2.4.3 Lack of Studies of Nonverbal Vocalizations as
Communication Independent of Typical
Verbal Speech
While researchers like Beukelman and Mirenda have noted
that mv* individuals use vocalizations to express emotions
and communicate, systematic study and tools that can work
with these expressions remain undeveloped. For people
who are mv*, these nonverbal vocalizations serve a unique
linguistic and communicative purpose as they occur inde-
pendently of verbal speech [60]. These vocalizations include
traditional nonverbal cues (e.g., laughter or yells), as well as
unique utterances of varying pitch, phonetic content, and2240 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:58 UTC from IEEE Xplore.  Restrictions apply. tone that do not fall into the typical categories of nonverbal
vocalizations.
To our knowledge, no other studies have acquired non-
verbal vocalizations from mv* individuals using personal-
ized labels in real-world settings. In our prior work, we
describe in detail the participatory design process used to
design our approach [61], our novel data collection system
[4], [62], and provide preliminary classiﬁcation results with
three mv* communicators [4]. Here we extend this work to
include new analytical approaches for classiﬁcation with
eight mv* communicators, and discuss our experimental
results in the context of real-world data collection.
3M ETHODS
3.1 Data Collection and Pre-Processing
Participants were recruited through conversations with com-
munity members and word-of-mouth. The study was
approved by the MIT institutional review board (IRB). Data
was collected as described in [4] and [63] and the dataset
used in experiments, ReCANVo (Real-World Communica-
tive and Affective Nonverbal Vocalizations), is presented
fully in [63]. The study was conducted entirely remotely, in
order to reach a geographically distributed population and
minimize time burden. The remote nature of the study
enabled data collection even during COVID-19. Data were
collected in communicators’ natural environments, primar-
ily in and around the home. Participants were encouraged to
go about their typical day-to-day activities while recording.
Background and demographic information (provided by
families via a web survey) for participating mv* communi-
cators are in Table 1. Additional information on nonverbal
communication practices by each participant is summarized
in [64] and [65], including details on communication modal-
ities and vocal communication for each participant. In our
study, ﬁve of the eight participants used AAC with two par-
ticipants reporting an AAC vocabulary size of twenty
words/phrases or greater. In the survey, respondents also
reported that the mv* individuals communicated via ges-
tures, picture exchange, and hand leading or pulling.
Audio was recorded using a small recorder (Sony IDC-
TX800, recording in 16-bit 44.1 kHz stereo) attached to the
communicator’s clothing (P01, P02, P03, P06, P11, P16),
worn as a necklace (P08) or placed nearby (P05). Recorder
placement was ﬂexible to accommodate tactile sensitivities.
Vocalizations were labeled in-the-moment by a close familymember (i.e., a ”labeler”) using a custom app (Fig. 1). The
labeling process was designed to be highly ﬂexible so that it
could integrate into day-to-day life. Labelers provided
labels when circumstances allowed – often in short clusters.
Labelers were instructed to only label when they could do
so conﬁdently. In [65] and [61], we provide additional infor-
mation on how the labeling system was designed and vali-
dated. The labeler used the app to designate both a start
and end time for each vocalization, as accurately as possi-
ble. The app had six labels that were shared by all families,
that were described to the family as below:
/C15Self-talk: Vocalizations that appear to be associated
with being content, happy, or relaxed, and are gener-
ally made without apparent communicative intent.
(The individual seems to be making them to him/
herself.)
/C15Dysregulated: Vocalizations that appear to be associ-
ated with being irritated, upset, agitated, bored,
uncomfortable, overstimulated, or distressed. These
vocalizations are often (but not always) made with-
out an apparent speciﬁc communicative intent.
/C15Delighted: Vocalizations that appear to be associated
with being excited, very happy, or states of glee.TABLE 1
Demographic and Background Information for Participating mv* Communicators
Participant ID Gender Age (years) Conditions affecting
speech and/or languageTime span of
included data (weeks)Number of spoken words or
word approximations (parent report)
P01 M 18-25 Autism, Down syndrome (DS) 64 0
P02 M 18-25 Autism 7 4
P03 M 6-9 Autism, Genetic disorder 56 0
P05 F 9-12 Autism 11 0
P06 M 9-12 Autism, Cerebral Palsy (CP) 4 3
P08 F 6-9 Autism 20 0
P11 M 9-12 CP 19 1
P16 M 6-9 Autism 10 5-8
Participants who enrolled in the study but did not collect enough data were not included in the analysis.
Fig. 1. Custom app for in-the-moment labeling of nonverbal vocalizations
[4]. A) Main labeling screen B) Partial list of preset options.NARAIN ET AL.: MODELING REAL-WORLD AFFECTIVE AND COMMUNICATIVE NONVERBAL VOCALIZATIONS FROM MINIMALLY SPEAKING... 2241
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:58 UTC from IEEE Xplore.  Restrictions apply. /C15Frustrated: Vocalizations that appear to be associ-
ated with being frustrated, angry, or protesting.
/C15Request: Vocalizations that appear to be associated
with making a request.
/C15Social: Vocalizations that appear to be social in
nature (e.g., as part of a back-and-forth vocal
exchange while playing a game).
These six labels and descriptions were selected based on
conversations with families of mv* communicators and a
speech and language pathologist (SLP). The labels span
both affective and communicative functions. The labels
include positive valence classes (”delighted”, ”selftalk”),
negative valence classes (”frustrated”, ”dysregulated”),
higher arousal classes (”delighted”, ”frustrated”) and lower
arousal classes (”selftalk”, ”dysregulated”). Labels empha-
sizing communicative functions (e.g., ”request” and
”social”) were included because they capture a critical
dimension of how nonverbal vocalizations are used func-
tionally by this population. Importantly, social vocalizations
were only labeled as such if they 1) overtly social in nature,
such as a back-and-forth vocal exchange while playing a
game, and 2) could not be better described by a different
label. For example, requesting something is an inherently
social exchange, but these vocalizations would be labeled as
”Request”. As in verbal speech, nonverbal vocalizations
from mv* individuals can express complex meanings span-
ning both affective and communicative functions (e.g., a
”social” vocalization might also express happiness and
excitement about an interaction, and a ”frustrated” vocali-
zation might be used to communicate that a request has not
been adequately met). In the study, families were asked to
select the label that they felt would help others best under-
stand how to respond to a vocalization.
Families could also customize four additional labels,
by selecting from a drop-down list of twenty-ﬁve morespeciﬁc preset options (e.g., ”hungry”,”greeting”). The
app and data collection process were created using a
longitudinal participatory design process [61]. The study
was designed to give participants ﬂexibility to set the
pace, settings, and schedule for data collection. This ﬂex-
ibility resulted in variability in data collection and label-
ing practices between participants, but was critical in
enabling a ﬁrst-of-its-kind real-world data collection
with mv* communicators.
The collected audio recordings were pre-processed as
described in [4], [63] to extract labeled vocalizations. Vol-
ume-based segmentation was used to ﬁnd audio segments of
interest. Because the recorder was placed close to the com-
municator, vocalizations were generally louder than other
content in the recording. The recorded audio was temporally
aligned with label timestamps. Label matching accounted
for human delay in labeling and small clock shifts (see [63]
and [65] for details on the alignment and segmentation pro-
cess). Segments that were not temporally near labels were
discarded. A researcher listened to each segment to ensure it
contained a vocalization from the communicator. As needed,
the researcher trimmed noise surrounding the vocalization,
leaving approximately 0.15s of buffer at the beginning and
end of a vocalization. Fig. 2 shows the number of samples of
each vocalization type collected from each participant.
3.2 Analysis of Labeling and Data
Collection Practices
Exit interviews were conducted with seven of the eight
labelers (the labeler for P02 was not available for an inter-
view) to understand the ﬁdelity of the labeling scheme.
Labeling and data collection practices were tabulated for
each participant including labeling delay, average recording
session length, number of uploaded sessions, average label
Fig. 2. Number of collected nonverbal vocalization samples per label for each mv* communicator .2242 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:58 UTC from IEEE Xplore.  Restrictions apply. duration (using the label start and end time), average num-
ber of vocalizations per label, average number of labels per
session, and median number of unique labels per session
(results are summarized in Table 5). The labeling delay was
deﬁned the time passed between the start of a vocalization
and the label button push on the app to associated with that
vocalization. To minimize the effect of any small clock drifts
between the recorder and labeling app, only the ﬁrst two
weeks of labeling data were used in calculating the labeling
delay. For each participant, the range of delays were similar
at the start and end of the two-week period.
3.3 Machine Learning Evaluation and
Sampling Strategies
Throughout the evaluation and results sections the term
”sample” is used to refer to a labeled vocalization used for
model training or evaluation. Because of the heterogeneity
of the participants (Table 1), and known differences in non-
verbal vocal expressions between mv* individuals, person-
alized models were trained for each participant. Rough
session stratiﬁcation was used to reduce ﬁtting to back-
ground noise. For a given label and participant, the distribu-
tion of labels across sessions were often skewed. To avoid a
model learning to associate a label with the soundscape
from a dominant session, the maximum number of vocaliza-
tions having the same label per session per participant was
limited to 10 via random undersampling. To illustrate this,
Fig. 3 shows the distribution of P02’s vocalizations labeled
as ”social” by session before and after rough session stratiﬁ-
cation. Models were trained and evaluated with and with-
out rough session stratiﬁcation.
Models were evaluated using two strategies: 5-fold cross
validation and a leave one session out (LOSO) evaluation. A
session was a single uploaded recording from a participant
and contained many vocalizations. Sessions were time-sepa-
rated and correlated to a day or a speciﬁc activity. Back-
ground soundscapes between sessions were generally
distinct from each other. The LOSO approach was imple-
mented to further prevent model ﬁtting to the background
soundscapes of the vocalization recordings. The 5-fold cross
validation evaluations were run with 3 distinct random
seeds. The average metric and 95% conﬁdence interval for5-fold cross validation are reported using each fold and ran-
dom seed.
Within each fold/session, the outer loop (or held out ses-
sion) was used for evaluation and the inner loop was used
for regularization parameter selection. The training data in
each loop was balanced to the minimum of twice the smallest
class size and the largest class size using random downsam-
pling and the synthetic minority oversampling technique
(”SMOTE”) [66] (Fig. 4). SMOTE creates synthetic data points
by selecting randomly along lines in the feature space
between nearest neighbor points in the minority classes [66].
In each loop, the oversampler was ﬁt only on the training
data for that loop. This sampling scheme ensured that no
class had more synthetic than real data and that, in each loop,
at least one class has no synthetic data. This bound was
selected empirically after systematic experimentation with
different allowed synthetic class proportions for its consis-
tency in performance and interpretability across loops with
varying class distributions.
Tables 2 and 3 show the number of training samples per
class per fold for each evaluation strategy. The classes were
balanced within each fold. Because of the difference in vocali-
zation distribution between sessions, the balanced per class
training size varies between folds for the LOSO evaluation
strategy. Because the test dataset is not balanced, every sam-
ple is used as a test sample exactly once. Reported metrics are
macro-averaged across classes to weigh each class equally.
3.4 Multi-Class Classiﬁcation
Classes were selected for analysis for each participant based
on the number and distribution of samples. The selected
classes for each participant are shown in gray in Tables 2
and 3. For a participant, a label was included in the analysis
if there were at least 30 vocalization samples spread across
at least 3 sessions. These thresholds were chosen because
they allowed for inclusion of at least 4 labels per participant,
while maintaining sample volume (by requiring at least 30
samples) and diversity (by requiring the samples to come
from at least 3 recording sessions). Additionally, in this
Fig. 3. Distribution of P02’s ”social” vocalizations by session before and
after rough session stratiﬁcation.
Fig. 4. The class sizes were balanced in each fold using random down-
sampling and the synthetic minority oversampling technique (SMOTE).
In each fold, the number of training samples per class was balanced to
the minimum of twice the smallest class size and the largest class size.
The ﬁgure illustrates the balancing strategy within a fold, using pseudo
class sizes. The test data was not balanced, so metrics are reported
using macro-averages to weigh each class equally .NARAIN ET AL.: MODELING REAL-WORLD AFFECTIVE AND COMMUNICATIVE NONVERBAL VOCALIZATIONS FROM MINIMALLY SPEAKING... 2243
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:58 UTC from IEEE Xplore.  Restrictions apply. analysis, if a participant had two closely related labels (e.g.,
”delighted” and ”happy”), only one was included in the
model because of the increased difﬁculty of differentiating
between closely overlapping categories. As more samples
are collected, models could be trained for more ﬁne-grained
classiﬁcation. Multi-class models were trained using four or
ﬁve classes for each participant.
3.5 Binary Valence Classiﬁcation
Binary valence classiﬁcation experiments were conducted to
evaluate the model performance on larger meta-classes.
”Delighted” and ”selftalk” vocalizations were merged into a
positive valence class and ”dysregulated” and frustrated”
samples were merged into a negative valence class. The
selected classes, number of training samples per class, and the
number of training samples per class per fold with rough ses-
sion stratiﬁcation are given in Table 2. Binary valence models
were only evaluated with LOSO and rough session stratiﬁca-
tion, the most conservative evaluation approach.
Only binary valence experiments were conducted
because mapping to binary arousal states was not well-
deﬁned for the selected labels. The labels included lower
and higher arousal states within a particular valence:
”dysregulated” is generally lower arousal than ”frustrated”
and ”selftalk” is generally lower arousal than ”delighted”.
Within a valence, the arousals have a general relative rela-
tionship but are not necessarily independently ”low” or
”high”. For instance, for some participants, ”dysregulated”
may be associated with high arousal and ”frustrated” may
be associated with very high arousal. There is not a clear
distinction in relative arousals between ”dysregulated” and
”delighted” and between ”selftalk” and ”frustrated”. Addi-
tionally, labelers were not asked to consider broad arousal
mappings among labels.
3.6 Feature Extraction and Modeling
Experiments were conducted with Random Forest models,
a support vector machine (SVM) model with a radial basisfunction (RBF) kernel, and a linear SVM with stochastic gra-
dient descent (SGD) training. These models were selected
after experimenting with a broader selection of models and
based on their use in the literature for similar classiﬁcation
tasks. Models were evaluated with statistical features aggre-
gated for each vocalization, bag-of-phones features, and
data-learned features extracted using auDeep [67].
Aggregate features were extracted for each vocalization
with the following feature sets:
/C15the extended Geneva minimalistic acoustic parame-
ter set extracted using openSMILE [68], size 88 [69]
/C15a custom feature set, size 63
/C15mean mel frequency cepstral coefﬁcients (MFCC),
size 13
/C15mean gammatone cepstral coefﬁcients (GTCC),
size 13
/C15means of a mel-base ﬁlter bank applied to a short-
time Fourier transform (STFT), size 40
/C15means of an ERB-based ﬁlter bank applied to a STFT,
size 40
Table 4 lists the features included in the custom feature
set. The custom feature set was designed using the research
team’s observations, feedback from practicing speech and
language pathologists, and prior work with related classiﬁ-
cation tasks [70], [71]. Consulted speech and language path-
ologists suggested that the prosody of the vocalizations
might be particularly informative in nonverbal vocaliza-
tions. As such, the custom feature set includes features that
capture characteristics of the pitch contour like number of
peaks and polynomial ﬁt coefﬁcients. Parents of mv* com-
municators suggested that the phonetic content of vocaliza-
tions might vary with usage and so the custom feature set
applies a functional that extracts the longest constant value
of the ﬁrst and second formants, which are related to the
vowel content of a vocalization. The custom set also
includes mean GTCC and BFCC values (which have been
used in the related task of cry classiﬁcation [55]), the audio
amplitude duration and mean auto-correlation, andTABLE 2
Number of Samples for Each Class for Each Participant With Rough Session Stratiﬁcation2244 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:58 UTC from IEEE Xplore.  Restrictions apply. functionals applied to power, cepstral peak prominence
(related to voice quality), and harmonics.
The custom feature set includes features used in auto-
matic speech recognition that are related to lexical content
(e.g., cepstral coefﬁcients), features with a clear interpretable
mapping to nonverbal phonetic content (e.g., formant val-
ues), and features that capture aspects of speech often associ-
ated with affect (e.g., prosody and voice quality measures).
Implementation details for the custom feature set are pro-
vided in Appendix A.1, which can be found on the Computer
Society Digital Library at http://doi.ieeecomputersociety.
org/10.1109/TAFFC.2022.3208233. Details on the extraction
of the cepstral coefﬁcients and ﬁlter-bank coefﬁcients are
provided in Appendices A.2 and A.3, respectively, available
in the online supplemental material.
The eGeMAPs feature sets and cepstral coefﬁcient features
have been used extensively in prior work in speech emotion
recognition and other speech modeling tasks. Filterbank-
based features of size 40 were evaluated because they have
been used effectively in noise-robust speech emotion recogni-
tion in prior work [72]. The bag-of-phones feature set had size
50 and consisted of language independent phones extracted
from each vocalization using allosaurus , a Python library for
phone extraction [73]. The bag-of-phones included the 50
phones that appeared most frequently in the dataset. Simi-
larly to bag-of-words approaches, the bag-of-phones
encoded phone multiplicity but not order. Data-learned fea-
tures were extracted using the auDeep autoencoder to learn a
feature representation via unsupervised learning with a deep
neural net (DNN). The autoencoding was generated for eachfold split for 5-fold nested cross validation, before session
stratiﬁcation or class balancing. Because of the fold-based
learning structure, results with the auDeep feature set are
only reported with 5-fold nested CV evaluation and not
LOSO evaluation. A parameter specifying the length of
vocalization used in training the autoencoder and the param-
eters deﬁning the DNN architecture were optimized for each
participant. The selected parameters are provided in Appen-
dix B, available in the online supplemental material. Because
of the computationally-intensive nature of selecting hyper-
parameters for a given fold split, the evaluation metrics with
auDeep features are provided using one set of folds.
4R ESULTS AND DISCUSSION
4.1 Analysis of Labeling and Data Collection
Practices
In exit interviews, labelers generally reported high conﬁ-
dence in the ﬁdelity of their labels and cited that contextual
information (e.g., gestures and setting) helped them label
conﬁdently [65]. Fig. 5 shows the distribution of labels by
session for each participant. Fig. 6 shows box-and-whisker
plots of the labeling delay for each participating labeler. For
each participant, Table 5 shows the average session length,
number of sessions, average label duration, average number
of vocalizations per label, the average number of labels per
session, and the median number of unique labels per ses-
sion. Additionally, Appendix C, available in the online sup-
plemental material, shows the average and standard
deviations for label duration per class per participant. TheTABLE 3
Number of Samples for Each Class for Each Participant Without Rough Session Stratiﬁcation
TABLE 4
Features and Applied Functionals Used in the Custom Feature Set (Additional Implementation Details are Provided in Appendix A)
Feature Applied functionals
Audio amplitude Duration; Mean auto-correlation
Formants 1, 2, and 3 Mean; variation; frequency and duration of longest constant value (formants 1 & 2)
Power Freq associated with max. power; Variation; Interquartile range
Pitch (fundamental freq.) Mean; range; max; min; quartiles 1-3; number of peaks; overall rise/fall; quartile
1; quartile 2; quartile 3; Fit coefﬁcients for polynomials of order 1, 2, 3
GTCC,13 coeff. Mean
BFCC, 13 coeff. Mean
Cepstral peak prominence Mean
H1-H2; H2-H4 MeanNARAIN ET AL.: MODELING REAL-WORLD AFFECTIVE AND COMMUNICATIVE NONVERBAL VOCALIZATIONS FROM MINIMALLY SPEAKING... 2245
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:58 UTC from IEEE Xplore.  Restrictions apply. presented statistics can inform the design of other in-the-
moment tagging systems.
Labelers for P01 and P16 often deviated from the pre-
ferred labeling protocol by assigning labels to entire record-
ings, based on the general mood or communicative intent,
instead of individual vocalizations. As a result, they often
have only a single label in a given session (Fig. 5) and were
not included in the delay analyses and some of the tabulated
statistics could not be calculated for P01 and P16. Still, these
handwritten labels could be aligned with the recordings and
used in classiﬁcation models. The other participants more
closely followed the preferred collection protocol, designat-
ing labels for vocalizations that occurred within a recording.The average labeling delay ranged from 3.5-7 seconds
and were signiﬁcantly different between labelers. The Krus-
kal-Wallis nonparametric test found that differences in label
delay between labelers were signiﬁcant (p = 4.4* 10-9). Data
collection practices varied between participants. In future
studies, data collection could utilize an integrated recording
and labeling app (as shown in [65]). In such a system, the
clock for the recording system would be coupled directly to
the clock for the labeling system thereby eliminating any
clock drift and enabling in-depth analysis on the effect of
labeling delay on model performance.
P08 had signiﬁcantly longer average label durations than
other participants. Very long labels risk encompassing
Fig. 5. Each horizontal bar shows the distribution of labels in a particular session for a participant.2246 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:58 UTC from IEEE Xplore.  Restrictions apply. vocalizations of multiple categories, and could be associated
with mislabeled data. Generally, for a given participant, the
average label duration was within one standard deviation
across class types (Appendix C), available in the online sup-
plemental material. Frustrated had the highest average
labeling duration for four participants. P02, P03, P05, P06,
P08, and P11 all tended to have multiple unique labels in a
given session. Having multiple unique labels per session
may reduce the model’s likelihood of learning to associate a
label with a particular background soundscape.
4.2 Classiﬁcation
Evaluation metrics are reported using macro-averages
across classes. If a device or human were to listen and
respond to nonverbal vocalizations, having both high recall
and precision would be important for enabling consistent
appropriate responses. For this reason, the F1 score (the har-
monic mean of recall and precision which is2/C3precision /C3recall
precision /C3recall)
was used to evaluate and compare models. The unweighted
average recall (UAR) is also reported in the results tables.
4.2.1 Multi-Class Classiﬁcation
Multi-class classiﬁcation results for each model and evalua-
tion strategy with each aggregated feature set with and
without session stratiﬁcation are provided in Appendix Din Tables D1, D2, D3, and D4, available in the online supple-
mental material. Results with the feature-learned auDeep
features and the bag-of-phones feature set are provided in
Appendix D in Tables D5 and D6, respectively, available in
the online supplemental material.
Fig. 7 shows the highest F1 score across aggregate feature
sets and model types for each evaluation strategy. Fig. 8
shows model performance for only the LOSO evaluation
with session stratiﬁcation, the strategy least susceptible to
ﬁtting to background sounds. The best performing models
for each evaluation strategy had multi-class F1 scores higher
than chance for all participants (Fig. 7).
4.2.2 Evaluation Strategies
Generally, model performance is higher for models evalu-
ated using 5-fold cross validation. Model performance with
the 5-fold evaluation scheme, particularly without session
stratiﬁcation, is likely artiﬁcially inﬂated due to ﬁtting to
background noise. While the LOSO evaluation scheme is
less likely to classify samples correctly by ﬁtting to the back-
ground soundscape, it cannot correctly classify vocaliza-
tions that were expressed uniquely in one session. For each
participant, each vocalization label encompassed many dif-
ferent sounds. For example, for P05 ”selftalk” included
laughter, sighs, and complex phonetic expressions with
multiple constant-vowel components and transitions. The
LOSO evaluation scheme cannot classify unique sub-types
of a vocalization category that appeared only in one session.
For some participants (P01, P02, P03, P11), models with
session stratiﬁcation had better performance than models
without session stratiﬁcation even though session stratiﬁca-
tion reduced the number of available training samples. With-
out session stratiﬁcation a model is more likely to ﬁt to the
background soundscape. For LOSO evaluations, the test data
for each split has distinct background soundscapes from the
training data and so ﬁtting to background noise can reduce
model performance. In these cases, session stratiﬁcation can
improve model performance - i.e., for P01 models with LOSO
evaluation. The P01 data tended to have a single label for an
entire session which may have made models for P01 particu-
larly susceptible to ﬁtting to background noise (Fig. 5).
4.2.3 Differences in Model Performance Between
Participants
All F1 scores are above chance, but there are large variations in
performance between participants (Figs. 7 and 8). These
Fig. 6. Distribution of labeling delays for each participant. The labeling
delay was deﬁned as the time passed between a button push on the app
to start a label and the start of the ﬁrst vocalization associated with that
label. P01 and P16 were not included in the delay analysis because they
deviated from the preferred labeling protocol and did not assign labels
using the app. Still, the labelers for P01 and P16 did provide handwritten
labels for each ﬁle that could be aligned with the data and used in classi-
ﬁcation models.
TABLE 5
Statistics Describing Labeling and Data Collection Practices
P01 P02 P03 P05 P06 P08 P11 P16
Avg. session length (s) 943.6 2151.4 6704.7 5515.6 1641.4 1165.6 804.8 142.0
Number of sessions 38 11 12 33 11 21 28 57
Avg. label duration (s) N/A 9.7 0.9* 13.5 10.7 50.3 3.3 N/A
Avg. vocalizations per label 47.4 8.0 2.5 4.7 2.4 7.2 2.1 13.7
Avg. number of labels per session 2.6 11.3 26.1 11.7 30.0 16.7 11.7 1.1
Unique labels per session (med.) 1 3 3.5 2 5 4 3 1
*P03 collected some data using a previous version of the app which did not require specifying an ’end’ time and assumed a 2 second label duration.NARAIN ET AL.: MODELING REAL-WORLD AFFECTIVE AND COMMUNICATIVE NONVERBAL VOCALIZATIONS FROM MINIMALLY SPEAKING... 2247
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:58 UTC from IEEE Xplore.  Restrictions apply. variations could be due to differences in data collection and
labeling as well as inherent differences in vocal communication.
The number of samples used in training varied between
participants. P05 and P16 had the largest number of training
s a m p l e sp e rc l a s sa n dr e l a t i v e l yh i g hm o d e lp e r f o r m a n c e s
(Fig. 8). Still, the variations are not due to training sizes alone –
P03 also had a relatively high model performance but had one
of the fewest number of training samples per class. Differences
in labeling quality and style may have also affected model per-
formance. During follow-up interviews, the labelers for P06and P08 both mentioned forgetting to end a label when the
vocalization ended on occasion. This could have led to misla-
beled vocalizations in the dataset that affected model perfor-
mance. P03 had a high model performance despite a low
number of training samples per class and had the lowest aver-
age label duration (Table 5). A low labeling duration indicates
a close mapping between vocalizations and labels and a lower
likelihood of mislabels. P08 had a relatively low modeling per-
formance and the highest average label duration (Table 5).
Inherent differences in vocal communication between par-
ticipants may also contribute to variations in model perfor-
mance. The age, diagnoses affecting speech and language,
and number of spoken words or word approximations varied
between participants (Table 1). Future work with additional
participants and revised data collection procedures could
allow for the decoupling of labeling practices, model perfor-
mance, and demographics, providing valuable insights for
clinical and modeling practices.
Speaker-independent models were trained for each par-
ticipant (using only data from other participants’) to explore
the performance of non-personalized models. F1-scores for
speaker independent models were below chance, with the
exception of models evaluated with P05 data. Speaker-inde-
pendent models evaluated with P05 data had F1-scores
around 0.30, above chance but much lower than the F1-
scores for P05’s personalized models (Fig. 7).
4.2.4 Labels
Fig. 9 shows multi-class confusion matrices for the leave
one session out with session stratiﬁcation evaluation.
Fig. 7. F1 score for best performing model and feature set for each participant, evaluated using leave-one-session-out (“LOSO”) and 5-fold cross-va li-
dation (“CV”) with and without session stratiﬁcation (“Sess Str .”) The labels under each bar indicate the model and feature set. The number of trainin g
samples per class is shown in parentheses. A range is provided for LOSO evaluations, where the number of training samples varied between folds.
The conﬁdence intervals for the 5-fold CVevaluation are provided in Tables D1 and D2 in the Appendix, available in the online supplemental material.
Fig. 8. F1 score for best performing model and feature set for each par-
ticipant, evaluated with LOSO and session stratiﬁcation.2248 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:58 UTC from IEEE Xplore.  Restrictions apply. Differences in how well a particular label could be classiﬁed
could be characteristic of the vocalization itself the number
and quality of available training samples for that label.
”Frustrated” had high recall even when it had relatively
few samples, like for P03 (Table 2). For P06, ”frustrated”
had a high recall even though it had fewer than half of the
number of samples of the other classes. For P11, ”delighted”
vocalizations had the highest recall and the largest number
of training samples (Table 2).
Some classes might have poor classiﬁcation perfor-
mance because vocalizations in that class tended to have
multiple meanings (i.e., a ”frustrated request”) for an mv*
communicator, in which case the class predicted by the
model might be accurate even if it didn’t match the single
given label. We experimented with multiple labels while
piloting the study but found that asking labelers to desig-
nate multiple classes imposed too high of a cognitive load
and reduced the overall ﬁdel ity of the marked labels.
Labelers were asked to choose the most representative
label for a vocalization, and to only label a vocalization if
they were conﬁdent in its label. Still, understanding that
a vocalization could fall into multiple categories is impor-
tant when interpreting the results. Future studies could
revisit developing labeling methods to allow for assigning
multiple labels to a vocalization, which would enable fur-
ther analyses.
For many participants, there was a label that had more
ambiguity than the others. For instance, removing the
”selftalk” class for P01 and P06 improved the F1 score of the
best performing model (with LOSO and session stratiﬁca-
tion) to 0.50 (+0.12) and 0.37 (+0.07), respectively. Removing
the ”delighted” label from P05 and P08 improved the F1
scores to 0.62 (+0.13) and 0.39 (+0.09), respectively.
4.2.5 Model and Feature Set Performance
The nonlinear models (Random Forest with RBF kernel) gener-
ally had better performance for t he multi-class classiﬁcationtask. The custom feature set had the best model performance
for ﬁve of the eight participant with LOSO and session stratiﬁ-
cation (Fig. 8), suggesting that the distinct features and applied
functionals chosen for the custom feature set capture the
unique differences between nonver bal vocalizations of differ-
ent types. The cross-validati on approach utilized here was
appropriate for the small, hig hly varying dataset. However,
future work with sufﬁcient data for unique training/valida-
tion/evaluation splits would en able investigations of feature
performance. These results could then inform the development
of an improved custom feature set and broader applications to
nonverbal vocalization classiﬁcation models.
The bag-of-phones feature s et generally had poor per-
formance compared to the other feature sets. For some
participants - particularly, P03, P05, and P16 - the bag-of-
phones feature set had classiﬁcation performance greater
than chance. This may indicate a clearer variation in pho-
netics between vocalizations of different types for these
participants. The utilized phone extraction model was not
trained for nonverbal vocaliz ations and had performance
limitations even on typical verbal speech [73], which
likely contributed to the poor performance of the bag-of-
phones feature set. The data-learned features extracted
using auDeep generally performed similarly to the aggre-
gate feature sets. For 5-fold c ross-validation with session
stratiﬁcation, the features extracted using auDeep had the
highest F1 score for P01 and P02 by 3% and 4% respec-
tively, compared to the best performing aggregate feature
set. Generating data-learned features (as in auDeep) is
computationally intensive compared to the other
approaches explored in this paper but was evaluated
because such features have been shown in the literature
to contribute to signiﬁcant performance improvements
for some audio classiﬁcation tasks [74]. The presented
results suggest that, if additional data were collected, fur-
ther explorations of data-learned features, including other
autoencoder architectures an d self-supervised learning,
may be beneﬁcial.
Fig. 9. Multi-class confusion matrices for best performing model and feature set for each participant with LOSO with session stratiﬁcation evaluati on.
The diagonal entries of the matrix are the recall for each class.NARAIN ET AL.: MODELING REAL-WORLD AFFECTIVE AND COMMUNICATIVE NONVERBAL VOCALIZATIONS FROM MINIMALLY SPEAKING... 2249
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:58 UTC from IEEE Xplore.  Restrictions apply. 4.2.6 Binary Valence Classiﬁcation
Binary valence classiﬁcation results are provided for the
leave-one-session-out with session stratiﬁcation evaluation
method, the method least susceptible to ﬁtting to back-
ground soundscapes. Results for each model with each
aggregated feature set are provided in Table D7 in Appen-
dix D, available in the online supplemental material. Fig. 10
shows the best performing model and feature set for binary
valence classiﬁcation, evaluated with LOSO and session
stratiﬁcation.
Variations in model performance between participants
for the binary classiﬁcation task were less pronounced
(Fig. 10). This may be because there were a larger number of
training samples available per class for each participant for
the binary task. The linear SVM with stochastic gradient
descent (SGD) training had the highest performance for
four of the eight participants for binary classiﬁcation. The
linear model may have had comparatively better perfor-
mance for the binary classiﬁcation task than the multi-class
task because of the simpler nature of the task and the larger
amount of training data.
4.3 Limitations
The amount of data available for the presented analysis was
limited due to the time-intensive nature of the data collec-
tion process. As such, the presented results may have been
affected by overﬁtting due to experimenting with different
architectures and feature sets, particularly for participants
P01, P02, and P03. However, the other participants were not
used in architecture and feature development, so their
results reﬂect a more conservative representation of model
ﬁtting with this data.
Custom descriptive labels were an option in the labeling
app, but could be difﬁcult to create in-the-moment. Addi-
tionally, while labelers often incorporated cues from the
communicator’s gestures, body language, and other com-
munication when selecting a label, the labeling system did
not directly capture feedback from the communicator. In
the future, the labeling system could be improved byallowing for higher complexity and diversity of labels, and
by integrating feedback directly from the communicator.
5C ONCLUSION
In this paper, we presented results from the largest study of
nonverbal vocalizations with mv* communicators to date
along with modeling approaches appropriate for dealing
with real-world, messy data. Nonverbal vocalizations from
mv* communicators contain important communicative and
affective information but are understudied and not well
understood by those who don’t know a communicator well.
Developing methods to classify nonverbal vocalizations by
affect and intent is an important step towards improved
understanding of this unique type of communication. The
F1 score for each participant for multi-class classiﬁcation
was above chance, even with conservative evaluation
schemes. This result suggests that there are inherent acous-
tic differences between vocalizations of different types for
the eight mv* communicators in this study. This result is
important because it shows, for the ﬁrst time, that it is possi-
ble for models to classify nonverbal vocalizations by affect
and intent with mv* individuals using audio alone.
There were large variations in model performance
between mv* communicators, which may have been due
to presented differences in labeling and data collection
practices between participating families and due to inher-
ent differences in communication practices between par-
ticipants. Additional training data from each participant
would likely improve multi-class classiﬁcation results -
models had relatively high performance with data from
participants with many traini ng samples, even for partici-
pants with potentially lower labeling ﬁdelity (i.e., P01).
High quality labels (i.e., P03) may allow for accurate clas-
siﬁcation even with a smaller number of training samples.
In future work, vocalizations from more mv* communica-
tors could also be used to explore whether there are sub-
groups of communicators with similar vocalization
practices.
We contribute to an improved understanding of non-
verbal vocalizations from mv* communicators. There has
been little prior work with this unique and understudied
population. Better understan ding of nonverbal vocaliza-
tions with mv* communicators could lead to improved
communication technology: for instance, this understand-
ing could be used to develop real-time vocalization classi-
ﬁcation systems and educational tools for individuals
who don’t know a mv* communicator well, and vocaliza-
tion controlled AAC devic es for mv* communicators.
Real-world studies of vocal communication can also con-
tribute to answering scientiﬁc questions around language
development, such as how and when phonemes are used
across development, especially within early affective and
communicative expression.
While many families and clinical practitioners under-
stand that nonverbal vocalizations are communication, indi-
viduals who are new to communicating with mv*
communicators often do not know to listen for this type of
communication. We hope that our study of nonverbal com-
munication with mv* communicators will also lead to
improved awareness among the community-at-large that
Fig. 10. F1 score for best performing binary valence model and feature
set for each participant, evaluated with LOSO and session stratiﬁcation.2250 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:58 UTC from IEEE Xplore.  Restrictions apply. nonverbal vocalizations that occur without typical verbal
speech are communication that should be acknowledged
and responded to appropriately.
ACKNOWLEDGMENTS
The authors would like to thank the participants in this
study. Amanda O’Brien and Ayelet Kershenbaum provided
feedback on the study design and data analysis. Michelle
Luo and Yuji Chan contributed to instructional materials
used with the study. Kristina Johnson and Thomas Quatieri
were participants in the study.
REFERENCES
[1] H. Tager-Flusberg and C. Kasari, “Minimally verbal school-aged
children with autism spectrum disorder: The neglected end of the
spectrum,” Autism Res. , vol. 6, no. 6, pp. 468–478, 2013.
[2] P. M. Dietz, C. E. Rose, D. McArthur, and M. Maenner, “National
and state estimates of adults with autism spectrum disorder,”
J. Autism Develop. Disord. , vol. 50, no. 12, pp. 4258–4266, Dec. 2020.
[3] M. D. Kogan et al., “The prevalence of parent-reported autism
spectrum disorder among US children,” Pediatrics , vol. 142, no. 6,
2018, Art. no. e20174161.
[4] J. Narain et al., “Personalized modeling of real-world vocaliza-
tions from nonverbal individuals,” in Proc. Int. Conf. Multimodal
Interaction , 2020, pp. 665–669.
[5] A. Batten, “Inclusion and the autism spectrum,” Improving Sch. ,
vol. 8, no. 1, pp. 93–96, 2005.
[6] E. Marchi, B. Schuller, A. Batliner, S. Fridenzon, S. Tal, and
O. Golan, “Emotion in the speech of children with autism spec-
trum conditions: Prosody and everything else,” in Proc. 3rd Work-
shop Child Comput. Interaction , 2012.
[7] E. Marchi et al., “Typicality and emotion in the voice of children
with autism spectrum condition: Evidence across three
languages,” in Proc. 16th Annu. Conf. Int. Speech Commun. Assoc. ,
2015.
[8] O. Rudovic et al., “CultureNet: A deep learning approach for
engagement intensity estimation from face images of children
with autism,” in Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst. , 2018,
pp. 339–346.
[9] D. J. Faso, N. J. Sasson, and A. E. Pinkham, “Evaluating posed and
evoked facial expressions of emotion from adults with autism
spectrum disorder,” J. Autism Develop. Disord. , vol. 45, no. 1,
pp. 75–89, Jan. 2015.
[10] K. K. Hyde et al., “Applications of supervised machine learning in
autism spectrum disorder research: A review,” Rev. J. Autism
Develop. Disord. , vol. 6, no. 2, pp. 128–146, 2019.
[11] R. W. Picard, “Future affective technology for autism and emotion
communication,” Philos. Trans. Roy. Soc. B: Biol. Sci. , vol. 364,
no. 1535, pp. 3575–3584, 2009.
[12] S. Sarabadani, L. C. Schudlo, A. A. Samadani, and A. Kushski,
“Physiological detection of affective states in children with autism
spectrum disorder,” IEEE Trans. Affective Comput. , vol. 11, no. 4,
pp. 588–600, Oct.–Dec. 2020.
[13] A. Kushki, A. Khan, J. Brian, and E. Anagnostou, “A kalman ﬁlter-
ing framework for physiological detection of anxiety-related
arousal in children with autism spectrum disorder,” IEEE Trans.
Biomed. Eng. , vol. 62, no. 3, pp. 990–1000, Mar. 2015.
[14] W. L. Stone, O. Y. Ousley, P. J. Yoder, K. L. Hogan, and S. L. Hep-
burn, “Nonverbal communication in two-and three-year-old chil-
dren with autism,” J. Autism Develop. Disord. , vol. 27, no. 6,
pp. 677–696, 1997.
[15] R. G. Gordon and L. R. Watson, “Brief report: Gestures in children
at risk for autism spectrum disorders,” J. Autism Develop. Disord. ,
vol. 45, no. 7, pp. 2267–2273, 2015.
[16] S. E. Colgan, E. Lanter, C. McComish, L. R. Watson, E. R.
Crais, and G. T. Baranek, “Analysis of social interaction ges-
tures in infants with autism,” Child Neuropsychol. , vol. 12,
no. 4/5, pp. 307–319, 2006.
[17] A. de Marchena and I.-M. Eigsti, “Conversational gestures in
autism spectrum disorders: Asynchrony but not decreased
frequency,” Autism Res. , vol. 3, no. 6, pp. 311–322, 2010.[18] C.-H. Chiang, W.-T. Soong, T.-L. Lin, and S. J. Rogers, “Nonverbal
communication skills in young children with autism,” J. Autism
Develop. Disord. , vol. 38, no. 10, pp. 1898–1906, 2008.
[19] H. Sowden, J. Clegg, and M. Perkins, “The development of co-
speech gesture in the communication of children with autism
spectrum disorders,” Clin. Linguistics Phonetics , vol. 27, no. 12,
pp. 922–939, 2013.
[20] J. Hashemi et al., “Computer vision analysis for quantiﬁcation of
autism risk behaviors,” IEEE Trans. Affective Comput. , vol. 12,
no. 1, pp. 215–226, 1st Quart. 2021.
[21] L. Couper et al., “Comparing acquisition of and preference for
manual signs, picture exchange, and speech-generating devices in
nine children with autism spectrum disorder,” Develop. Neuroreha-
bilitation , vol. 17, no. 2, pp. 99–109, 2014.
[22] C. C. Peterson, V. Slaughter, and C. Brownell, “Children with
autism spectrum disorder are skilled at reading emotion body
language,” J. Exp. Child Psychol. , vol. 139, pp. 35–50, 2015.
[23] K. Schindler, L. Van Gool, and B. De Gelder, “Recognizing emo-
tions expressed by body pose: A biologically inspired neural mod-
el,”Neural Netw. , vol. 21, no. 9, pp. 1238–1246, 2008.
[24] C. Holyﬁeld, K. D. Drager, J. M. Kremkow, and J. Light,
“Systematic review of AAC intervention research for adolescents
and adults with autism spectrum disorder,” Augmentative Altern.
Commun. , vol. 33, no. 4, pp. 201–212, 2017.
[25] M. Romski, R. A. Sevcik, A. Barton-Hulsey, and A. S. Whitmore,
“Early intervention and AAC: What a difference 30 years makes,”
Augmentative Altern. Commun. , vol. 31, no. 3, pp. 181–202, 2015.
[26] E. Zamagni, C. Dolcini, E. Gessaroli, E. Santelli, and F. Frassi-
netti, “Scared by you: Modulation of bodily-self by emotional
body-postures in autism,” Neuropsychology , vol. 25, no. 2, 2011,
Art. no. 270.
[27] R. Ca ~nigueral and A. F. D. C. Hamilton, “The role of eye gaze dur-
ing natural social interactions in typical and autistic people,”
Front. Psychol. , vol. 10, 2019, Art. no. 560.
[28] C. Wilson, M. Brereton, B. Ploderer, and L. Sitbon, “Co-design
beyond words: ‘moments of interaction’ with minimally-verbal
children on the autism spectrum,” in Proc. CHI Conf. Hum. Factors
Comput. Syst. , 2019, pp. 1–15.
[29] C. Wilson, L. Sitbon, B. Ploderer, J. Opie, and M. Brereton, “Self-
expression by design: Co-designing the expressiball with mini-
mally-verbal children on the autism spectrum,” in Proc. CHI Conf.
Hum. Factors Comput. Syst. , 2020, pp. 1–13.
[30] M. Gratier and E. Devouche, “Imitation and repetition of prosodic
contour in vocal interaction at 3 months,” Develop. Psychol. ,
vol. 47, no. 1, 2011, Art. no. 67.
[31] E. Donnellan, C. Bannard, M. L. McGillion, K. E. Slocombe, and
D. Matthews, “Infants’ intentionally communicative vocalizations
elicit responses from caregivers and are the best predictors of the
transition to language: A longitudinal investigation of infants’
vocalizations, gestures and word production,” Develop. Sci. ,
vol. 23, no. 1, 2020, Art. no. e12843.
[32] J. McDaniel, K. D. Slaboch, and P. Yoder, “A meta-analysis of the
association between vocalizations and expressive language in
children with autism spectrum disorder,” Res. Develop. Disabilities ,
vol. 72, pp. 202–213, 2018.
[33] L. Morgan and Y. E. Wren, “A systematic review of the literature
on early vocalizations and babbling patterns in young children,”
Commun. Disord. Quart. , vol. 40, no. 1, pp. 3–14, 2018.
[34] S. R. Morris, “Clinical application of the mean babbling level and
syllable structure level,” Lang. Speech Hear. Serv. Sch. , vol. 41,
pp. 223–230, 2010.
[35] D. K. Oller et al., “Automated vocal analysis of naturalistic record-
ings from children with autism, language delay, and typical
development,” Proc. Nat. Acad. Sci. USA , vol. 107, no. 30,
pp. 13 354–13 359, 2010.
[36] E. C. Bacon, S. Osuna, E. Courchesne, and K. Pierce, “Naturalistic
language sampling to characterize the language abilities of 3-year-
olds with autism spectrum disorder,” Autism , vol. 23, no. 3,
pp. 699–712, 2019.
[37] A. Gregory, M. Tabain, and M. Robb, “Duration and voice quality
of early infant vocalizations,” J. Speech Lang. Hear. Res. , vol. 61,
no. 7, pp. 1591–1602, 2018.
[38] S. J. Sheinkopf, J. M. Iverson, M. L. Rinaldi, and B. M. Lester,
“Atypical cry acoustics in 6-month-old infants at risk for
autism spectrum disorder,” Autism Res. ,v o l .5 ,n o .5 ,
pp. 331–339, 2012.NARAIN ET AL.: MODELING REAL-WORLD AFFECTIVE AND COMMUNICATIVE NONVERBAL VOCALIZATIONS FROM MINIMALLY SPEAKING... 2251
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:58 UTC from IEEE Xplore.  Restrictions apply. [39] E. J. Tenenbaum et al., “A six-minute measure of vocalizations in
toddlers with autism spectrum disorder,” Autism Res. , vol. 13,
no. 8, pp. 1373–1382, 2020.
[40] L. R. Hamrick, A. Seidl, and B. L. Tonnsen, “Acoustic properties of
early vocalizations in infants with fragile x syndrome,” Autism
Res., vol. 12, no. 11, pp. 1663–1679, 2019.
[41] G. E. Martin, J. Klusek, B. Estigarribia, and J. E. Roberts,
“Language characteristics of individuals with Down syndrome,”
Topics Lang. Disord. , vol. 29, no. 2, 2009, Art. no. 112.
[42] L. Rescorla and N. B. Ratner, “Phonetic proﬁles of toddlers with
speciﬁc expressive language impairment (SLI-E),” J. Speech Lang.
Hear. Res. , vol. 39, no. 1, pp. 153–165, 1996.
[43] H.-M. Chiang, “Differences between spontaneous and elicited
expressive communication in children with autism,” Res. Autism
Spectr. Disord. , vol. 3, no. 1, pp. 214–222, 2009.
[44] F. H. Wilhelm and P. Grossman, “Emotions beyond the labora-
tory: Theoretical fundaments, study design, and analytic strate-
gies for advanced ambulatory assessment,” Biol. Psychol. , vol. 84,
no. 3, pp. 552–569, 2010.
[45] D. A. Sauter, F. Eisner, A. J. Calder, and S. K. Scott, “Perceptual
cues in nonverbal vocal expressions of emotion,” Quart. J. Exp.
Psychol. , vol. 63, no. 11, pp. 2251–2272, 2010.
[46] I. Poggi, A. Ansani, and C. Cecconi, “Sighs in everyday and politi-
cal communication,” in Proc. Laughter Workshop , 2018, pp. 50–53.
[47] M. L. Knapp, Essentials of Nonverbal Communication . San Diego,
CA, USA: Harcourt School, 1980.
[48] D. A. Sauter, F. Eisner, P. Ekman, and S. K. Scott, “Cross-cultural
recognition of basic emotions through nonverbal emotional vocal-
izations,” Proc. Nat. Acad. Sci. USA , vol. 107, no. 6, pp. 2408–2412,
2010.
[49] A. Anikin and C. F. Lima, “Perceptual and acoustic differences
between authentic and acted nonverbal emotional vocalizations,”
Quart. J. Exp. Psychol. , vol. 71, no. 3, pp. 622–641, 2018.
[50] N. Holz, P. Larrouy-Maestri, and D. Poeppel, “The paradoxical
role of emotional intensity in the perception of vocal affect,” Sci.
Rep., vol. 11, no. 1, pp. 1–10, 2021.
[51] A. Anikin, “A moan of pleasure should be breathy: The effect of
voice quality on the meaning of human nonverbal vocalizations,”
Phonetica , vol. 77, no. 5, pp. 327–349, 2020.
[52] J. Trouvain and K. P. Truong, “Comparing non-verbal vocalisa-
tions in conversational speech corpora,” in Proc. LREC Workshop
Corpora Res. Emotion Sentiment Soc. Signals , 2012, pp. 36–39.
[53] M. Schr €oder, “Experimental study of affect bursts,” Speech Com-
mun. , vol. 40, no. 1/2, pp. 99–116, 2003.
[54] O. Wasz-H €ockert, T. Partanen, V. Vuorenkoski, K. Michelsson,
and E. Valanne, “The identiﬁcation of some speciﬁc meanings
in infant vocalization,” Experientia , vol. 20, no. 3, pp. 154–154,
1964.
[55] L. Liu, W. Li, X. Wu, and B. X. Zhou, “Infant cry language analysis
and recognition: An experimental approach,” IEEE/CAA J. Auto-
matica Sinica , vol. 6, no. 3, pp. 778–788, May 2019.
[56] I.-A. B /C21anic/C21a, H. Cucu, A. Buzo, D. Burileanu, and C. Burileanu,
“Automatic methods for infant cry classiﬁcation,” in Proc. Int.
Conf. Commun. , 2016, pp. 51–54.
[57] S. Sharma and V. K. Mittal, “Infant cry analysis of cry signal seg-
ments towards identifying the cry-cause factors,” in Proc. IEEE
Region 10 Conf. , 2017, pp. 3105–3110.
[58] T. Fuhr, H. Reetz, and C. Wegener, “Comparison of supervised-
learning models for infant cry classiﬁcation/vergleich von klas-
siﬁkationsmodellen zur s €auglingsschreianalyse,” Int. J. Health
Professions , vol. 2, no. 1, pp. 4–15, 2015.
[59] O. Weisman et al., “Dynamics of non-verbal vocalizations and
hormones during father-infant interaction,” IEEE Trans. Affective
Comput. , vol. 7, no. 4, pp. 337–345, Oct.–Dec. 2016.
[60] D. R. Beukelman et al., Augmentative and Alternative Communica-
tion. Baltimore, MD, USA: Paul H. Brookes, 1998.
[61] K. T. Johnson, J. Narain, C. Ferguson, R. Picard, and P. Maes, “The
ECHOS platform to enhance communication for nonverbal chil-
dren with Autism: A case study,” in Proc. Extended Abstr. CHI
Conf. Hum. Factors Comput. Syst. , 2020, pp. 1–8.
[62] J. Narain, K. T. Johnson, R. Picard, and P. Maes, “Zero-shot trans-
fer learning to enhance communication for minimally verbal indi-
viduals with autism using naturalistic data,” in Proc. AI Soc. Good
Workshop NeurIPS , 2019.[63] K. T. Johnson, J. Narain, T. Quatieri, R. Picard, and P. Maes,
“ReCANVo: A database of real-world communicative and
affective nonverbal vocalizations,” Submitted for publication ,
2022.
[64] K. T. Johnson et al., “Phonemic content of nonverbal vocalizations
from individuals with 0-10 spoken words,” INSAR , 2022.
[65] J. Narain, “Interfaces and models for improved understanding of
real-world communicative and affective nonverbal vocalizations
by minimally speaking individuals,” Ph.D. dissertation, Dept.
Mech. Eng., Massachusetts Inst. Technol., Cambridge, MA, 2021.
[66] G. Lema ^ıtre, F. Nogueira, and C. K. Aridas, “Imbalanced-learn: A
python toolbox to tackle the curse of imbalanced datasets in
machine learning,” J. Mach. Learn. Res. , vol. 18, no. 1, pp. 559–563,
2017.
[67] M. Freitag, S. Amiriparian, S. Pugachevskiy, N. Cummins, and B.
Schuller, “auDeep: Unsupervised learning of representations
from audio with deep recurrent neural networks,” J. Mach. Learn.
Res., vol. 18, no. 1, pp. 6340–6344, 2017.
[68] F. Eyben, F. Weninger, F. Gross, and B. Schuller, “Recent develop-
ments in openSMILE, the Munich open-source multimedia fea-
ture extractor,” in Proc. 21st ACM Int. Conf. Multimedia , 2013,
pp. 835–838.
[69] F. Eyben et al., “The Geneva minimalistic acoustic parameter
set (GeMAPS) for voice research and affective computing,”
IEEE Trans. xAffective Comput. , vol. 7, no. 2, pp. 190–202, Apr.–
Jun. 2016.
[70] D. D. Mehta, D. Rudoy, and P. J. Wolfe, “Kalman-based autore-
gressive moving average modeling and inference for formant and
antiformant tracking,” J. Acoustical Soc. Amer. , vol. 132, no. 3,
pp. 1732–1746, 2012.
[71] Y.-L. Shue, “The voice source in speech production: Data, analysis
and models,” Univ. California, Los Angeles, Los Angeles, CA,
2010.
[72] M. Jaiswal and E. M. Provost, “Best practices for noise-based aug-
mentation to improve the performance of emotion recognition “in
the wild”,” 2021, arXiv:2104.08806 .
[73] X. Li et al., “Universal phone recognition with a multilingual allo-
phone system,” in Proc. IEEE Int. Conf. Acoust. Speech Signal Pro-
cess., 2020, pp. 8249–8253.
[74] B. W. Schuller, “Speech emotion recognition: Two decades in a
nutshell, benchmarks, and ongoing trends,” Commun. ACM ,
vol. 61, no. 5, pp. 90–99, 2018.
Jaya Narain received the BS, MS, and PhD
degree from the Massachusetts Institute of Tech-
nology (MIT), in 2015, 2017, and 2021, respec-
tively . She conducted her doctoral research in the
Fluid Interfaces group in the MIT Media Lab. She
was named an Apple Scholar in AI/ML, in 2020
and an NSF Graduate research fellow, in 2016.
Her research interests are machine learning
and human-centered design for accessibility and
health applications, and particularly real-world
data collection and personalized systems.
Kristina T. Johnson received the bachelor’s and
master’s degrees with highest honors in physics,
and the PhD degree from the Massachusetts
Institute of Technology (MIT), in 2021, where she
was a member of the Affective Computing Group,
MIT Media Lab. She is a multi-disciplinary
researcher whose work lies with the intersection
of neuroscience, engineering, computer science,
affective sciences, technology development, and
clinical applications, especially for individuals
with complex neurodevelopmental differences,
genetic disorders, or autism. She was twice named an MIT Hugh Hamp-
ton Young Fellow and three times named an MIT Media Lab Learning
Innovation Fellow during her doctoral studies. As a physicist, she was
awarded the national Barry M. Goldwater Scholarship.2252 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 13, NO. 4, OCTOBER-DECEMBER 2022
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:58 UTC from IEEE Xplore.  Restrictions apply. Thomas F. Quatieri (Fellow, IEEE) received the
BS degree (summa cum laude) from Tufts Univer-
sity , and the SM, EE, and ScD degrees from the
Massachusetts Institute of Technology (MIT). He
is a senior member of the technical staff with MIT
Lincoln Laboratory, Lexington, focused on speech
and auditory signal processing and neuro-bio-
physical modeling with application to detection
and monitoring of neurological, neurotraumatic,
and stress conditions. He holds a faculty appoint-
ment in the Harvard-MIT Speech and Hearing Bio-
science and Technology Program. He is an author on more than 200
publications, holds 12 patents, and authored the textbook Discrete-Time
Speech Signal Processing: Principles and Practice. He is a recipient of
four IEEE Transactions best paper awards and the 2010 MIT Lincoln Lab-
oratory Best Paper Award. He led the Lincoln Laboratory team that won
the 2013 and 2014 AVEC Depression Challenges and the 2015 MIT Lin-
coln Laboratory Team Award for their work on vocal and facial biomarkers.
He has served on many IEEE signal processing and speech technical
committees and currently is an associate editor of Computer, Speech,
and Language . He is a member of Tau Beta Pi, Eta Kappa Nu, Sigma Xi,
ICSA, ARO, ASA, and SFN.
Rosalind W. Picard (Fellow, IEEE) received the
BS degree in electrical engineering from the
Georgia Institute of Techology, and an SM and
ScD degrees in electrical engineering and com-
puter science from MIT . She is professor of media
arts and sciences with MIT , founder and director
of the Affective Computing Research Group, MIT
Media Lab, and co-founder of the startups Affec-
tiva and Empatica. In 2019, she received one of
the highest professional honors accorded an
engineer , election to the National Academy of
Engineering for her contributions on affective computing and wearable
computing. She is credited with starting the branch of computer science
known as affective computing with her 1997 book of the same name.
She is author of the book Affective Computing, and author or co-author
of over 350 scientiﬁc articles (more than 66k citations).
Pattie Maes (Member , IEEE) received the PhD
degree in computer science from the University
of Brussels, Belgium. She is a professor in MIT’s
Program in media arts and sciences and until
recently served as its academic head. She runs
the Media Lab’s Fluid Interfaces research group,
which aims to radically reinvent the human-
machine interaction. Coming from a background
in artiﬁcial intelligence and human-computer
interaction, she focuses on immersive and wear-
able systems that can actively assist people with
memory, attention, learning, decision making, communication, and well-
being. She is the editor of three books, and is an editorial board member
and reviewer for numerous professional journals and conferences. Fast
Company named her one of 50 most inﬂuential designers (2011); News-
week picked her as one of the ”100 Americans to watch for” in the year
2000; TIME Digital selected her as a member of the “Cyber Elite,” the
top 50 technological pioneers of the high-tech world; the World Eco-
nomic Forum honored her with the title ”Global Leader for Tomorrow”;
Ars Electronica Awarded her the 1995 World Wide Web category prize;
and in 2000 she was recognized with the ”Lifetime Achievement Award”
by the Massachusetts Interactive Media Council.
"For more information on this or any other computing topic,
please visit our Digital Library at www.computer .org/csdl.NARAIN ET AL.: MODELING REAL-WORLD AFFECTIVE AND COMMUNICATIVE NONVERBAL VOCALIZATIONS FROM MINIMALLY SPEAKING... 2253
Authorized licensed use limited to: The University of British Columbia Library. Downloaded on January 27,2023 at 07:05:58 UTC from IEEE Xplore.  Restrictions apply. 